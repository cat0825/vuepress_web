{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理 (NLP)</li>\n<li><strong>标签</strong>：WordPiece, 分词算法, 自然语言处理, NLP模型, Tokenization</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"wordpiece分词算法简介\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece分词算法简介\"><span>WordPiece分词算法简介</span></a></h2>\n<p>WordPiece是一种常见的分词算法，广泛应用于自然语言处理任务中（如BERT模型）。其核心思想与BPE（Byte Pair Encoding）类似，但在合并子词时采用了基于互信息（Mutual Information）的策略，能更好地平衡词表大小和OOV（Out-Of-Vocabulary，未登录词）问题。</p>\n<p>💡 <strong>启发点</strong>：通过互信息优化子词合并，提升了语言模型的表现力。</p>\n<hr>\n<h2 id=\"核心观点与实现步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点与实现步骤\"><span>核心观点与实现步骤</span></a></h2>\n<h3 id=\"wordpiece的核心思想\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece的核心思想\"><span>WordPiece的核心思想</span></a></h3>\n<ul>\n<li>与BPE类似，WordPiece从一个基础词表出发，通过不断合并子词生成最终的词表。</li>\n<li>不同于BPE按频率选择合并对，WordPiece通过计算子词间的互信息来决定合并顺序。</li>\n<li><strong>互信息的公式</strong>：<br>\n假设合并子词 <code v-pre>x</code> 和 <code v-pre>y</code> 后生成新子词 <code v-pre>z</code>，互信息得分计算如下：<div class=\"language-math line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"math\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span>score = P(x) * P(y) / P(z)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>其中，<code v-pre>P(x)</code> 表示子词 <code v-pre>x</code> 在语料中的出现频率。</li>\n</ul>\n<hr>\n<h3 id=\"wordpiece的实现步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece的实现步骤\"><span>WordPiece的实现步骤</span></a></h3>\n<p>以下是WordPiece分词的主要步骤，用简单的符号和标记描述：</p>\n<p>✅ <strong>步骤1</strong>：准备基础词表</p>\n<ul>\n<li>包含26个英文字母及常见符号，如<code v-pre>a, b, c, @, #</code>等。</li>\n</ul>\n<p>✅ <strong>步骤2</strong>：将语料拆分为最小单元</p>\n<ul>\n<li>每个单词被拆分为基础字母或符号，例如<code v-pre>hello</code>被拆分为<code v-pre>h, e, l, l, o</code>。</li>\n</ul>\n<p>✅ <strong>步骤3</strong>：训练语言模型</p>\n<ul>\n<li>基于拆分后的数据，使用Unigram语言模型训练子词概率。</li>\n</ul>\n<p>✅ <strong>步骤4</strong>：选择互信息最大的子词对合并</p>\n<ul>\n<li>从所有可能的子词对中选择，使得合并后能最大程度提高语料的概率。</li>\n</ul>\n<p>✅ <strong>步骤5</strong>：重复合并，直到满足条件</p>\n<ul>\n<li>条件可以是达到预设的词表大小或概率增量低于某一阈值。</li>\n</ul>\n<hr>\n<h3 id=\"实现代码片段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#实现代码片段\"><span>实现代码片段</span></a></h3>\n<p>以下是WordPiece中计算互信息得分的核心代码：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 计算子词对的互信息得分</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">scores </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> {</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> freq </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">/</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> (</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">letter_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]]</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> letter_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]])</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> freq </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pair_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">items</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">}</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">return</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scores</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><hr>\n<h2 id=\"优缺点分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优缺点分析\"><span>优缺点分析</span></a></h2>\n<table>\n<thead>\n<tr>\n<th><strong>优点</strong></th>\n<th><strong>缺点</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>较好地平衡了词表大小和未登录词问题</td>\n<td>可能产生不合理的子词切分</td>\n</tr>\n<tr>\n<td>子词间关联性强，提高语言模型表现</td>\n<td>对拼写错误敏感</td>\n</tr>\n<tr>\n<td>支持高效的语言模型训练</td>\n<td>对前缀处理效果不佳</td>\n</tr>\n</tbody>\n</table>\n<p>💡 <strong>启发点</strong>：可以通过改进前缀和复合词处理，进一步优化算法效果。</p>\n<hr>\n<h2 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h2>\n<p>⚠️ <strong>常见错误1</strong>：忽略基础词表的重要性</p>\n<ul>\n<li>基础词表过小会导致过多无意义的子词生成。</li>\n</ul>\n<p>⚠️ <strong>常见错误2</strong>：不合理设置阈值</p>\n<ul>\n<li>如果合并阈值过低，可能导致训练时间过长或出现低质量子词。</li>\n</ul>\n<p>⚠️ <strong>常见错误3</strong>：未考虑语料质量</p>\n<ul>\n<li>拼写错误、噪声数据会显著影响分词效果。</li>\n</ul>\n<hr>\n<h2 id=\"思考与延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考与延伸问题\"><span>思考与延伸问题</span></a></h2>\n<ol>\n<li>如何在WordPiece中更好地处理拼写错误或前缀问题？</li>\n<li>是否可以结合BPE和WordPiece的优点创建新的分词算法？</li>\n<li>在多语言环境下，WordPiece是否需要特殊优化？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：<a href=\"https://arxiv.org/pdf/2012.15524\" target=\"_blank\" rel=\"noopener noreferrer\">论文《Fast WordPiece Tokenization》</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实现WordPiece分词算法，并测试不同语料下的效果。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 比较BPE与WordPiece在OOV处理上的性能差异。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索基于互信息优化的新型分词方法。</label></li>\n</ul>\n<hr>\n<h2 id=\"后续追踪计划\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪计划\"><span>后续追踪计划</span></a></h2>\n<p>📈 <strong>趋势预测</strong>：随着NLP模型对多语言支持需求增加，更高效、更通用的分词算法将成为研究热点。<br>\n📋 <strong>研究计划</strong>：</p>\n<ol>\n<li>开展对比实验，评估不同分词算法在实际任务中的表现。</li>\n<li>探索如何结合深度学习优化分词过程，例如用Transformer预测子词合并。</li>\n</ol>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/分词/WordPiece分词算法解析与实践.md","filePathRelative":"notes_bak/大语言模型学习/分词/WordPiece分词算法解析与实践.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/分词/wordpiece","tags":["NLP"],"permalink":"/大语言模型学习/分词/wordpiece/","dgPassFrontmatter":true,"noteIcon":null,"created":"2025-03-27T02:20:58.168Z","updated":"2025-04-12T04:53:49.745Z","title":"WordPiece分词算法解析与实践","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理 (NLP)</li>\n<li><strong>标签</strong>：WordPiece, 分词算法, 自然语言处理, NLP模型, Tokenization</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"wordpiece分词算法简介\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece分词算法简介\"><span>WordPiece分词算法简介</span></a></h2>\n<p>WordPiece是一种常见的分词算法，广泛应用于自然语言处理任务中（如BERT模型）。其核心思想与BPE（Byte Pair Encoding）类似，但在合并子词时采用了基于互信息（Mutual Information）的策略，能更好地平衡词表大小和OOV（Out-Of-Vocabulary，未登录词）问题。</p>\n<p>💡 <strong>启发点</strong>：通过互信息优化子词合并，提升了语言模型的表现力。</p>\n<hr>\n<h2 id=\"核心观点与实现步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点与实现步骤\"><span>核心观点与实现步骤</span></a></h2>\n<h3 id=\"wordpiece的核心思想\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece的核心思想\"><span>WordPiece的核心思想</span></a></h3>\n<ul>\n<li>与BPE类似，WordPiece从一个基础词表出发，通过不断合并子词生成最终的词表。</li>\n<li>不同于BPE按频率选择合并对，WordPiece通过计算子词间的互信息来决定合并顺序。</li>\n<li><strong>互信息的公式</strong>：<br>\n假设合并子词 <code v-pre>x</code> 和 <code v-pre>y</code> 后生成新子词 <code v-pre>z</code>，互信息得分计算如下：<div class=\"language-math line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"math\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span>score = P(x) * P(y) / P(z)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>其中，<code v-pre>P(x)</code> 表示子词 <code v-pre>x</code> 在语料中的出现频率。</li>\n</ul>\n<hr>\n<h3 id=\"wordpiece的实现步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece的实现步骤\"><span>WordPiece的实现步骤</span></a></h3>\n<p>以下是WordPiece分词的主要步骤，用简单的符号和标记描述：</p>\n<p>✅ <strong>步骤1</strong>：准备基础词表</p>\n<ul>\n<li>包含26个英文字母及常见符号，如<code v-pre>a, b, c, @, #</code>等。</li>\n</ul>\n<p>✅ <strong>步骤2</strong>：将语料拆分为最小单元</p>\n<ul>\n<li>每个单词被拆分为基础字母或符号，例如<code v-pre>hello</code>被拆分为<code v-pre>h, e, l, l, o</code>。</li>\n</ul>\n<p>✅ <strong>步骤3</strong>：训练语言模型</p>\n<ul>\n<li>基于拆分后的数据，使用Unigram语言模型训练子词概率。</li>\n</ul>\n<p>✅ <strong>步骤4</strong>：选择互信息最大的子词对合并</p>\n<ul>\n<li>从所有可能的子词对中选择，使得合并后能最大程度提高语料的概率。</li>\n</ul>\n<p>✅ <strong>步骤5</strong>：重复合并，直到满足条件</p>\n<ul>\n<li>条件可以是达到预设的词表大小或概率增量低于某一阈值。</li>\n</ul>\n<hr>\n<h3 id=\"实现代码片段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#实现代码片段\"><span>实现代码片段</span></a></h3>\n<p>以下是WordPiece中计算互信息得分的核心代码：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 计算子词对的互信息得分</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">scores </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> {</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> freq </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">/</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> (</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">letter_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]]</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> letter_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]])</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> freq </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pair_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">items</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">}</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">return</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scores</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><hr>\n<h2 id=\"优缺点分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优缺点分析\"><span>优缺点分析</span></a></h2>\n<table>\n<thead>\n<tr>\n<th><strong>优点</strong></th>\n<th><strong>缺点</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>较好地平衡了词表大小和未登录词问题</td>\n<td>可能产生不合理的子词切分</td>\n</tr>\n<tr>\n<td>子词间关联性强，提高语言模型表现</td>\n<td>对拼写错误敏感</td>\n</tr>\n<tr>\n<td>支持高效的语言模型训练</td>\n<td>对前缀处理效果不佳</td>\n</tr>\n</tbody>\n</table>\n<p>💡 <strong>启发点</strong>：可以通过改进前缀和复合词处理，进一步优化算法效果。</p>\n<hr>\n<h2 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h2>\n<p>⚠️ <strong>常见错误1</strong>：忽略基础词表的重要性</p>\n<ul>\n<li>基础词表过小会导致过多无意义的子词生成。</li>\n</ul>\n<p>⚠️ <strong>常见错误2</strong>：不合理设置阈值</p>\n<ul>\n<li>如果合并阈值过低，可能导致训练时间过长或出现低质量子词。</li>\n</ul>\n<p>⚠️ <strong>常见错误3</strong>：未考虑语料质量</p>\n<ul>\n<li>拼写错误、噪声数据会显著影响分词效果。</li>\n</ul>\n<hr>\n<h2 id=\"思考与延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考与延伸问题\"><span>思考与延伸问题</span></a></h2>\n<ol>\n<li>如何在WordPiece中更好地处理拼写错误或前缀问题？</li>\n<li>是否可以结合BPE和WordPiece的优点创建新的分词算法？</li>\n<li>在多语言环境下，WordPiece是否需要特殊优化？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：<a href=\"https://arxiv.org/pdf/2012.15524\" target=\"_blank\" rel=\"noopener noreferrer\">论文《Fast WordPiece Tokenization》</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实现WordPiece分词算法，并测试不同语料下的效果。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 比较BPE与WordPiece在OOV处理上的性能差异。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索基于互信息优化的新型分词方法。</label></li>\n</ul>\n<hr>\n<h2 id=\"后续追踪计划\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪计划\"><span>后续追踪计划</span></a></h2>\n<p>📈 <strong>趋势预测</strong>：随着NLP模型对多语言支持需求增加，更高效、更通用的分词算法将成为研究热点。<br>\n📋 <strong>研究计划</strong>：</p>\n<ol>\n<li>开展对比实验，评估不同分词算法在实际任务中的表现。</li>\n<li>探索如何结合深度学习优化分词过程，例如用Transformer预测子词合并。</li>\n</ol>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理 (NLP)</li>\n<li><strong>标签</strong>：WordPiece, 分词算法, 自然语言处理, NLP模型, Tokenization</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"wordpiece分词算法简介\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece分词算法简介\"><span>WordPiece分词算法简介</span></a></h2>\n<p>WordPiece是一种常见的分词算法，广泛应用于自然语言处理任务中（如BERT模型）。其核心思想与BPE（Byte Pair Encoding）类似，但在合并子词时采用了基于互信息（Mutual Information）的策略，能更好地平衡词表大小和OOV（Out-Of-Vocabulary，未登录词）问题。</p>\n<p>💡 <strong>启发点</strong>：通过互信息优化子词合并，提升了语言模型的表现力。</p>\n<hr>\n<h2 id=\"核心观点与实现步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点与实现步骤\"><span>核心观点与实现步骤</span></a></h2>\n<h3 id=\"wordpiece的核心思想\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece的核心思想\"><span>WordPiece的核心思想</span></a></h3>\n<ul>\n<li>与BPE类似，WordPiece从一个基础词表出发，通过不断合并子词生成最终的词表。</li>\n<li>不同于BPE按频率选择合并对，WordPiece通过计算子词间的互信息来决定合并顺序。</li>\n<li><strong>互信息的公式</strong>：<br>\n假设合并子词 <code v-pre>x</code> 和 <code v-pre>y</code> 后生成新子词 <code v-pre>z</code>，互信息得分计算如下：<div class=\"language-math line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"math\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span>score = P(x) * P(y) / P(z)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>其中，<code v-pre>P(x)</code> 表示子词 <code v-pre>x</code> 在语料中的出现频率。</li>\n</ul>\n<hr>\n<h3 id=\"wordpiece的实现步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#wordpiece的实现步骤\"><span>WordPiece的实现步骤</span></a></h3>\n<p>以下是WordPiece分词的主要步骤，用简单的符号和标记描述：</p>\n<p>✅ <strong>步骤1</strong>：准备基础词表</p>\n<ul>\n<li>包含26个英文字母及常见符号，如<code v-pre>a, b, c, @, #</code>等。</li>\n</ul>\n<p>✅ <strong>步骤2</strong>：将语料拆分为最小单元</p>\n<ul>\n<li>每个单词被拆分为基础字母或符号，例如<code v-pre>hello</code>被拆分为<code v-pre>h, e, l, l, o</code>。</li>\n</ul>\n<p>✅ <strong>步骤3</strong>：训练语言模型</p>\n<ul>\n<li>基于拆分后的数据，使用Unigram语言模型训练子词概率。</li>\n</ul>\n<p>✅ <strong>步骤4</strong>：选择互信息最大的子词对合并</p>\n<ul>\n<li>从所有可能的子词对中选择，使得合并后能最大程度提高语料的概率。</li>\n</ul>\n<p>✅ <strong>步骤5</strong>：重复合并，直到满足条件</p>\n<ul>\n<li>条件可以是达到预设的词表大小或概率增量低于某一阈值。</li>\n</ul>\n<hr>\n<h3 id=\"实现代码片段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#实现代码片段\"><span>实现代码片段</span></a></h3>\n<p>以下是WordPiece中计算互信息得分的核心代码：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 计算子词对的互信息得分</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">scores </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> {</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> freq </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">/</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> (</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">letter_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]]</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> letter_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]])</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pair</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> freq </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pair_freqs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">items</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">}</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">return</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scores</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><hr>\n<h2 id=\"优缺点分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优缺点分析\"><span>优缺点分析</span></a></h2>\n<table>\n<thead>\n<tr>\n<th><strong>优点</strong></th>\n<th><strong>缺点</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>较好地平衡了词表大小和未登录词问题</td>\n<td>可能产生不合理的子词切分</td>\n</tr>\n<tr>\n<td>子词间关联性强，提高语言模型表现</td>\n<td>对拼写错误敏感</td>\n</tr>\n<tr>\n<td>支持高效的语言模型训练</td>\n<td>对前缀处理效果不佳</td>\n</tr>\n</tbody>\n</table>\n<p>💡 <strong>启发点</strong>：可以通过改进前缀和复合词处理，进一步优化算法效果。</p>\n<hr>\n<h2 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h2>\n<p>⚠️ <strong>常见错误1</strong>：忽略基础词表的重要性</p>\n<ul>\n<li>基础词表过小会导致过多无意义的子词生成。</li>\n</ul>\n<p>⚠️ <strong>常见错误2</strong>：不合理设置阈值</p>\n<ul>\n<li>如果合并阈值过低，可能导致训练时间过长或出现低质量子词。</li>\n</ul>\n<p>⚠️ <strong>常见错误3</strong>：未考虑语料质量</p>\n<ul>\n<li>拼写错误、噪声数据会显著影响分词效果。</li>\n</ul>\n<hr>\n<h2 id=\"思考与延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考与延伸问题\"><span>思考与延伸问题</span></a></h2>\n<ol>\n<li>如何在WordPiece中更好地处理拼写错误或前缀问题？</li>\n<li>是否可以结合BPE和WordPiece的优点创建新的分词算法？</li>\n<li>在多语言环境下，WordPiece是否需要特殊优化？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：<a href=\"https://arxiv.org/pdf/2012.15524\" target=\"_blank\" rel=\"noopener noreferrer\">论文《Fast WordPiece Tokenization》</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实现WordPiece分词算法，并测试不同语料下的效果。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 比较BPE与WordPiece在OOV处理上的性能差异。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索基于互信息优化的新型分词方法。</label></li>\n</ul>\n<hr>\n<h2 id=\"后续追踪计划\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪计划\"><span>后续追踪计划</span></a></h2>\n<p>📈 <strong>趋势预测</strong>：随着NLP模型对多语言支持需求增加，更高效、更通用的分词算法将成为研究热点。<br>\n📋 <strong>研究计划</strong>：</p>\n<ol>\n<li>开展对比实验，评估不同分词算法在实际任务中的表现。</li>\n<li>探索如何结合深度学习优化分词过程，例如用Transformer预测子词合并。</li>\n</ol>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 元数据\n- **分类**：自然语言处理 (NLP)\n- **标签**：WordPiece, 分词算法, 自然语言处理, NLP模型, Tokenization\n- **日期**：2025年4月2日  \n\n---\n\n\n\n## WordPiece分词算法简介\nWordPiece是一种常见的分词算法，广泛应用于自然语言处理任务中（如BERT模型）。其核心思想与BPE（Byte Pair Encoding）类似，但在合并子词时采用了基于互信息（Mutual Information）的策略，能更好地平衡词表大小和OOV（Out-Of-Vocabulary，未登录词）问题。\n\n💡 **启发点**：通过互信息优化子词合并，提升了语言模型的表现力。\n\n---\n\n\n\n## 核心观点与实现步骤\n\n### WordPiece的核心思想\n- 与BPE类似，WordPiece从一个基础词表出发，通过不断合并子词生成最终的词表。\n- 不同于BPE按频率选择合并对，WordPiece通过计算子词间的互信息来决定合并顺序。\n- **互信息的公式**：  \n  假设合并子词 `x` 和 `y` 后生成新子词 `z`，互信息得分计算如下：\n  ```math\n  score = P(x) * P(y) / P(z)\n  ```\n  其中，`P(x)` 表示子词 `x` 在语料中的出现频率。\n\n---\n\n\n### WordPiece的实现步骤\n以下是WordPiece分词的主要步骤，用简单的符号和标记描述：\n\n✅ **步骤1**：准备基础词表  \n   - 包含26个英文字母及常见符号，如`a, b, c, @, #`等。\n\n✅ **步骤2**：将语料拆分为最小单元  \n   - 每个单词被拆分为基础字母或符号，例如`hello`被拆分为`h, e, l, l, o`。\n\n✅ **步骤3**：训练语言模型  \n   - 基于拆分后的数据，使用Unigram语言模型训练子词概率。\n\n✅ **步骤4**：选择互信息最大的子词对合并  \n   - 从所有可能的子词对中选择，使得合并后能最大程度提高语料的概率。\n\n✅ **步骤5**：重复合并，直到满足条件  \n   - 条件可以是达到预设的词表大小或概率增量低于某一阈值。\n\n---\n\n\n### 实现代码片段\n以下是WordPiece中计算互信息得分的核心代码：\n\n```python\n# 计算子词对的互信息得分\nscores = {\n    pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n    for pair, freq in pair_freqs.items()\n}\nreturn scores\n```\n\n---\n\n\n\n## 优缺点分析\n| **优点**                             | **缺点**                                   |\n|--------------------------------------|--------------------------------------------|\n| 较好地平衡了词表大小和未登录词问题   | 可能产生不合理的子词切分                  |\n| 子词间关联性强，提高语言模型表现      | 对拼写错误敏感                             |\n| 支持高效的语言模型训练                | 对前缀处理效果不佳                        |\n\n💡 **启发点**：可以通过改进前缀和复合词处理，进一步优化算法效果。\n\n---\n\n\n\n## 常见错误与注意事项\n⚠️ **常见错误1**：忽略基础词表的重要性  \n- 基础词表过小会导致过多无意义的子词生成。\n\n⚠️ **常见错误2**：不合理设置阈值  \n- 如果合并阈值过低，可能导致训练时间过长或出现低质量子词。\n\n⚠️ **常见错误3**：未考虑语料质量  \n- 拼写错误、噪声数据会显著影响分词效果。\n\n---\n\n\n\n## 思考与延伸问题\n1. 如何在WordPiece中更好地处理拼写错误或前缀问题？\n2. 是否可以结合BPE和WordPiece的优点创建新的分词算法？\n3. 在多语言环境下，WordPiece是否需要特殊优化？\n\n---\n\n> **来源**：[论文《Fast WordPiece Tokenization》](https://arxiv.org/pdf/2012.15524)\n\n---\n\n\n\n## 行动清单\n- [ ] 实现WordPiece分词算法，并测试不同语料下的效果。\n- [ ] 比较BPE与WordPiece在OOV处理上的性能差异。\n- [ ] 探索基于互信息优化的新型分词方法。\n\n---\n\n\n\n## 后续追踪计划\n📈 **趋势预测**：随着NLP模型对多语言支持需求增加，更高效、更通用的分词算法将成为研究热点。  \n📋 **研究计划**：\n1. 开展对比实验，评估不同分词算法在实际任务中的表现。\n2. 探索如何结合深度学习优化分词过程，例如用Transformer预测子词合并。","excerpt":"","includedFiles":[],"tasklistId":3,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"WordPiece分词算法简介","slug":"wordpiece分词算法简介","link":"#wordpiece分词算法简介","children":[]},{"level":2,"title":"核心观点与实现步骤","slug":"核心观点与实现步骤","link":"#核心观点与实现步骤","children":[{"level":3,"title":"WordPiece的核心思想","slug":"wordpiece的核心思想","link":"#wordpiece的核心思想","children":[]},{"level":3,"title":"WordPiece的实现步骤","slug":"wordpiece的实现步骤","link":"#wordpiece的实现步骤","children":[]},{"level":3,"title":"实现代码片段","slug":"实现代码片段","link":"#实现代码片段","children":[]}]},{"level":2,"title":"优缺点分析","slug":"优缺点分析","link":"#优缺点分析","children":[]},{"level":2,"title":"常见错误与注意事项","slug":"常见错误与注意事项","link":"#常见错误与注意事项","children":[]},{"level":2,"title":"思考与延伸问题","slug":"思考与延伸问题","link":"#思考与延伸问题","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"后续追踪计划","slug":"后续追踪计划","link":"#后续追踪计划","children":[]}]}}
