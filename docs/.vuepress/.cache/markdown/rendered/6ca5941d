{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能技术</li>\n<li>标签：Prefix Tuning、Prompt Tuning、Transformer、机器学习</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Prefix Tuning是一种通过在输入token前构造任务相关的连续virtual tokens作为Prefix的方法。它在训练过程中只更新Prefix部分的参数，而保持LLM其他部分参数不变。此方法对于不同模型结构需要构造不同的Prefix，并在每层都加上prompt的参数以提高性能。\n<img src=\"/img/user/附件/Pasted image 20250423225341.png\" alt=\"Pasted image 20250423225341.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"prefix-tuning的实现\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#prefix-tuning的实现\"><span>Prefix Tuning的实现</span></a></h3>\n<p>Prefix Tuning通过在输入token前构造任务相关的连续virtual tokens作为Prefix。这些virtual tokens不对应于真实tokens，而是自由参数。在训练过程中，仅更新这些Prefix的参数，保持LLM其他部分参数固定。\n<img src=\"/img/user/附件/Pasted image 20250423225444.png\" alt=\"Pasted image 20250423225444.png\"></p>\n<h3 id=\"应用于不同模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#应用于不同模型结构\"><span>应用于不同模型结构</span></a></h3>\n<ol>\n<li><strong>自回归架构模型</strong>: 在句子前面添加前缀，形成 z = [PREFIX; x; y]。合适的上文能够在固定LM的情况下引导生成下文。</li>\n<li><strong>编码器-解码器架构模型</strong>: Encoder和Decoder都增加了前缀，形成 z = [PREFIX; x; PREFIX0; y]。Encoder端增加前缀是为了引导输入部分的编码，Decoder端增加前缀是为了引导后续token的生成。\n<img src=\"/img/user/附件/Pasted image 20250423225409.png\" alt=\"Pasted image 20250423225409.png\"></li>\n</ol>\n<h3 id=\"防止训练不稳定\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#防止训练不稳定\"><span>防止训练不稳定</span></a></h3>\n<p>为了防止直接更新Prefix参数导致训练不稳定和性能下降，在Prefix层前面加了MLP结构。在训练完成后，仅保留Prefix的参数。此方法通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。</p>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>Prefix</strong>: 在输入数据前添加的一段可调整的虚拟数据，用于优化模型性能。</li>\n<li><strong>MLP结构</strong>: 多层感知器结构，用于处理复杂数据关系。</li>\n<li><strong>消融实验</strong>: 一种实验方法，通过去除某个组件来评估其对系统整体性能的影响。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 构造任务相关的连续virtual tokens作为Prefix。</li>\n<li>⚠ 在训练过程中仅更新Prefix部分参数。</li>\n<li>❗ 在每层都加上prompt的参数以提高性能。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意：直接更新Prefix参数可能导致训练不稳定和性能下降，应在Prefix层前面加MLP结构。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究不同模型结构下Prefix Tuning的具体实现。</li>\n<li>进行消融实验以评估各组件对系统性能的影响。</li>\n<li>探索其他可能的优化策略以提高模型性能。</li>\n</ul>\n<blockquote>\n<p>原始出处：[原文内容]</p>\n</blockquote>\n<p>注意：所有公式或公式字母（如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>max</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\max</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mop\">max</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>lim</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\lim</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mop\">lim</span></span></span></span> 等），一律用块级公式包裹，以便在Obsidian中识别为块级公式。</p>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/PEFT参数高效微调/Prefix-Tuning.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/PEFT参数高效微调/Prefix-Tuning.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/PEFT参数高效微调/Prefix-Tuning","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/PEFT参数高效微调/Prefix-Tuning/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-23T14:48:57.007Z","updated":"2025-04-23T14:54:46.178Z","title":"Prefix-Tuning","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能技术</li>\n<li>标签：Prefix Tuning、Prompt Tuning、Transformer、机器学习</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Prefix Tuning是一种通过在输入token前构造任务相关的连续virtual tokens作为Prefix的方法。它在训练过程中只更新Prefix部分的参数，而保持LLM其他部分参数不变。此方法对于不同模型结构需要构造不同的Prefix，并在每层都加上prompt的参数以提高性能。\n<img src=\"/img/user/附件/Pasted image 20250423225341.png\" alt=\"Pasted image 20250423225341.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"prefix-tuning的实现\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#prefix-tuning的实现\"><span>Prefix Tuning的实现</span></a></h3>\n<p>Prefix Tuning通过在输入token前构造任务相关的连续virtual tokens作为Prefix。这些virtual tokens不对应于真实tokens，而是自由参数。在训练过程中，仅更新这些Prefix的参数，保持LLM其他部分参数固定。\n<img src=\"/img/user/附件/Pasted image 20250423225444.png\" alt=\"Pasted image 20250423225444.png\"></p>\n<h3 id=\"应用于不同模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#应用于不同模型结构\"><span>应用于不同模型结构</span></a></h3>\n<ol>\n<li><strong>自回归架构模型</strong>: 在句子前面添加前缀，形成 z = [PREFIX; x; y]。合适的上文能够在固定LM的情况下引导生成下文。</li>\n<li><strong>编码器-解码器架构模型</strong>: Encoder和Decoder都增加了前缀，形成 z = [PREFIX; x; PREFIX0; y]。Encoder端增加前缀是为了引导输入部分的编码，Decoder端增加前缀是为了引导后续token的生成。\n<img src=\"/img/user/附件/Pasted image 20250423225409.png\" alt=\"Pasted image 20250423225409.png\"></li>\n</ol>\n<h3 id=\"防止训练不稳定\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#防止训练不稳定\"><span>防止训练不稳定</span></a></h3>\n<p>为了防止直接更新Prefix参数导致训练不稳定和性能下降，在Prefix层前面加了MLP结构。在训练完成后，仅保留Prefix的参数。此方法通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。</p>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>Prefix</strong>: 在输入数据前添加的一段可调整的虚拟数据，用于优化模型性能。</li>\n<li><strong>MLP结构</strong>: 多层感知器结构，用于处理复杂数据关系。</li>\n<li><strong>消融实验</strong>: 一种实验方法，通过去除某个组件来评估其对系统整体性能的影响。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 构造任务相关的连续virtual tokens作为Prefix。</li>\n<li>⚠ 在训练过程中仅更新Prefix部分参数。</li>\n<li>❗ 在每层都加上prompt的参数以提高性能。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意：直接更新Prefix参数可能导致训练不稳定和性能下降，应在Prefix层前面加MLP结构。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究不同模型结构下Prefix Tuning的具体实现。</li>\n<li>进行消融实验以评估各组件对系统性能的影响。</li>\n<li>探索其他可能的优化策略以提高模型性能。</li>\n</ul>\n<blockquote>\n<p>原始出处：[原文内容]</p>\n</blockquote>\n<p>注意：所有公式或公式字母（如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>max</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\max</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mop\">max</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>lim</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\lim</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mop\">lim</span></span></span></span> 等），一律用块级公式包裹，以便在Obsidian中识别为块级公式。</p>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能技术</li>\n<li>标签：Prefix Tuning、Prompt Tuning、Transformer、机器学习</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Prefix Tuning是一种通过在输入token前构造任务相关的连续virtual tokens作为Prefix的方法。它在训练过程中只更新Prefix部分的参数，而保持LLM其他部分参数不变。此方法对于不同模型结构需要构造不同的Prefix，并在每层都加上prompt的参数以提高性能。\n<img src=\"/img/user/附件/Pasted image 20250423225341.png\" alt=\"Pasted image 20250423225341.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"prefix-tuning的实现\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#prefix-tuning的实现\"><span>Prefix Tuning的实现</span></a></h3>\n<p>Prefix Tuning通过在输入token前构造任务相关的连续virtual tokens作为Prefix。这些virtual tokens不对应于真实tokens，而是自由参数。在训练过程中，仅更新这些Prefix的参数，保持LLM其他部分参数固定。\n<img src=\"/img/user/附件/Pasted image 20250423225444.png\" alt=\"Pasted image 20250423225444.png\"></p>\n<h3 id=\"应用于不同模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#应用于不同模型结构\"><span>应用于不同模型结构</span></a></h3>\n<ol>\n<li><strong>自回归架构模型</strong>: 在句子前面添加前缀，形成 z = [PREFIX; x; y]。合适的上文能够在固定LM的情况下引导生成下文。</li>\n<li><strong>编码器-解码器架构模型</strong>: Encoder和Decoder都增加了前缀，形成 z = [PREFIX; x; PREFIX0; y]。Encoder端增加前缀是为了引导输入部分的编码，Decoder端增加前缀是为了引导后续token的生成。\n<img src=\"/img/user/附件/Pasted image 20250423225409.png\" alt=\"Pasted image 20250423225409.png\"></li>\n</ol>\n<h3 id=\"防止训练不稳定\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#防止训练不稳定\"><span>防止训练不稳定</span></a></h3>\n<p>为了防止直接更新Prefix参数导致训练不稳定和性能下降，在Prefix层前面加了MLP结构。在训练完成后，仅保留Prefix的参数。此方法通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。</p>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>Prefix</strong>: 在输入数据前添加的一段可调整的虚拟数据，用于优化模型性能。</li>\n<li><strong>MLP结构</strong>: 多层感知器结构，用于处理复杂数据关系。</li>\n<li><strong>消融实验</strong>: 一种实验方法，通过去除某个组件来评估其对系统整体性能的影响。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 构造任务相关的连续virtual tokens作为Prefix。</li>\n<li>⚠ 在训练过程中仅更新Prefix部分参数。</li>\n<li>❗ 在每层都加上prompt的参数以提高性能。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意：直接更新Prefix参数可能导致训练不稳定和性能下降，应在Prefix层前面加MLP结构。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究不同模型结构下Prefix Tuning的具体实现。</li>\n<li>进行消融实验以评估各组件对系统性能的影响。</li>\n<li>探索其他可能的优化策略以提高模型性能。</li>\n</ul>\n<blockquote>\n<p>原始出处：[原文内容]</p>\n</blockquote>\n<p>注意：所有公式或公式字母（如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>max</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\max</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mop\">max</span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>lim</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\lim</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mop\">lim</span></span></span></span> 等），一律用块级公式包裹，以便在Obsidian中识别为块级公式。</p>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：人工智能技术\n- 标签：Prefix Tuning、Prompt Tuning、Transformer、机器学习\n- 日期：2025年4月12日\n\n\n## 核心观点总结\nPrefix Tuning是一种通过在输入token前构造任务相关的连续virtual tokens作为Prefix的方法。它在训练过程中只更新Prefix部分的参数，而保持LLM其他部分参数不变。此方法对于不同模型结构需要构造不同的Prefix，并在每层都加上prompt的参数以提高性能。\n![Pasted image 20250423225341.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250423225341.png)\n\n\n## 重点段落\n\n### Prefix Tuning的实现\nPrefix Tuning通过在输入token前构造任务相关的连续virtual tokens作为Prefix。这些virtual tokens不对应于真实tokens，而是自由参数。在训练过程中，仅更新这些Prefix的参数，保持LLM其他部分参数固定。\n![Pasted image 20250423225444.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250423225444.png)\n\n\n### 应用于不同模型结构\n1. **自回归架构模型**: 在句子前面添加前缀，形成 z = [PREFIX; x; y]。合适的上文能够在固定LM的情况下引导生成下文。\n2. **编码器-解码器架构模型**: Encoder和Decoder都增加了前缀，形成 z = [PREFIX; x; PREFIX0; y]。Encoder端增加前缀是为了引导输入部分的编码，Decoder端增加前缀是为了引导后续token的生成。\n![Pasted image 20250423225409.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250423225409.png)\n\n\n### 防止训练不稳定\n为了防止直接更新Prefix参数导致训练不稳定和性能下降，在Prefix层前面加了MLP结构。在训练完成后，仅保留Prefix的参数。此方法通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。\n\n\n## 技术术语通俗解释\n- **Prefix**: 在输入数据前添加的一段可调整的虚拟数据，用于优化模型性能。\n- **MLP结构**: 多层感知器结构，用于处理复杂数据关系。\n- **消融实验**: 一种实验方法，通过去除某个组件来评估其对系统整体性能的影响。\n\n\n## 操作步骤\n1. ✅ 构造任务相关的连续virtual tokens作为Prefix。\n2. ⚠ 在训练过程中仅更新Prefix部分参数。\n3. ❗ 在每层都加上prompt的参数以提高性能。\n\n\n## 常见错误\n> 注意：直接更新Prefix参数可能导致训练不稳定和性能下降，应在Prefix层前面加MLP结构。\n\n\n## 💡启发点\n通过消融实验验证，仅调整embedding层表现力不够，会导致性能显著下降，因此在每层都加了prompt的参数。\n\n\n## 行动清单\n- 研究不同模型结构下Prefix Tuning的具体实现。\n- 进行消融实验以评估各组件对系统性能的影响。\n- 探索其他可能的优化策略以提高模型性能。\n\n> 原始出处：[原文内容]\n\n注意：所有公式或公式字母（如 $\\alpha$、$\\beta$、$\\max$、$\\lim$ 等），一律用块级公式包裹，以便在Obsidian中识别为块级公式。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[{"level":3,"title":"Prefix Tuning的实现","slug":"prefix-tuning的实现","link":"#prefix-tuning的实现","children":[]},{"level":3,"title":"应用于不同模型结构","slug":"应用于不同模型结构","link":"#应用于不同模型结构","children":[]},{"level":3,"title":"防止训练不稳定","slug":"防止训练不稳定","link":"#防止训练不稳定","children":[]}]},{"level":2,"title":"技术术语通俗解释","slug":"技术术语通俗解释","link":"#技术术语通俗解释","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
