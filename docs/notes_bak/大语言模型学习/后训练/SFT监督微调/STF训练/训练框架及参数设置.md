---
dg-publish: true
dg-permalink: /大语言模型学习/后训练/SFT监督微调/STF训练/训练框架及参数设置
dg-home: false
dg-description: 在此输入笔记的描述
dg-hide: false
dg-hide-title: false
dg-show-backlinks: true
dg-show-local-graph: true
dg-show-inline-title: true
dg-pinned: false
dg-passphrase: 在此输入访问密码
dg-enable-mathjax: false
dg-enable-mermaid: false
dg-enable-uml: false
dg-note-icon: 0
dg-enable-dataview: false
tags:
  - NLP
permalink: /大语言模型学习/后训练/SFT监督微调/STF训练/训练框架及参数设置/
dgShowBacklinks: true
dgShowLocalGraph: true
dgShowInlineTitle: true
dgPassFrontmatter: true
noteIcon: 0
created: 2025-04-11T11:00:48.000+08:00
updated: 2025-04-13T13:06:02.000+08:00
title: 训练框架及参数设置
createTime: 2025/05/13 17:33:52
---



# SFT训练指南：优化策略与参数设置
**分类**：机器学习

**标签**：SFT训练，OpenRLHF，DeepSpeed

**日期**：2023年10月27日

## 综述
在SFT（监督微调）训练过程中，通常我们不会对损失函数和训练策略进行大幅修改，而是通过调整一些关键参数来优化训练效果。这些参数包括 `checkpoint_path`、`model_path`、`data_path`、`dp`（数据并行）、`pp`（流水线并行）和 `lr`（学习率）。推荐使用OpenRLHF框架，基于Ray和DeepSpeed，简单易用。


## 参数设置与注意事项
1. **epoch设置**：
   - 通常设置为1个epoch。
   - 如果数据量较小（如1万条以内），可以设为3个epoch以避免过拟合。

2. **梯度累积与批量大小**：
   - `gradient_accumulation_steps`：指在更新模型参数前，梯度会在多少个小批次上累积。
   - 全局批量大小计算公式：
     $$
     \text{global\_batch\_size} = \text{gradient\_accumulation\_steps} \times \text{per\_device\_batch\_size} \times \text{num\_devices}
     $$

3. **学习率与调度器**：
   - SFT阶段的学习率一般是预训练阶段的10倍。
   - 常用的学习率调度器类型有：`constant`、`linear`、`cosine`、`exponential`，其中 `cosine` 使用较多。


## 技术细节
- **梯度检查点**：
  使用 PyTorch 的 `torch.utils.checkpoint.checkpoint` 和 `torch.utils.checkpoint.checkpoint_sequential` 可以降低内存使用量。其原理是保存模型每个函数的输入元组，在反向传播时重新计算。

- **DeepSpeed设置**：
  - `zero_stage`：一般设置为 `zero2`，以平衡显存占用和训练速度。
  - `max_seq_len`：通常设为4K。
  - `offload`：通常不设置，以免影响训练效率。


## 常见错误
> ⚠ 在使用DeepSpeed时，确保正确配置`zero_stage`，以避免因带宽通信成本导致的训练速度减慢。


## 💡启发点
- 通过合理设置 `epoch` 和 `gradient_accumulation_steps`，可以在有限资源下最大化训练效率。
- 使用 `cosine` 学习率调度器通常能带来更稳定的收敛效果。


## [思考] 未来展望与问题
- 如何在更大规模的数据集上优化SFT训练效率？
- 是否有可能通过自动化工具进一步简化参数调整过程？
- 在不同领域模型微调时，是否有通用的参数优化策略？

> 来源：本文内容基于对SFT训练的技术分析与实践经验总结。


## 行动清单
- [ ] 探索OpenRLHF框架的更多应用场景。
- [ ] 实验不同学习率调度器对训练效果的影响。
- [ ] 研究DeepSpeed的最新更新与优化功能。


## 📈趋势预测
随着深度学习框架的不断发展，预计未来SFT训练将更加高效，并且会有更多自动化工具来简化参数调整过程。


## 后续追踪
- 关注DeepSpeed在处理大规模模型训练中的新特性。
- 跟踪OpenRLHF框架在社区中的应用案例。
