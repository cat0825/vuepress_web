{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p>分类：人工智能研究</p>\n<p>标签：Qwen2.5模型，长文本处理，预训练数据</p>\n<p>日期：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Qwen2.5是一系列专注于长文本处理的高性能模型，采用多阶段预训练方法和先进的数据过滤技术，旨在提升处理长文本的能力。其预训练数据经过精细过滤，确保高质量输入，并通过渐进式训练适应不同长度的上下文。模型系列包括多个参数规模的版本，从0.5B到72B不等。长序列生成和数学推理是其后训练的重点领域。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"模型系列与结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型系列与结构\"><span>模型系列与结构</span></a></h3>\n<p>Qwen2.5包含多个参数规模的模型，包括base和instruct版本，以及MoE模型如Qwen2.5-Turbo和Qwen2.5-Plus。其结构采用SwiGLU、RoPE、QKV bias、RMSNorm、GQA + YaRN + DCA，与前代模型一致。Tokenizer使用BBPE，词表大小为151643。</p>\n<h3 id=\"预训练数据与方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练数据与方法\"><span>预训练数据与方法</span></a></h3>\n<p>使用18T tokens量级的数据进行预训练，数据经过Qwen2-Instruct模型过滤以确保质量。加入了专门的数学与代码数据，并对合成数据进行了严格过滤和奖励机制评估。数据混合策略通过对不同领域内容进行分类与平衡，以确保高质量信息的代表性。</p>\n<h3 id=\"长文本预训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#长文本预训练\"><span>长文本预训练</span></a></h3>\n<p>采用两阶段预训练方法：初始阶段使用4K token上下文长度，最终阶段扩展至32K token。Qwen2.5-Turbo经过四个阶段训练，最终达到256K token，能够处理最多1M个token。渐进式方法帮助模型适应增加的上下文长度，并应用YARN和DCA技术以扩展处理能力。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始预训练使用4K token上下文长度。</li>\n<li>⚠ 最终阶段扩展至32K token。</li>\n<li>❗ Qwen2.5-Turbo经过四个阶段达到256K token。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在数据混合过程中，需避免过度代表的领域影响整体数据质量，确保高价值领域得到足够重视。</p>\n</blockquote>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型系列</th>\n<th>参数规模</th>\n<th>上下文长度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Qwen2.5</td>\n<td>0.5B-72B</td>\n<td>4K-32K</td>\n</tr>\n<tr>\n<td>Turbo</td>\n<td>MoE</td>\n<td>256K</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>通过渐进式训练方法和先进的数据过滤技术，Qwen2.5实现了对长文本的高效处理能力，显著提升了模型的广泛适用性。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>进一步研究Qwen2.5在不同领域的应用效果。</li>\n<li>探索更多数据过滤和奖励机制以提升样本质量。</li>\n<li>扩展长文本处理能力至更多实际场景。</li>\n</ul>\n<blockquote>\n<p>来源：Qwen2.5 Technical Report, https://arxiv.org/pdf/2412.15115</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/Qwen系列/Qwen2.5.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/Qwen系列/Qwen2.5.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/Qwen系列/Qwen2.5","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/Qwen系列/Qwen2.5/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-25T03:16:40.000Z","updated":"2025-04-25T03:17:25.000Z","title":"Qwen2.5","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p>分类：人工智能研究</p>\n<p>标签：Qwen2.5模型，长文本处理，预训练数据</p>\n<p>日期：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Qwen2.5是一系列专注于长文本处理的高性能模型，采用多阶段预训练方法和先进的数据过滤技术，旨在提升处理长文本的能力。其预训练数据经过精细过滤，确保高质量输入，并通过渐进式训练适应不同长度的上下文。模型系列包括多个参数规模的版本，从0.5B到72B不等。长序列生成和数学推理是其后训练的重点领域。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"模型系列与结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型系列与结构\"><span>模型系列与结构</span></a></h3>\n<p>Qwen2.5包含多个参数规模的模型，包括base和instruct版本，以及MoE模型如Qwen2.5-Turbo和Qwen2.5-Plus。其结构采用SwiGLU、RoPE、QKV bias、RMSNorm、GQA + YaRN + DCA，与前代模型一致。Tokenizer使用BBPE，词表大小为151643。</p>\n<h3 id=\"预训练数据与方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练数据与方法\"><span>预训练数据与方法</span></a></h3>\n<p>使用18T tokens量级的数据进行预训练，数据经过Qwen2-Instruct模型过滤以确保质量。加入了专门的数学与代码数据，并对合成数据进行了严格过滤和奖励机制评估。数据混合策略通过对不同领域内容进行分类与平衡，以确保高质量信息的代表性。</p>\n<h3 id=\"长文本预训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#长文本预训练\"><span>长文本预训练</span></a></h3>\n<p>采用两阶段预训练方法：初始阶段使用4K token上下文长度，最终阶段扩展至32K token。Qwen2.5-Turbo经过四个阶段训练，最终达到256K token，能够处理最多1M个token。渐进式方法帮助模型适应增加的上下文长度，并应用YARN和DCA技术以扩展处理能力。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始预训练使用4K token上下文长度。</li>\n<li>⚠ 最终阶段扩展至32K token。</li>\n<li>❗ Qwen2.5-Turbo经过四个阶段达到256K token。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在数据混合过程中，需避免过度代表的领域影响整体数据质量，确保高价值领域得到足够重视。</p>\n</blockquote>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型系列</th>\n<th>参数规模</th>\n<th>上下文长度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Qwen2.5</td>\n<td>0.5B-72B</td>\n<td>4K-32K</td>\n</tr>\n<tr>\n<td>Turbo</td>\n<td>MoE</td>\n<td>256K</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>通过渐进式训练方法和先进的数据过滤技术，Qwen2.5实现了对长文本的高效处理能力，显著提升了模型的广泛适用性。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>进一步研究Qwen2.5在不同领域的应用效果。</li>\n<li>探索更多数据过滤和奖励机制以提升样本质量。</li>\n<li>扩展长文本处理能力至更多实际场景。</li>\n</ul>\n<blockquote>\n<p>来源：Qwen2.5 Technical Report, https://arxiv.org/pdf/2412.15115</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p>分类：人工智能研究</p>\n<p>标签：Qwen2.5模型，长文本处理，预训练数据</p>\n<p>日期：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Qwen2.5是一系列专注于长文本处理的高性能模型，采用多阶段预训练方法和先进的数据过滤技术，旨在提升处理长文本的能力。其预训练数据经过精细过滤，确保高质量输入，并通过渐进式训练适应不同长度的上下文。模型系列包括多个参数规模的版本，从0.5B到72B不等。长序列生成和数学推理是其后训练的重点领域。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"模型系列与结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型系列与结构\"><span>模型系列与结构</span></a></h3>\n<p>Qwen2.5包含多个参数规模的模型，包括base和instruct版本，以及MoE模型如Qwen2.5-Turbo和Qwen2.5-Plus。其结构采用SwiGLU、RoPE、QKV bias、RMSNorm、GQA + YaRN + DCA，与前代模型一致。Tokenizer使用BBPE，词表大小为151643。</p>\n<h3 id=\"预训练数据与方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练数据与方法\"><span>预训练数据与方法</span></a></h3>\n<p>使用18T tokens量级的数据进行预训练，数据经过Qwen2-Instruct模型过滤以确保质量。加入了专门的数学与代码数据，并对合成数据进行了严格过滤和奖励机制评估。数据混合策略通过对不同领域内容进行分类与平衡，以确保高质量信息的代表性。</p>\n<h3 id=\"长文本预训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#长文本预训练\"><span>长文本预训练</span></a></h3>\n<p>采用两阶段预训练方法：初始阶段使用4K token上下文长度，最终阶段扩展至32K token。Qwen2.5-Turbo经过四个阶段训练，最终达到256K token，能够处理最多1M个token。渐进式方法帮助模型适应增加的上下文长度，并应用YARN和DCA技术以扩展处理能力。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始预训练使用4K token上下文长度。</li>\n<li>⚠ 最终阶段扩展至32K token。</li>\n<li>❗ Qwen2.5-Turbo经过四个阶段达到256K token。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在数据混合过程中，需避免过度代表的领域影响整体数据质量，确保高价值领域得到足够重视。</p>\n</blockquote>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型系列</th>\n<th>参数规模</th>\n<th>上下文长度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Qwen2.5</td>\n<td>0.5B-72B</td>\n<td>4K-32K</td>\n</tr>\n<tr>\n<td>Turbo</td>\n<td>MoE</td>\n<td>256K</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>通过渐进式训练方法和先进的数据过滤技术，Qwen2.5实现了对长文本的高效处理能力，显著提升了模型的广泛适用性。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>进一步研究Qwen2.5在不同领域的应用效果。</li>\n<li>探索更多数据过滤和奖励机制以提升样本质量。</li>\n<li>扩展长文本处理能力至更多实际场景。</li>\n</ul>\n<blockquote>\n<p>来源：Qwen2.5 Technical Report, https://arxiv.org/pdf/2412.15115</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n分类：人工智能研究\n\n标签：Qwen2.5模型，长文本处理，预训练数据\n\n日期：2025年4月12日\n\n\n## 核心观点总结\nQwen2.5是一系列专注于长文本处理的高性能模型，采用多阶段预训练方法和先进的数据过滤技术，旨在提升处理长文本的能力。其预训练数据经过精细过滤，确保高质量输入，并通过渐进式训练适应不同长度的上下文。模型系列包括多个参数规模的版本，从0.5B到72B不等。长序列生成和数学推理是其后训练的重点领域。\n\n\n## 重点段落\n\n### 模型系列与结构\nQwen2.5包含多个参数规模的模型，包括base和instruct版本，以及MoE模型如Qwen2.5-Turbo和Qwen2.5-Plus。其结构采用SwiGLU、RoPE、QKV bias、RMSNorm、GQA + YaRN + DCA，与前代模型一致。Tokenizer使用BBPE，词表大小为151643。\n\n\n### 预训练数据与方法\n使用18T tokens量级的数据进行预训练，数据经过Qwen2-Instruct模型过滤以确保质量。加入了专门的数学与代码数据，并对合成数据进行了严格过滤和奖励机制评估。数据混合策略通过对不同领域内容进行分类与平衡，以确保高质量信息的代表性。\n\n\n### 长文本预训练\n采用两阶段预训练方法：初始阶段使用4K token上下文长度，最终阶段扩展至32K token。Qwen2.5-Turbo经过四个阶段训练，最终达到256K token，能够处理最多1M个token。渐进式方法帮助模型适应增加的上下文长度，并应用YARN和DCA技术以扩展处理能力。\n\n\n## 操作步骤\n1. ✅ 初始预训练使用4K token上下文长度。\n2. ⚠ 最终阶段扩展至32K token。\n3. ❗ Qwen2.5-Turbo经过四个阶段达到256K token。\n\n\n## 常见错误\n> 警告：在数据混合过程中，需避免过度代表的领域影响整体数据质量，确保高价值领域得到足够重视。\n\n\n## 数据表格\n| 模型系列 | 参数规模 | 上下文长度 |\n|----------|----------|------------|\n| Qwen2.5  | 0.5B-72B | 4K-32K     |\n| Turbo    | MoE      | 256K       |\n\n\n## 💡启发点\n通过渐进式训练方法和先进的数据过滤技术，Qwen2.5实现了对长文本的高效处理能力，显著提升了模型的广泛适用性。\n\n\n## 行动清单\n- 进一步研究Qwen2.5在不同领域的应用效果。\n- 探索更多数据过滤和奖励机制以提升样本质量。\n- 扩展长文本处理能力至更多实际场景。\n\n> 来源：Qwen2.5 Technical Report, https://arxiv.org/pdf/2412.15115","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[{"level":3,"title":"模型系列与结构","slug":"模型系列与结构","link":"#模型系列与结构","children":[]},{"level":3,"title":"预训练数据与方法","slug":"预训练数据与方法","link":"#预训练数据与方法","children":[]},{"level":3,"title":"长文本预训练","slug":"长文本预训练","link":"#长文本预训练","children":[]}]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"数据表格","slug":"数据表格","link":"#数据表格","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
