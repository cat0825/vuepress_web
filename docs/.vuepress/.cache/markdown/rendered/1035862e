{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：MDP模型，深度学习，奖励机制</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>在自然语言处理（NLP）任务中，Prompt到Response的过程可以被看作是马尔可夫决策过程（MDP）的一个实例。本文探讨了将这一过程视为单步MDP和多步MDP的不同视角，并分析了奖励机制的设计。</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<ol>\n<li>\n<p><strong>单步MDP视角</strong>：在这种视角下，初始状态是Prompt，动作是Response，二者皆为token序列。此时，没有状态转移，而是直接根据当前状态和动作给予奖励。这种视角采用了AGG聚合操作中的-1操作。</p>\n</li>\n<li>\n<p><strong>多步MDP视角</strong>：在这一视角中，每个token被视为一个动作，状态则由当前动作和之前所有token拼接而成。整个过程被看作一个整体，最后一个token位置的输出奖励值聚合了整个句子的信息。</p>\n</li>\n<li>\n<p><strong>句子级别奖励的必要性</strong>：由于偏好标签是基于句子级别的，因此需要采用代表整个句子的奖励值。</p>\n</li>\n</ol>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"单步mdp模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#单步mdp模型\"><span>单步MDP模型</span></a></h3>\n<p>在单步MDP中，初始状态 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">s_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 为Prompt，动作 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">a_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 为Response。两者都是token序列，但与传统NLP RL MDP不同，这里的动作是一个完整的token序列。奖励函数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(s_0, a_0)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 即为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(x, y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>。</p>\n<h3 id=\"多步mdp模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多步mdp模型\"><span>多步MDP模型</span></a></h3>\n<p>多步MDP将[prompt, response]中的每个token视为一个动作，状态为当前动作与之前token的序列。此时，最后一个token位置的输出奖励值可视作聚合整个句子信息的Transformer操作。</p>\n<h3 id=\"奖励机制设计\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#奖励机制设计\"><span>奖励机制设计</span></a></h3>\n<p>由于偏好标签是基于句子级别的，因此需要设计能够代表整个句子的奖励值。在PRM过程中，也会有中间过程的偏好标签。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 将prompt视为初始状态，response视为动作。</li>\n<li>⚠ 在单步MDP中，不进行状态转移，直接给予奖励。</li>\n<li>❗ 在多步MDP中，将每个token视为动作，聚合信息给予奖励。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在设计奖励机制时，容易忽略句子级别的偏好标签，从而导致奖励值不准确。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>使用MDP模型分析NLP任务中的prompt-response过程，为奖励机制设计提供了新的思路。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更多关于MDP模型在NLP中的应用。</li>\n<li>研究如何优化奖励机制以提高模型性能。</li>\n</ul>\n<hr>\n<blockquote>\n<p>原始出处：[来源未提供]</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/深入理解Prompt到Response的MDP模型分析.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/深入理解Prompt到Response的MDP模型分析.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/LLM对齐下的RLHF+PPO/深入理解Prompt到Response的MDP模型分析","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/LLM对齐下的RLHF+PPO/深入理解Prompt到Response的MDP模型分析/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-16T13:10:16.000Z","updated":"2025-04-17T01:02:24.000Z","title":"深入理解Prompt到Response的MDP模型分析","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：MDP模型，深度学习，奖励机制</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>在自然语言处理（NLP）任务中，Prompt到Response的过程可以被看作是马尔可夫决策过程（MDP）的一个实例。本文探讨了将这一过程视为单步MDP和多步MDP的不同视角，并分析了奖励机制的设计。</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<ol>\n<li>\n<p><strong>单步MDP视角</strong>：在这种视角下，初始状态是Prompt，动作是Response，二者皆为token序列。此时，没有状态转移，而是直接根据当前状态和动作给予奖励。这种视角采用了AGG聚合操作中的-1操作。</p>\n</li>\n<li>\n<p><strong>多步MDP视角</strong>：在这一视角中，每个token被视为一个动作，状态则由当前动作和之前所有token拼接而成。整个过程被看作一个整体，最后一个token位置的输出奖励值聚合了整个句子的信息。</p>\n</li>\n<li>\n<p><strong>句子级别奖励的必要性</strong>：由于偏好标签是基于句子级别的，因此需要采用代表整个句子的奖励值。</p>\n</li>\n</ol>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"单步mdp模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#单步mdp模型\"><span>单步MDP模型</span></a></h3>\n<p>在单步MDP中，初始状态 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">s_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 为Prompt，动作 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">a_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 为Response。两者都是token序列，但与传统NLP RL MDP不同，这里的动作是一个完整的token序列。奖励函数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(s_0, a_0)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 即为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(x, y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>。</p>\n<h3 id=\"多步mdp模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多步mdp模型\"><span>多步MDP模型</span></a></h3>\n<p>多步MDP将[prompt, response]中的每个token视为一个动作，状态为当前动作与之前token的序列。此时，最后一个token位置的输出奖励值可视作聚合整个句子信息的Transformer操作。</p>\n<h3 id=\"奖励机制设计\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#奖励机制设计\"><span>奖励机制设计</span></a></h3>\n<p>由于偏好标签是基于句子级别的，因此需要设计能够代表整个句子的奖励值。在PRM过程中，也会有中间过程的偏好标签。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 将prompt视为初始状态，response视为动作。</li>\n<li>⚠ 在单步MDP中，不进行状态转移，直接给予奖励。</li>\n<li>❗ 在多步MDP中，将每个token视为动作，聚合信息给予奖励。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在设计奖励机制时，容易忽略句子级别的偏好标签，从而导致奖励值不准确。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>使用MDP模型分析NLP任务中的prompt-response过程，为奖励机制设计提供了新的思路。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更多关于MDP模型在NLP中的应用。</li>\n<li>研究如何优化奖励机制以提高模型性能。</li>\n</ul>\n<hr>\n<blockquote>\n<p>原始出处：[来源未提供]</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：MDP模型，深度学习，奖励机制</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>在自然语言处理（NLP）任务中，Prompt到Response的过程可以被看作是马尔可夫决策过程（MDP）的一个实例。本文探讨了将这一过程视为单步MDP和多步MDP的不同视角，并分析了奖励机制的设计。</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<ol>\n<li>\n<p><strong>单步MDP视角</strong>：在这种视角下，初始状态是Prompt，动作是Response，二者皆为token序列。此时，没有状态转移，而是直接根据当前状态和动作给予奖励。这种视角采用了AGG聚合操作中的-1操作。</p>\n</li>\n<li>\n<p><strong>多步MDP视角</strong>：在这一视角中，每个token被视为一个动作，状态则由当前动作和之前所有token拼接而成。整个过程被看作一个整体，最后一个token位置的输出奖励值聚合了整个句子的信息。</p>\n</li>\n<li>\n<p><strong>句子级别奖励的必要性</strong>：由于偏好标签是基于句子级别的，因此需要采用代表整个句子的奖励值。</p>\n</li>\n</ol>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"单步mdp模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#单步mdp模型\"><span>单步MDP模型</span></a></h3>\n<p>在单步MDP中，初始状态 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">s_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 为Prompt，动作 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">a_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 为Response。两者都是token序列，但与传统NLP RL MDP不同，这里的动作是一个完整的token序列。奖励函数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(s_0, a_0)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 即为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(x, y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>。</p>\n<h3 id=\"多步mdp模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多步mdp模型\"><span>多步MDP模型</span></a></h3>\n<p>多步MDP将[prompt, response]中的每个token视为一个动作，状态为当前动作与之前token的序列。此时，最后一个token位置的输出奖励值可视作聚合整个句子信息的Transformer操作。</p>\n<h3 id=\"奖励机制设计\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#奖励机制设计\"><span>奖励机制设计</span></a></h3>\n<p>由于偏好标签是基于句子级别的，因此需要设计能够代表整个句子的奖励值。在PRM过程中，也会有中间过程的偏好标签。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 将prompt视为初始状态，response视为动作。</li>\n<li>⚠ 在单步MDP中，不进行状态转移，直接给予奖励。</li>\n<li>❗ 在多步MDP中，将每个token视为动作，聚合信息给予奖励。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在设计奖励机制时，容易忽略句子级别的偏好标签，从而导致奖励值不准确。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>使用MDP模型分析NLP任务中的prompt-response过程，为奖励机制设计提供了新的思路。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更多关于MDP模型在NLP中的应用。</li>\n<li>研究如何优化奖励机制以提高模型性能。</li>\n</ul>\n<hr>\n<blockquote>\n<p>原始出处：[来源未提供]</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：自然语言处理\n- 标签：MDP模型，深度学习，奖励机制\n- 日期：2025年4月12日\n\n\n## 内容概述\n在自然语言处理（NLP）任务中，Prompt到Response的过程可以被看作是马尔可夫决策过程（MDP）的一个实例。本文探讨了将这一过程视为单步MDP和多步MDP的不同视角，并分析了奖励机制的设计。\n\n\n## 核心观点总结\n1. **单步MDP视角**：在这种视角下，初始状态是Prompt，动作是Response，二者皆为token序列。此时，没有状态转移，而是直接根据当前状态和动作给予奖励。这种视角采用了AGG聚合操作中的-1操作。\n\n2. **多步MDP视角**：在这一视角中，每个token被视为一个动作，状态则由当前动作和之前所有token拼接而成。整个过程被看作一个整体，最后一个token位置的输出奖励值聚合了整个句子的信息。\n\n3. **句子级别奖励的必要性**：由于偏好标签是基于句子级别的，因此需要采用代表整个句子的奖励值。\n\n\n## 重点段落\n\n### 单步MDP模型\n在单步MDP中，初始状态 $s_0$ 为Prompt，动作 $a_0$ 为Response。两者都是token序列，但与传统NLP RL MDP不同，这里的动作是一个完整的token序列。奖励函数 $r(s_0, a_0)$ 即为 $r(x, y)$。\n\n\n### 多步MDP模型\n多步MDP将[prompt, response]中的每个token视为一个动作，状态为当前动作与之前token的序列。此时，最后一个token位置的输出奖励值可视作聚合整个句子信息的Transformer操作。\n\n\n### 奖励机制设计\n由于偏好标签是基于句子级别的，因此需要设计能够代表整个句子的奖励值。在PRM过程中，也会有中间过程的偏好标签。\n\n\n## 操作步骤\n1. ✅ 将prompt视为初始状态，response视为动作。\n2. ⚠ 在单步MDP中，不进行状态转移，直接给予奖励。\n3. ❗ 在多步MDP中，将每个token视为动作，聚合信息给予奖励。\n\n\n## 常见错误\n> 在设计奖励机制时，容易忽略句子级别的偏好标签，从而导致奖励值不准确。\n\n\n## 💡 启发点\n- 使用MDP模型分析NLP任务中的prompt-response过程，为奖励机制设计提供了新的思路。\n\n\n## 行动清单\n- 探索更多关于MDP模型在NLP中的应用。\n- 研究如何优化奖励机制以提高模型性能。\n\n---\n\n> 原始出处：[来源未提供]","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容概述","slug":"内容概述","link":"#内容概述","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[{"level":3,"title":"单步MDP模型","slug":"单步mdp模型","link":"#单步mdp模型","children":[]},{"level":3,"title":"多步MDP模型","slug":"多步mdp模型","link":"#多步mdp模型","children":[]},{"level":3,"title":"奖励机制设计","slug":"奖励机制设计","link":"#奖励机制设计","children":[]}]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
