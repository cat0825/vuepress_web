{"content":"<p class=\"block-language-dataview\"></p>\n<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：人工智能、深度学习</li>\n<li><strong>标签</strong>：长上下文模型、注意力机制、LongLoRA、优化算法</li>\n<li><strong>日期</strong>：2024年10月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>在长上下文语言模型的优化中，论文《LongLoRA: Efficient Fine-Tuning of Long Context Large Language Models》提出了一种名为**Shifted Sparse Attention (S2-Attention)**的方法。该技术通过分组计算注意力并引入移位机制，实现了长上下文训练的高效性，同时避免了信息泄漏问题。</p>\n<hr>\n<h2 id=\"技术解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术解析\"><span>技术解析</span></a></h2>\n<h3 id=\"✅-核心原理-shifted-sparse-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-核心原理-shifted-sparse-attention\"><span>✅ 核心原理：Shifted Sparse Attention</span></a></h3>\n<ul>\n<li><strong>分组处理</strong>：将上下文长度划分为多个组，每组单独计算注意力。</li>\n<li><strong>移位机制</strong>：在半注意力头中，将token按组大小的一半进行移位，确保相邻组之间的信息流动。</li>\n<li><strong>信息泄漏控制</strong>：通过对注意力掩码的微调，可以规避移位可能引入的信息泄漏问题。</li>\n</ul>\n<h3 id=\"⚠️-操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-操作步骤\"><span>⚠️ 操作步骤</span></a></h3>\n<ol>\n<li><strong>特征分块</strong>：沿头部维度将特征分为两大块（如8行4列，8行表示token数量，4列表示头部数量）。</li>\n<li><strong>移位处理</strong>：\n<ul>\n<li>第一个块的部分标记按组大小的一半移位。</li>\n<li>例如，第8个token的后一半表示移动到第2块的第1行，其余标记整体向下移一行。</li>\n</ul>\n</li>\n<li><strong>分组计算</strong>：将token重新分组，重塑为批量维度，仅在组内计算注意力。</li>\n<li><strong>信息流动优化</strong>：移位机制保证跨组的信息流动，同时通过掩码调整避免信息泄漏。</li>\n</ol>\n<hr>\n<h2 id=\"数据与示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据与示例\"><span>数据与示例</span></a></h2>\n<h3 id=\"📊-数据表格-上下文长度分组与注意力计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📊-数据表格-上下文长度分组与注意力计算\"><span>📊 数据表格：上下文长度分组与注意力计算</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>总上下文长度</td>\n<td>8192</td>\n<td>模型训练的最大上下文长度</td>\n</tr>\n<tr>\n<td>每组大小</td>\n<td>2048</td>\n<td>单组内上下文长度</td>\n</tr>\n<tr>\n<td>注意力计算范围</td>\n<td>组内独立计算</td>\n<td>每组单独处理</td>\n</tr>\n<tr>\n<td>信息流动机制</td>\n<td>移位处理</td>\n<td>跨组信息流动</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"示例代码-分组与移位处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#示例代码-分组与移位处理\"><span>示例代码：分组与移位处理</span></a></h3>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 分组与移位示例</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">group_size </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2048</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 3</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> ...</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 8192</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 分组</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">groups </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">i</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">i</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">+</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">group_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> i </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> range</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">),</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 移位处理</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">shifted_groups </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> groups</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    half_size </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group_size </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">//</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    shifted_group </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">half_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:]</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> +</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">half_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    shifted_groups</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">shifted_group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><hr>\n<h2 id=\"常见错误警告-⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告-⚠️\"><span>常见错误警告 ⚠️</span></a></h2>\n<ol>\n<li><strong>信息泄漏未处理</strong>：未对注意力掩码进行微调可能导致跨组信息泄漏。</li>\n<li><strong>组大小设置不合理</strong>：若组大小过小，可能无法充分利用长上下文特性。</li>\n<li><strong>移位逻辑错误</strong>：移位操作未正确实现可能导致数据错乱。</li>\n</ol>\n<hr>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li><strong>创新点</strong>：通过移位机制实现跨组信息流动，避免传统方法的高计算成本。</li>\n<li><strong>应用场景</strong>：适用于需要处理超长文本的任务，如法律文档分析、小说生成等。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>学习并实现S2-Attention的核心算法。</li>\n<li>测试不同上下文长度对模型性能的影响。</li>\n<li>探索注意力掩码微调的最佳实践。</li>\n</ol>\n<hr>\n<h2 id=\"思考-🤔\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-🤔\"><span>思考 🤔</span></a></h2>\n<h3 id=\"延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#延伸问题\"><span>延伸问题</span></a></h3>\n<ol>\n<li>移位机制是否适用于其他类型的注意力模型？</li>\n<li>如何进一步优化跨组信息流动而不增加计算复杂度？</li>\n<li>是否可以将S2-Attention应用于多模态任务（如图像+文本）？</li>\n</ol>\n<hr>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原文出处：<a href=\"https://arxiv.org/pdf/2309.12307\" target=\"_blank\" rel=\"noopener noreferrer\">论文链接</a><br>\n项目源码：<a href=\"https://github.com/dvlab-research/LongLoRA\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub地址</a></p>\n</blockquote>\n<hr>\n<h2 id=\"后续追踪-📈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪-📈\"><span>后续追踪 📈</span></a></h2>\n<ul>\n<li><strong>研究计划</strong>：\n<ul>\n<li>深入分析S2-Attention在不同任务上的表现。</li>\n<li>探索长上下文模型在实时交互场景中的潜力。</li>\n<li>开发更灵活的注意力掩码微调方法以提升鲁棒性。</li>\n</ul>\n</li>\n</ul>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted Sparse Attention的创新方法.md","filePathRelative":"notes_bak/大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted Sparse Attention的创新方法.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted-Sparse-Attention的创新方法","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted-Sparse-Attention的创新方法/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-04T03:18:35.000Z","updated":"2025-04-13T05:06:02.000Z","title":"【长上下文模型优化】基于Shifted Sparse Attention的创新方法","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p class=\"block-language-dataview\"></p>\n<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：人工智能、深度学习</li>\n<li><strong>标签</strong>：长上下文模型、注意力机制、LongLoRA、优化算法</li>\n<li><strong>日期</strong>：2024年10月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>在长上下文语言模型的优化中，论文《LongLoRA: Efficient Fine-Tuning of Long Context Large Language Models》提出了一种名为**Shifted Sparse Attention (S2-Attention)**的方法。该技术通过分组计算注意力并引入移位机制，实现了长上下文训练的高效性，同时避免了信息泄漏问题。</p>\n<hr>\n<h2 id=\"技术解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术解析\"><span>技术解析</span></a></h2>\n<h3 id=\"✅-核心原理-shifted-sparse-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-核心原理-shifted-sparse-attention\"><span>✅ 核心原理：Shifted Sparse Attention</span></a></h3>\n<ul>\n<li><strong>分组处理</strong>：将上下文长度划分为多个组，每组单独计算注意力。</li>\n<li><strong>移位机制</strong>：在半注意力头中，将token按组大小的一半进行移位，确保相邻组之间的信息流动。</li>\n<li><strong>信息泄漏控制</strong>：通过对注意力掩码的微调，可以规避移位可能引入的信息泄漏问题。</li>\n</ul>\n<h3 id=\"⚠️-操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-操作步骤\"><span>⚠️ 操作步骤</span></a></h3>\n<ol>\n<li><strong>特征分块</strong>：沿头部维度将特征分为两大块（如8行4列，8行表示token数量，4列表示头部数量）。</li>\n<li><strong>移位处理</strong>：\n<ul>\n<li>第一个块的部分标记按组大小的一半移位。</li>\n<li>例如，第8个token的后一半表示移动到第2块的第1行，其余标记整体向下移一行。</li>\n</ul>\n</li>\n<li><strong>分组计算</strong>：将token重新分组，重塑为批量维度，仅在组内计算注意力。</li>\n<li><strong>信息流动优化</strong>：移位机制保证跨组的信息流动，同时通过掩码调整避免信息泄漏。</li>\n</ol>\n<hr>\n<h2 id=\"数据与示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据与示例\"><span>数据与示例</span></a></h2>\n<h3 id=\"📊-数据表格-上下文长度分组与注意力计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📊-数据表格-上下文长度分组与注意力计算\"><span>📊 数据表格：上下文长度分组与注意力计算</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>总上下文长度</td>\n<td>8192</td>\n<td>模型训练的最大上下文长度</td>\n</tr>\n<tr>\n<td>每组大小</td>\n<td>2048</td>\n<td>单组内上下文长度</td>\n</tr>\n<tr>\n<td>注意力计算范围</td>\n<td>组内独立计算</td>\n<td>每组单独处理</td>\n</tr>\n<tr>\n<td>信息流动机制</td>\n<td>移位处理</td>\n<td>跨组信息流动</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"示例代码-分组与移位处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#示例代码-分组与移位处理\"><span>示例代码：分组与移位处理</span></a></h3>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 分组与移位示例</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">group_size </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2048</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 3</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> ...</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 8192</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 分组</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">groups </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">i</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">i</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">+</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">group_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> i </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> range</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">),</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 移位处理</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">shifted_groups </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> groups</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    half_size </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group_size </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">//</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    shifted_group </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">half_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:]</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> +</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">half_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    shifted_groups</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">shifted_group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><hr>\n<h2 id=\"常见错误警告-⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告-⚠️\"><span>常见错误警告 ⚠️</span></a></h2>\n<ol>\n<li><strong>信息泄漏未处理</strong>：未对注意力掩码进行微调可能导致跨组信息泄漏。</li>\n<li><strong>组大小设置不合理</strong>：若组大小过小，可能无法充分利用长上下文特性。</li>\n<li><strong>移位逻辑错误</strong>：移位操作未正确实现可能导致数据错乱。</li>\n</ol>\n<hr>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li><strong>创新点</strong>：通过移位机制实现跨组信息流动，避免传统方法的高计算成本。</li>\n<li><strong>应用场景</strong>：适用于需要处理超长文本的任务，如法律文档分析、小说生成等。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>学习并实现S2-Attention的核心算法。</li>\n<li>测试不同上下文长度对模型性能的影响。</li>\n<li>探索注意力掩码微调的最佳实践。</li>\n</ol>\n<hr>\n<h2 id=\"思考-🤔\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-🤔\"><span>思考 🤔</span></a></h2>\n<h3 id=\"延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#延伸问题\"><span>延伸问题</span></a></h3>\n<ol>\n<li>移位机制是否适用于其他类型的注意力模型？</li>\n<li>如何进一步优化跨组信息流动而不增加计算复杂度？</li>\n<li>是否可以将S2-Attention应用于多模态任务（如图像+文本）？</li>\n</ol>\n<hr>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原文出处：<a href=\"https://arxiv.org/pdf/2309.12307\" target=\"_blank\" rel=\"noopener noreferrer\">论文链接</a><br>\n项目源码：<a href=\"https://github.com/dvlab-research/LongLoRA\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub地址</a></p>\n</blockquote>\n<hr>\n<h2 id=\"后续追踪-📈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪-📈\"><span>后续追踪 📈</span></a></h2>\n<ul>\n<li><strong>研究计划</strong>：\n<ul>\n<li>深入分析S2-Attention在不同任务上的表现。</li>\n<li>探索长上下文模型在实时交互场景中的潜力。</li>\n<li>开发更灵活的注意力掩码微调方法以提升鲁棒性。</li>\n</ul>\n</li>\n</ul>\n</template>","contentStripped":"<p class=\"block-language-dataview\"></p>\n<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：人工智能、深度学习</li>\n<li><strong>标签</strong>：长上下文模型、注意力机制、LongLoRA、优化算法</li>\n<li><strong>日期</strong>：2024年10月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>在长上下文语言模型的优化中，论文《LongLoRA: Efficient Fine-Tuning of Long Context Large Language Models》提出了一种名为**Shifted Sparse Attention (S2-Attention)**的方法。该技术通过分组计算注意力并引入移位机制，实现了长上下文训练的高效性，同时避免了信息泄漏问题。</p>\n<hr>\n<h2 id=\"技术解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术解析\"><span>技术解析</span></a></h2>\n<h3 id=\"✅-核心原理-shifted-sparse-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-核心原理-shifted-sparse-attention\"><span>✅ 核心原理：Shifted Sparse Attention</span></a></h3>\n<ul>\n<li><strong>分组处理</strong>：将上下文长度划分为多个组，每组单独计算注意力。</li>\n<li><strong>移位机制</strong>：在半注意力头中，将token按组大小的一半进行移位，确保相邻组之间的信息流动。</li>\n<li><strong>信息泄漏控制</strong>：通过对注意力掩码的微调，可以规避移位可能引入的信息泄漏问题。</li>\n</ul>\n<h3 id=\"⚠️-操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-操作步骤\"><span>⚠️ 操作步骤</span></a></h3>\n<ol>\n<li><strong>特征分块</strong>：沿头部维度将特征分为两大块（如8行4列，8行表示token数量，4列表示头部数量）。</li>\n<li><strong>移位处理</strong>：\n<ul>\n<li>第一个块的部分标记按组大小的一半移位。</li>\n<li>例如，第8个token的后一半表示移动到第2块的第1行，其余标记整体向下移一行。</li>\n</ul>\n</li>\n<li><strong>分组计算</strong>：将token重新分组，重塑为批量维度，仅在组内计算注意力。</li>\n<li><strong>信息流动优化</strong>：移位机制保证跨组的信息流动，同时通过掩码调整避免信息泄漏。</li>\n</ol>\n<hr>\n<h2 id=\"数据与示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据与示例\"><span>数据与示例</span></a></h2>\n<h3 id=\"📊-数据表格-上下文长度分组与注意力计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📊-数据表格-上下文长度分组与注意力计算\"><span>📊 数据表格：上下文长度分组与注意力计算</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>总上下文长度</td>\n<td>8192</td>\n<td>模型训练的最大上下文长度</td>\n</tr>\n<tr>\n<td>每组大小</td>\n<td>2048</td>\n<td>单组内上下文长度</td>\n</tr>\n<tr>\n<td>注意力计算范围</td>\n<td>组内独立计算</td>\n<td>每组单独处理</td>\n</tr>\n<tr>\n<td>信息流动机制</td>\n<td>移位处理</td>\n<td>跨组信息流动</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"示例代码-分组与移位处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#示例代码-分组与移位处理\"><span>示例代码：分组与移位处理</span></a></h3>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 分组与移位示例</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">group_size </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2048</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 3</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> ...</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 8192</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 分组</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">groups </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">i</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">i</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">+</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">group_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> i </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> range</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">tokens</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">),</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 移位处理</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">shifted_groups </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> groups</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    half_size </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group_size </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">//</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    shifted_group </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">half_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:]</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> +</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">half_size</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    shifted_groups</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">shifted_group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><hr>\n<h2 id=\"常见错误警告-⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告-⚠️\"><span>常见错误警告 ⚠️</span></a></h2>\n<ol>\n<li><strong>信息泄漏未处理</strong>：未对注意力掩码进行微调可能导致跨组信息泄漏。</li>\n<li><strong>组大小设置不合理</strong>：若组大小过小，可能无法充分利用长上下文特性。</li>\n<li><strong>移位逻辑错误</strong>：移位操作未正确实现可能导致数据错乱。</li>\n</ol>\n<hr>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li><strong>创新点</strong>：通过移位机制实现跨组信息流动，避免传统方法的高计算成本。</li>\n<li><strong>应用场景</strong>：适用于需要处理超长文本的任务，如法律文档分析、小说生成等。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>学习并实现S2-Attention的核心算法。</li>\n<li>测试不同上下文长度对模型性能的影响。</li>\n<li>探索注意力掩码微调的最佳实践。</li>\n</ol>\n<hr>\n<h2 id=\"思考-🤔\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-🤔\"><span>思考 🤔</span></a></h2>\n<h3 id=\"延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#延伸问题\"><span>延伸问题</span></a></h3>\n<ol>\n<li>移位机制是否适用于其他类型的注意力模型？</li>\n<li>如何进一步优化跨组信息流动而不增加计算复杂度？</li>\n<li>是否可以将S2-Attention应用于多模态任务（如图像+文本）？</li>\n</ol>\n<hr>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原文出处：<a href=\"https://arxiv.org/pdf/2309.12307\" target=\"_blank\" rel=\"noopener noreferrer\">论文链接</a><br>\n项目源码：<a href=\"https://github.com/dvlab-research/LongLoRA\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub地址</a></p>\n</blockquote>\n<hr>\n<h2 id=\"后续追踪-📈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪-📈\"><span>后续追踪 📈</span></a></h2>\n<ul>\n<li><strong>研究计划</strong>：\n<ul>\n<li>深入分析S2-Attention在不同任务上的表现。</li>\n<li>探索长上下文模型在实时交互场景中的潜力。</li>\n<li>开发更灵活的注意力掩码微调方法以提升鲁棒性。</li>\n</ul>\n</li>\n</ul>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"{ .block-language-dataview}\n\n\n\n## 元数据\n- **分类**：人工智能、深度学习\n- **标签**：长上下文模型、注意力机制、LongLoRA、优化算法\n- **日期**：2024年10月2日  \n\n---\n\n\n\n## 核心观点总结\n在长上下文语言模型的优化中，论文《LongLoRA: Efficient Fine-Tuning of Long Context Large Language Models》提出了一种名为**Shifted Sparse Attention (S2-Attention)**的方法。该技术通过分组计算注意力并引入移位机制，实现了长上下文训练的高效性，同时避免了信息泄漏问题。\n\n---\n\n\n\n## 技术解析\n\n### ✅ 核心原理：Shifted Sparse Attention\n- **分组处理**：将上下文长度划分为多个组，每组单独计算注意力。\n- **移位机制**：在半注意力头中，将token按组大小的一半进行移位，确保相邻组之间的信息流动。\n- **信息泄漏控制**：通过对注意力掩码的微调，可以规避移位可能引入的信息泄漏问题。\n\n\n### ⚠️ 操作步骤\n1. **特征分块**：沿头部维度将特征分为两大块（如8行4列，8行表示token数量，4列表示头部数量）。\n2. **移位处理**：\n   - 第一个块的部分标记按组大小的一半移位。\n   - 例如，第8个token的后一半表示移动到第2块的第1行，其余标记整体向下移一行。\n3. **分组计算**：将token重新分组，重塑为批量维度，仅在组内计算注意力。\n4. **信息流动优化**：移位机制保证跨组的信息流动，同时通过掩码调整避免信息泄漏。\n\n---\n\n\n\n## 数据与示例\n\n### 📊 数据表格：上下文长度分组与注意力计算\n| 参数            | 值                | 描述                       |\n|-----------------|-------------------|---------------------------|\n| 总上下文长度     | 8192             | 模型训练的最大上下文长度 |\n| 每组大小         | 2048             | 单组内上下文长度         |\n| 注意力计算范围   | 组内独立计算      | 每组单独处理             |\n| 信息流动机制     | 移位处理          | 跨组信息流动             |\n\n\n### 示例代码：分组与移位处理\n```python\n# 分组与移位示例\ngroup_size = 2048\ntokens = [1, 2, 3, ..., 8192]\n\n# 分组\ngroups = [tokens[i:i+group_size] for i in range(0, len(tokens), group_size)]\n\n# 移位处理\nshifted_groups = []\nfor group in groups:\n    half_size = group_size // 2\n    shifted_group = group[half_size:] + group[:half_size]\n    shifted_groups.append(shifted_group)\n```\n\n---\n\n\n\n## 常见错误警告 ⚠️\n1. **信息泄漏未处理**：未对注意力掩码进行微调可能导致跨组信息泄漏。\n2. **组大小设置不合理**：若组大小过小，可能无法充分利用长上下文特性。\n3. **移位逻辑错误**：移位操作未正确实现可能导致数据错乱。\n\n---\n\n\n\n## 💡 启发点\n- **创新点**：通过移位机制实现跨组信息流动，避免传统方法的高计算成本。\n- **应用场景**：适用于需要处理超长文本的任务，如法律文档分析、小说生成等。\n\n---\n\n\n\n## 行动清单\n1. 学习并实现S2-Attention的核心算法。\n2. 测试不同上下文长度对模型性能的影响。\n3. 探索注意力掩码微调的最佳实践。\n\n---\n\n\n\n## 思考 🤔\n\n### 延伸问题\n1. 移位机制是否适用于其他类型的注意力模型？\n2. 如何进一步优化跨组信息流动而不增加计算复杂度？\n3. 是否可以将S2-Attention应用于多模态任务（如图像+文本）？\n\n---\n\n\n\n## 来源标注\n> 原文出处：[论文链接](https://arxiv.org/pdf/2309.12307)  \n> 项目源码：[GitHub地址](https://github.com/dvlab-research/LongLoRA)  \n\n---\n\n\n\n## 后续追踪 📈\n- **研究计划**：\n  - 深入分析S2-Attention在不同任务上的表现。\n  - 探索长上下文模型在实时交互场景中的潜力。\n  - 开发更灵活的注意力掩码微调方法以提升鲁棒性。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"技术解析","slug":"技术解析","link":"#技术解析","children":[{"level":3,"title":"✅ 核心原理：Shifted Sparse Attention","slug":"✅-核心原理-shifted-sparse-attention","link":"#✅-核心原理-shifted-sparse-attention","children":[]},{"level":3,"title":"⚠️ 操作步骤","slug":"⚠️-操作步骤","link":"#⚠️-操作步骤","children":[]}]},{"level":2,"title":"数据与示例","slug":"数据与示例","link":"#数据与示例","children":[{"level":3,"title":"📊 数据表格：上下文长度分组与注意力计算","slug":"📊-数据表格-上下文长度分组与注意力计算","link":"#📊-数据表格-上下文长度分组与注意力计算","children":[]},{"level":3,"title":"示例代码：分组与移位处理","slug":"示例代码-分组与移位处理","link":"#示例代码-分组与移位处理","children":[]}]},{"level":2,"title":"常见错误警告 ⚠️","slug":"常见错误警告-⚠️","link":"#常见错误警告-⚠️","children":[]},{"level":2,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"思考 🤔","slug":"思考-🤔","link":"#思考-🤔","children":[{"level":3,"title":"延伸问题","slug":"延伸问题","link":"#延伸问题","children":[]}]},{"level":2,"title":"来源标注","slug":"来源标注","link":"#来源标注","children":[]},{"level":2,"title":"后续追踪 📈","slug":"后续追踪-📈","link":"#后续追踪-📈","children":[]}]}}
