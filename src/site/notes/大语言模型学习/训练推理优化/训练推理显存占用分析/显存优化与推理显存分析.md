---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/训练推理优化/训练推理显存占用分析/显存优化与推理显存分析","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/训练推理优化/训练推理显存占用分析/显存优化与推理显存分析/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-28T22:18:54.000+08:00","updated":"2025-04-29T11:00:58.000+08:00"}
---



## 元数据
分类：深度学习显存优化

标签：显存优化、大模型、GPU

日期：2025年4月12日



## 内容总结
在深度学习中，随着大模型参数的增长，显存优化变得尤为重要。显存优化可以通过提高算法效率或扩大显存空间来实现。推理阶段的显存占用可以通过公式估算，而显存优化则需要从多方面着手，包括多卡并行、算子优化、数据类型修改等。

### 推理阶段显存分析
推理阶段的显存占用可以通过以下公式估算：

$$
InferMemory \approx 1.2 \times ModelMemory
$$

此公式帮助我们快速了解推理阶段的显存需求。


### 显存优化方法
显存优化方法包括：

- **多卡并行**：使用频率最高的方法，通过设计新的参数来降低显存消耗。
- **算子优化**：选择精度相同但显存消耗更低的算子。
- **数据类型修改**：使用低精度数据替换高精度数据。
- **消除框架副本**：优化AI框架中产生的中间副本。
- **显存管理**：通过优化显存管理减少碎片。
- **底层API替换**：使用更节省显存的API替换默认操作。

💡启发点：这些方法不仅能降低显存消耗，还可能提高计算效率。



## 操作步骤
1. ✅ 使用多卡并行设计新的参数。
2. ⚠ 选择精度相同但显存消耗更低的算子。
3. ❗ 使用低精度数据替换高精度数据。



## 常见错误
> 在进行数据类型修改时，可能会影响训练收敛性或推理性能。



## 行动清单
- 研究多卡并行策略以进一步降低显存消耗。
- 探索更节省显存的算子和API。
- 优化AI框架的显存管理。

> 原始出处：[推理阶段显存分析](https://kipp.ly/transformer-inference-arithmetic/)

以上是关于显存优化与推理阶段显存分析的博客笔记，希望对您有所帮助。
