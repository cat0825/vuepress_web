{"content":"<h2 id=\"分类-机器学习技术\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-机器学习技术\"><span>分类：机器学习技术</span></a></h2>\n<h3 id=\"标签-peft-微调-大模型-参数优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-peft-微调-大模型-参数优化\"><span>标签：PEFT, 微调, 大模型, 参数优化</span></a></h3>\n<h3 id=\"日期-2025年4月12日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2025年4月12日\"><span>日期：2025年4月12日</span></a></h3>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>PEFT（参数高效微调）方法通过仅微调少量或额外的模型参数，并固定大部分预训练参数，实现了与全量微调相当的性能。这种方法在降低计算和存储成本的同时，能够有效避免过拟合问题。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>优势分析</strong>：</p>\n<ul>\n<li>PEFT方法显著减少显存占用，对硬件资源要求低。</li>\n<li>训练速度加快，耗时更短。</li>\n<li>存储成本降低，不同任务可以共享大部分权重参数。</li>\n</ul>\n</li>\n<li>\n<p><strong>性能表现</strong>：</p>\n<ul>\n<li>可能在某些情况下提供更好的模型性能。</li>\n<li>减轻过拟合问题，适用于多种下游任务。</li>\n</ul>\n</li>\n<li>\n<p><strong>应用局限</strong>：</p>\n<ul>\n<li>在有条件进行SFT（全量微调）时，通常仍选择全量微调。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>PEFT（参数高效微调）</strong>：一种优化技术，通过调整少量模型参数来适应新任务，而不需要重新训练整个模型。</li>\n<li><strong>全量微调</strong>：对模型的所有参数进行重新训练，以适应新任务需求。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定需要微调的模型及其任务。</li>\n<li>⚠ 识别需要调整的关键参数，固定其余参数。</li>\n<li>❗ 执行PEFT方法进行微调，监控性能变化。</li>\n<li>✅ 评估微调后的模型性能，与全量微调结果进行比较。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在选择需要微调的参数时，忽略了对任务特定需求的分析，可能导致模型性能不佳。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<p>PEFT方法展示了在资源受限环境下，如何通过优化少量参数达到理想性能的创新思路。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究PEFT方法在不同模型上的应用效果。</li>\n<li>比较PEFT与其他微调方法的优缺点。</li>\n<li>探索PEFT在实时系统中的应用潜力。</li>\n</ul>\n<blockquote>\n<p>原始出处：[来源未提供]</p>\n</blockquote>\n<p>通过本文的分析，我们可以更好地理解PEFT方法在大模型微调中的重要性及其应用场景，为后续研究提供了方向。</p>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/PEFT参数高效微调/介绍.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/PEFT参数高效微调/介绍.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/PEFT参数高效微调/介绍","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/PEFT参数高效微调/介绍/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-23T14:43:12.187Z","updated":"2025-04-23T14:45:08.029Z","title":"介绍","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"分类-机器学习技术\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-机器学习技术\"><span>分类：机器学习技术</span></a></h2>\n<h3 id=\"标签-peft-微调-大模型-参数优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-peft-微调-大模型-参数优化\"><span>标签：PEFT, 微调, 大模型, 参数优化</span></a></h3>\n<h3 id=\"日期-2025年4月12日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2025年4月12日\"><span>日期：2025年4月12日</span></a></h3>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>PEFT（参数高效微调）方法通过仅微调少量或额外的模型参数，并固定大部分预训练参数，实现了与全量微调相当的性能。这种方法在降低计算和存储成本的同时，能够有效避免过拟合问题。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>优势分析</strong>：</p>\n<ul>\n<li>PEFT方法显著减少显存占用，对硬件资源要求低。</li>\n<li>训练速度加快，耗时更短。</li>\n<li>存储成本降低，不同任务可以共享大部分权重参数。</li>\n</ul>\n</li>\n<li>\n<p><strong>性能表现</strong>：</p>\n<ul>\n<li>可能在某些情况下提供更好的模型性能。</li>\n<li>减轻过拟合问题，适用于多种下游任务。</li>\n</ul>\n</li>\n<li>\n<p><strong>应用局限</strong>：</p>\n<ul>\n<li>在有条件进行SFT（全量微调）时，通常仍选择全量微调。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>PEFT（参数高效微调）</strong>：一种优化技术，通过调整少量模型参数来适应新任务，而不需要重新训练整个模型。</li>\n<li><strong>全量微调</strong>：对模型的所有参数进行重新训练，以适应新任务需求。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定需要微调的模型及其任务。</li>\n<li>⚠ 识别需要调整的关键参数，固定其余参数。</li>\n<li>❗ 执行PEFT方法进行微调，监控性能变化。</li>\n<li>✅ 评估微调后的模型性能，与全量微调结果进行比较。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在选择需要微调的参数时，忽略了对任务特定需求的分析，可能导致模型性能不佳。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<p>PEFT方法展示了在资源受限环境下，如何通过优化少量参数达到理想性能的创新思路。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究PEFT方法在不同模型上的应用效果。</li>\n<li>比较PEFT与其他微调方法的优缺点。</li>\n<li>探索PEFT在实时系统中的应用潜力。</li>\n</ul>\n<blockquote>\n<p>原始出处：[来源未提供]</p>\n</blockquote>\n<p>通过本文的分析，我们可以更好地理解PEFT方法在大模型微调中的重要性及其应用场景，为后续研究提供了方向。</p>\n</template>","contentStripped":"<h2 id=\"分类-机器学习技术\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-机器学习技术\"><span>分类：机器学习技术</span></a></h2>\n<h3 id=\"标签-peft-微调-大模型-参数优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-peft-微调-大模型-参数优化\"><span>标签：PEFT, 微调, 大模型, 参数优化</span></a></h3>\n<h3 id=\"日期-2025年4月12日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2025年4月12日\"><span>日期：2025年4月12日</span></a></h3>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>PEFT（参数高效微调）方法通过仅微调少量或额外的模型参数，并固定大部分预训练参数，实现了与全量微调相当的性能。这种方法在降低计算和存储成本的同时，能够有效避免过拟合问题。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>优势分析</strong>：</p>\n<ul>\n<li>PEFT方法显著减少显存占用，对硬件资源要求低。</li>\n<li>训练速度加快，耗时更短。</li>\n<li>存储成本降低，不同任务可以共享大部分权重参数。</li>\n</ul>\n</li>\n<li>\n<p><strong>性能表现</strong>：</p>\n<ul>\n<li>可能在某些情况下提供更好的模型性能。</li>\n<li>减轻过拟合问题，适用于多种下游任务。</li>\n</ul>\n</li>\n<li>\n<p><strong>应用局限</strong>：</p>\n<ul>\n<li>在有条件进行SFT（全量微调）时，通常仍选择全量微调。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>PEFT（参数高效微调）</strong>：一种优化技术，通过调整少量模型参数来适应新任务，而不需要重新训练整个模型。</li>\n<li><strong>全量微调</strong>：对模型的所有参数进行重新训练，以适应新任务需求。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定需要微调的模型及其任务。</li>\n<li>⚠ 识别需要调整的关键参数，固定其余参数。</li>\n<li>❗ 执行PEFT方法进行微调，监控性能变化。</li>\n<li>✅ 评估微调后的模型性能，与全量微调结果进行比较。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在选择需要微调的参数时，忽略了对任务特定需求的分析，可能导致模型性能不佳。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<p>PEFT方法展示了在资源受限环境下，如何通过优化少量参数达到理想性能的创新思路。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究PEFT方法在不同模型上的应用效果。</li>\n<li>比较PEFT与其他微调方法的优缺点。</li>\n<li>探索PEFT在实时系统中的应用潜力。</li>\n</ul>\n<blockquote>\n<p>原始出处：[来源未提供]</p>\n</blockquote>\n<p>通过本文的分析，我们可以更好地理解PEFT方法在大模型微调中的重要性及其应用场景，为后续研究提供了方向。</p>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 分类：机器学习技术\n\n### 标签：PEFT, 微调, 大模型, 参数优化\n\n\n### 日期：2025年4月12日\n\n\n## 核心观点总结\nPEFT（参数高效微调）方法通过仅微调少量或额外的模型参数，并固定大部分预训练参数，实现了与全量微调相当的性能。这种方法在降低计算和存储成本的同时，能够有效避免过拟合问题。\n\n\n## 重点段落\n1. **优势分析**：\n   - PEFT方法显著减少显存占用，对硬件资源要求低。\n   - 训练速度加快，耗时更短。\n   - 存储成本降低，不同任务可以共享大部分权重参数。\n\n2. **性能表现**：\n   - 可能在某些情况下提供更好的模型性能。\n   - 减轻过拟合问题，适用于多种下游任务。\n\n3. **应用局限**：\n   - 在有条件进行SFT（全量微调）时，通常仍选择全量微调。\n\n\n## 技术术语通俗解释\n- **PEFT（参数高效微调）**：一种优化技术，通过调整少量模型参数来适应新任务，而不需要重新训练整个模型。\n- **全量微调**：对模型的所有参数进行重新训练，以适应新任务需求。\n\n\n## 操作步骤\n1. ✅ 确定需要微调的模型及其任务。\n2. ⚠ 识别需要调整的关键参数，固定其余参数。\n3. ❗ 执行PEFT方法进行微调，监控性能变化。\n4. ✅ 评估微调后的模型性能，与全量微调结果进行比较。\n\n\n## 常见错误\n> **警告**：在选择需要微调的参数时，忽略了对任务特定需求的分析，可能导致模型性能不佳。\n\n\n## 💡 启发点\nPEFT方法展示了在资源受限环境下，如何通过优化少量参数达到理想性能的创新思路。\n\n\n## 行动清单\n- 研究PEFT方法在不同模型上的应用效果。\n- 比较PEFT与其他微调方法的优缺点。\n- 探索PEFT在实时系统中的应用潜力。\n\n> 原始出处：[来源未提供]\n\n通过本文的分析，我们可以更好地理解PEFT方法在大模型微调中的重要性及其应用场景，为后续研究提供了方向。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"分类：机器学习技术","slug":"分类-机器学习技术","link":"#分类-机器学习技术","children":[{"level":3,"title":"标签：PEFT, 微调, 大模型, 参数优化","slug":"标签-peft-微调-大模型-参数优化","link":"#标签-peft-微调-大模型-参数优化","children":[]},{"level":3,"title":"日期：2025年4月12日","slug":"日期-2025年4月12日","link":"#日期-2025年4月12日","children":[]}]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[]},{"level":2,"title":"技术术语通俗解释","slug":"技术术语通俗解释","link":"#技术术语通俗解释","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
