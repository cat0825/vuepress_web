{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能、机器学习</li>\n<li>标签：Llama 2、模型训练、拒绝采样、数据质量</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概要\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概要\"><span>内容概要</span></a></h2>\n<p>Llama 2 是一种新型的开放基础和微调聊天模型，相较于 LLaMA1，Llama 2 在模型结构和训练数据上进行了多项优化。本文将深入探讨这些改进以及其对模型性能的影响。</p>\n<h2 id=\"模型结构改进\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构改进\"><span>模型结构改进</span></a></h2>\n<p>Llama 2 在以下几个方面对模型结构进行了优化：</p>\n<ul>\n<li><strong>GQA 增强</strong>：大参数模型引入了 GQA（Generalized Query Attention），虽然整体参数量有所减少，但模型的推理能力得到了提升。</li>\n<li><strong>FFN 扩充</strong>：FFN（Feed-Forward Network）模块的矩阵维度扩展，增强了模型的泛化能力。</li>\n<li><strong>上下文窗口延长</strong>：上下文窗口从 LLaMA1 的 2K 增加到 4K，能够处理更长的文本输入。</li>\n</ul>\n<h2 id=\"训练数据策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练数据策略\"><span>训练数据策略</span></a></h2>\n<p>Llama-2 采用了来自公开可用源的 2T 数据 token 进行预训练。尽管公开数据丰富，Meta 强调数据质量的重要性，选择使用自有标注数据以确保高质量训练。不同的数据源和标注供应商显著影响下游微调结果，强调了数据检查的重要性。\n<img src=\"/img/user/附件/Pasted image 20250424223130.png\" alt=\"Pasted image 20250424223130.png\"></p>\n<h2 id=\"拒绝采样方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#拒绝采样方法\"><span>拒绝采样方法</span></a></h2>\n<p>拒绝采样（Reject Sampling, RS）是一种从目标概率分布中获取样本的蒙特卡洛方法。在 LLM 中，模型对同一提示生成多个响应，并利用奖励模型对这些答案进行评分，选出得分最高的答案。这一过程提升了生成质量，并为模型进一步训练提供了优质样本。</p>\n<h2 id=\"后训练总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后训练总结\"><span>后训练总结</span></a></h2>\n<ul>\n<li><strong>SFT 阶段</strong>：SFT（Supervised Fine-Tuning）阶段不应停留太久，通常一万个样本足以达到标注员水平。</li>\n<li><strong>奖励模型语料构建</strong>：建议从自身模型中获取数据，以提升奖励模型的效果。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定模型结构调整，如 GQA 增强和 FFN 扩充。</li>\n<li>⚠ 收集并筛选高质量的训练数据。</li>\n<li>❗ 实施拒绝采样以优化生成质量。\n<img src=\"/img/user/附件/Pasted image 20250424223151.png\" alt=\"Pasted image 20250424223151.png\">\n<img src=\"/img/user/附件/Pasted image 20250424223139.png\" alt=\"Pasted image 20250424223139.png\"></li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>小心选择数据源，不同数据源可能导致微调结果不一致。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>数据质量比数量更重要，少量高质量数据优于大量低质量数据。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 验证 GQA 增强对推理能力的具体影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 比较不同标注数据对微调结果的差异。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 评估拒绝采样在其他模型中的适用性。</label></li>\n</ul>\n<blockquote>\n<p>原始出处：[Llama 2: Open Foundation and Fine-Tuned Chat Models]</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/LLama系列/LLama 2.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/LLama系列/LLama 2.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/LLama系列/LLama-2","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/LLama系列/LLama-2/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T14:31:06.620Z","updated":"2025-04-24T14:31:52.216Z","title":"LLama 2","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能、机器学习</li>\n<li>标签：Llama 2、模型训练、拒绝采样、数据质量</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概要\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概要\"><span>内容概要</span></a></h2>\n<p>Llama 2 是一种新型的开放基础和微调聊天模型，相较于 LLaMA1，Llama 2 在模型结构和训练数据上进行了多项优化。本文将深入探讨这些改进以及其对模型性能的影响。</p>\n<h2 id=\"模型结构改进\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构改进\"><span>模型结构改进</span></a></h2>\n<p>Llama 2 在以下几个方面对模型结构进行了优化：</p>\n<ul>\n<li><strong>GQA 增强</strong>：大参数模型引入了 GQA（Generalized Query Attention），虽然整体参数量有所减少，但模型的推理能力得到了提升。</li>\n<li><strong>FFN 扩充</strong>：FFN（Feed-Forward Network）模块的矩阵维度扩展，增强了模型的泛化能力。</li>\n<li><strong>上下文窗口延长</strong>：上下文窗口从 LLaMA1 的 2K 增加到 4K，能够处理更长的文本输入。</li>\n</ul>\n<h2 id=\"训练数据策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练数据策略\"><span>训练数据策略</span></a></h2>\n<p>Llama-2 采用了来自公开可用源的 2T 数据 token 进行预训练。尽管公开数据丰富，Meta 强调数据质量的重要性，选择使用自有标注数据以确保高质量训练。不同的数据源和标注供应商显著影响下游微调结果，强调了数据检查的重要性。\n<img src=\"/img/user/附件/Pasted image 20250424223130.png\" alt=\"Pasted image 20250424223130.png\"></p>\n<h2 id=\"拒绝采样方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#拒绝采样方法\"><span>拒绝采样方法</span></a></h2>\n<p>拒绝采样（Reject Sampling, RS）是一种从目标概率分布中获取样本的蒙特卡洛方法。在 LLM 中，模型对同一提示生成多个响应，并利用奖励模型对这些答案进行评分，选出得分最高的答案。这一过程提升了生成质量，并为模型进一步训练提供了优质样本。</p>\n<h2 id=\"后训练总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后训练总结\"><span>后训练总结</span></a></h2>\n<ul>\n<li><strong>SFT 阶段</strong>：SFT（Supervised Fine-Tuning）阶段不应停留太久，通常一万个样本足以达到标注员水平。</li>\n<li><strong>奖励模型语料构建</strong>：建议从自身模型中获取数据，以提升奖励模型的效果。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定模型结构调整，如 GQA 增强和 FFN 扩充。</li>\n<li>⚠ 收集并筛选高质量的训练数据。</li>\n<li>❗ 实施拒绝采样以优化生成质量。\n<img src=\"/img/user/附件/Pasted image 20250424223151.png\" alt=\"Pasted image 20250424223151.png\">\n<img src=\"/img/user/附件/Pasted image 20250424223139.png\" alt=\"Pasted image 20250424223139.png\"></li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>小心选择数据源，不同数据源可能导致微调结果不一致。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>数据质量比数量更重要，少量高质量数据优于大量低质量数据。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 验证 GQA 增强对推理能力的具体影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 比较不同标注数据对微调结果的差异。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 评估拒绝采样在其他模型中的适用性。</label></li>\n</ul>\n<blockquote>\n<p>原始出处：[Llama 2: Open Foundation and Fine-Tuned Chat Models]</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能、机器学习</li>\n<li>标签：Llama 2、模型训练、拒绝采样、数据质量</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概要\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概要\"><span>内容概要</span></a></h2>\n<p>Llama 2 是一种新型的开放基础和微调聊天模型，相较于 LLaMA1，Llama 2 在模型结构和训练数据上进行了多项优化。本文将深入探讨这些改进以及其对模型性能的影响。</p>\n<h2 id=\"模型结构改进\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构改进\"><span>模型结构改进</span></a></h2>\n<p>Llama 2 在以下几个方面对模型结构进行了优化：</p>\n<ul>\n<li><strong>GQA 增强</strong>：大参数模型引入了 GQA（Generalized Query Attention），虽然整体参数量有所减少，但模型的推理能力得到了提升。</li>\n<li><strong>FFN 扩充</strong>：FFN（Feed-Forward Network）模块的矩阵维度扩展，增强了模型的泛化能力。</li>\n<li><strong>上下文窗口延长</strong>：上下文窗口从 LLaMA1 的 2K 增加到 4K，能够处理更长的文本输入。</li>\n</ul>\n<h2 id=\"训练数据策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练数据策略\"><span>训练数据策略</span></a></h2>\n<p>Llama-2 采用了来自公开可用源的 2T 数据 token 进行预训练。尽管公开数据丰富，Meta 强调数据质量的重要性，选择使用自有标注数据以确保高质量训练。不同的数据源和标注供应商显著影响下游微调结果，强调了数据检查的重要性。\n<img src=\"/img/user/附件/Pasted image 20250424223130.png\" alt=\"Pasted image 20250424223130.png\"></p>\n<h2 id=\"拒绝采样方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#拒绝采样方法\"><span>拒绝采样方法</span></a></h2>\n<p>拒绝采样（Reject Sampling, RS）是一种从目标概率分布中获取样本的蒙特卡洛方法。在 LLM 中，模型对同一提示生成多个响应，并利用奖励模型对这些答案进行评分，选出得分最高的答案。这一过程提升了生成质量，并为模型进一步训练提供了优质样本。</p>\n<h2 id=\"后训练总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后训练总结\"><span>后训练总结</span></a></h2>\n<ul>\n<li><strong>SFT 阶段</strong>：SFT（Supervised Fine-Tuning）阶段不应停留太久，通常一万个样本足以达到标注员水平。</li>\n<li><strong>奖励模型语料构建</strong>：建议从自身模型中获取数据，以提升奖励模型的效果。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定模型结构调整，如 GQA 增强和 FFN 扩充。</li>\n<li>⚠ 收集并筛选高质量的训练数据。</li>\n<li>❗ 实施拒绝采样以优化生成质量。\n<img src=\"/img/user/附件/Pasted image 20250424223151.png\" alt=\"Pasted image 20250424223151.png\">\n<img src=\"/img/user/附件/Pasted image 20250424223139.png\" alt=\"Pasted image 20250424223139.png\"></li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>小心选择数据源，不同数据源可能导致微调结果不一致。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>数据质量比数量更重要，少量高质量数据优于大量低质量数据。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 验证 GQA 增强对推理能力的具体影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 比较不同标注数据对微调结果的差异。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 评估拒绝采样在其他模型中的适用性。</label></li>\n</ul>\n<blockquote>\n<p>原始出处：[Llama 2: Open Foundation and Fine-Tuned Chat Models]</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：人工智能、机器学习\n- 标签：Llama 2、模型训练、拒绝采样、数据质量\n- 日期：2025年4月12日\n\n\n## 内容概要\nLlama 2 是一种新型的开放基础和微调聊天模型，相较于 LLaMA1，Llama 2 在模型结构和训练数据上进行了多项优化。本文将深入探讨这些改进以及其对模型性能的影响。\n\n\n## 模型结构改进\nLlama 2 在以下几个方面对模型结构进行了优化：\n- **GQA 增强**：大参数模型引入了 GQA（Generalized Query Attention），虽然整体参数量有所减少，但模型的推理能力得到了提升。\n- **FFN 扩充**：FFN（Feed-Forward Network）模块的矩阵维度扩展，增强了模型的泛化能力。\n- **上下文窗口延长**：上下文窗口从 LLaMA1 的 2K 增加到 4K，能够处理更长的文本输入。\n\n\n## 训练数据策略\nLlama-2 采用了来自公开可用源的 2T 数据 token 进行预训练。尽管公开数据丰富，Meta 强调数据质量的重要性，选择使用自有标注数据以确保高质量训练。不同的数据源和标注供应商显著影响下游微调结果，强调了数据检查的重要性。\n![Pasted image 20250424223130.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424223130.png)\n\n\n## 拒绝采样方法\n拒绝采样（Reject Sampling, RS）是一种从目标概率分布中获取样本的蒙特卡洛方法。在 LLM 中，模型对同一提示生成多个响应，并利用奖励模型对这些答案进行评分，选出得分最高的答案。这一过程提升了生成质量，并为模型进一步训练提供了优质样本。\n\n\n## 后训练总结\n- **SFT 阶段**：SFT（Supervised Fine-Tuning）阶段不应停留太久，通常一万个样本足以达到标注员水平。\n- **奖励模型语料构建**：建议从自身模型中获取数据，以提升奖励模型的效果。\n\n\n## 操作步骤\n1. ✅ 确定模型结构调整，如 GQA 增强和 FFN 扩充。\n2. ⚠ 收集并筛选高质量的训练数据。\n3. ❗ 实施拒绝采样以优化生成质量。\n![Pasted image 20250424223151.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424223151.png)\n![Pasted image 20250424223139.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424223139.png)\n\n\n## 常见错误\n> 小心选择数据源，不同数据源可能导致微调结果不一致。\n\n\n## 💡 启发点\n- 数据质量比数量更重要，少量高质量数据优于大量低质量数据。\n\n\n## 行动清单\n- [ ] 验证 GQA 增强对推理能力的具体影响。\n- [ ] 比较不同标注数据对微调结果的差异。\n- [ ] 评估拒绝采样在其他模型中的适用性。\n\n> 原始出处：[Llama 2: Open Foundation and Fine-Tuned Chat Models]","excerpt":"","includedFiles":[],"tasklistId":3,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容概要","slug":"内容概要","link":"#内容概要","children":[]},{"level":2,"title":"模型结构改进","slug":"模型结构改进","link":"#模型结构改进","children":[]},{"level":2,"title":"训练数据策略","slug":"训练数据策略","link":"#训练数据策略","children":[]},{"level":2,"title":"拒绝采样方法","slug":"拒绝采样方法","link":"#拒绝采样方法","children":[]},{"level":2,"title":"后训练总结","slug":"后训练总结","link":"#后训练总结","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
