---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-10T21:55:04.000+08:00","updated":"2025-04-13T13:06:02.000+08:00"}
---



## 元数据
- **分类**：机器学习
- **标签**：监督微调, 预训练, 模型优化, 数据处理
- **日期**：2023年10月22日



## 核心观点总结
监督微调（SFT）与预训练（pretrain）在训练方式上无区别，但在数据处理和训练目的上存在显著差异。SFT数据不需要拼接，使用特殊标记符构造知识，并且强调指令遵循能力，而非知识注入。预训练主要是知识的学习，而SFT则是应用这些知识。

### 重点段落
1. **数据组成形式**：
   - 预训练数据达到模型输入长度上限，需拼接。
   - SFT数据保持原始长度，不需拼接，使用特殊标记符来构造语义。

2. **训练目标差异**：
   - 预训练旨在知识学习。
   - SFT专注于指令遵循能力。

3. **知识注入策略**：
   - SFT不适合进行大规模知识注入。
   - 知识注入应采用继续预训练策略，以维持模型通用能力。



## 技术术语通俗解释
- **特殊标记符（special_token）**：在文本中使用的特定符号，用来标识不同角色或语义。
- **EOS标记符（eos_token）**：表示文本结束的符号，帮助模型停止生成内容。



## 重点步骤
1. ✅ 确保SFT数据保持原始长度，不进行拼接。
2. ⚠ 使用特殊标记符分割角色和语义。
3. ❗ 避免在SFT阶段进行大量知识注入，保持模型的通用性。



## 常见错误
> 在SFT阶段进行过多的知识注入，导致模型的通用能力下降。



## 💡启发点
- 使用特殊标记符可以有效提升模型理解复杂语境的能力。
- 适当控制知识注入比例可保持模型的多样性和灵活性。



## 行动清单
- 研究如何优化特殊标记符的使用以提升模型性能。
- 探讨继续预训练策略在不同领域的应用效果。



## 📈趋势预测
随着自然语言处理技术的发展，SFT将更广泛应用于需要高度精确指令执行的领域，如医疗和法律文本分析。



## 后续追踪
- 探索SFT在多语言模型中的应用潜力。
- 研究继续预训练策略对不同类型数据集的影响。

> 来源：原文内容整理自关于监督微调与预训练的比较分析。
