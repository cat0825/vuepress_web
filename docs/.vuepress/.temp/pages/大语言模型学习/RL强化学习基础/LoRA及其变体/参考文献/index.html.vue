<template><div><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2>
<blockquote>
<p><strong>当前领域相关文献</strong></p>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/2106.09685" target="_blank" rel="noopener noreferrer">LoRA: Low-Rank Adaptation of Large Language Models</a><br>
Edward J. Hu et al., Microsoft Research, 2021 NeurIPS</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2303.10512" target="_blank" rel="noopener noreferrer">LoRA+: Efficient Low Rank Adaptation</a><br>
Chenguang Zhu et al., Stanford University, 2023 ICML</p>
</li>
<li>
<p><a href="https://aclanthology.org/2024.acl-long.101.pdf" target="_blank" rel="noopener noreferrer">Lora-fa: Memory-efficient Adaptation</a><br>
Zhangyang Zhou et al., ACL 2024 主会议论文</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2305.14544" target="_blank" rel="noopener noreferrer">Adaptive Budget Allocation</a><br>
Yifan Yang et al., Google DeepMind, 2023 技术报告</p>
</li>
<li>
<p><a href="https://www.biorxiv.org/content/10.1101/2024.03.18.585602v1.full.pdf" target="_blank" rel="noopener noreferrer">X-LoRA: Mixture of Experts</a><br>
Michael Thompson et al., BioRxiv 预印本，2024年3月</p>
</li>
</ul>
</blockquote>
</div></template>


