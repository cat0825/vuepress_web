{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>: 人工智能模型</p>\n<p><strong>标签</strong>: Llama3, 多语言处理, 机器学习, 模型优化</p>\n<p><strong>日期</strong>: 2025年4月12日</p>\n<hr>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>Llama 3系列模型是Meta公司推出的最新人工智能模型，包含Llama3和Llama3.1。该系列模型在多语言处理、长文本处理和工具使用方面进行了显著的改进。本文将深入解析Llama 3的模型结构、训练数据及训练流程。\n<img src=\"/img/user/附件/Pasted image 20250424223641.png\" alt=\"Pasted image 20250424223641.png\"></p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>与之前的LLaMA2相比，LLaMA3进行了以下改进：</p>\n<ul>\n<li><strong>词表扩展</strong>: tokenizer由sentencepiece更换为tiktoken，词表大小从32k扩展到128k。</li>\n<li><strong>上下文长度</strong>: 上下文长度扩展到了8k，预训练后期通过多阶段长文本训练达到了128K。</li>\n<li><strong>GQA技术应用</strong>: LLaMA3 8B和70B模型均采用了GQA技术。</li>\n</ul>\n<p>💡启发点：通过扩展词表和上下文长度，Llama 3显著提升了处理复杂任务的能力。</p>\n<h2 id=\"训练数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练数据\"><span>训练数据</span></a></h2>\n<p>Llama 3采用了精心设计的预训练语料库，扩展到15T Tokens，代码数据扩充了4倍，显著提升了模型在代码能力和逻辑推理能力方面的表现。数据来源包括30多种语言，超过5%的非英语token。这些措施不仅提高了英语内容处理效率，也增强了多语言处理能力。</p>\n<h3 id=\"数据过滤流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据过滤流程\"><span>数据过滤流程</span></a></h3>\n<p>Meta开发了一系列数据过滤工具，包括启发式过滤器、NSFW过滤器、语义重复数据删除技术及预测数据质量的文本分类器。这些工具确保了高质量数据的选择。</p>\n<h2 id=\"训练流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练流程\"><span>训练流程</span></a></h2>\n<p>Llama-3系列包括两个模型：预训练模型Llama-3和微调后的模型Llama-3-Instruct。</p>\n<h3 id=\"整体流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#整体流程\"><span>整体流程</span></a></h3>\n<ol>\n<li>✅ 初始预训练</li>\n<li>⚠ 长上下文预训练</li>\n<li>❗ 退火（Annealing）</li>\n</ol>\n<p>后训练阶段包括监督式微调（SFT）、拒绝采样、RLHF和直接微调等步骤。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：LLaMA-3没有采用MOE结构，这可能导致在性能上无法与同规模的密集型模型相比。随着模型规模的扩大，如何降低推理成本将成为一个需要关注的问题。</p>\n</blockquote>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>Llama 3 8B</th>\n<th>Llama 3 70B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>上下文长度</td>\n<td>8k</td>\n<td>8k</td>\n</tr>\n<tr>\n<td>GQA</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Token数量</td>\n<td>15T+</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何进一步优化多语言处理能力。</li>\n<li>探讨降低模型推理成本的方法。</li>\n<li>开发更高效的数据过滤技术以提升数据质量。</li>\n</ul>\n<blockquote>\n<p>原文出处: &quot;The Llama 3 Herd of Models&quot;</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/LLama系列/LLama 3.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/LLama系列/LLama 3.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/LLama系列/LLama-3","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/LLama系列/LLama-3/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T14:35:12.558Z","updated":"2025-04-24T14:36:42.556Z","title":"LLama 3","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>: 人工智能模型</p>\n<p><strong>标签</strong>: Llama3, 多语言处理, 机器学习, 模型优化</p>\n<p><strong>日期</strong>: 2025年4月12日</p>\n<hr>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>Llama 3系列模型是Meta公司推出的最新人工智能模型，包含Llama3和Llama3.1。该系列模型在多语言处理、长文本处理和工具使用方面进行了显著的改进。本文将深入解析Llama 3的模型结构、训练数据及训练流程。\n<img src=\"/img/user/附件/Pasted image 20250424223641.png\" alt=\"Pasted image 20250424223641.png\"></p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>与之前的LLaMA2相比，LLaMA3进行了以下改进：</p>\n<ul>\n<li><strong>词表扩展</strong>: tokenizer由sentencepiece更换为tiktoken，词表大小从32k扩展到128k。</li>\n<li><strong>上下文长度</strong>: 上下文长度扩展到了8k，预训练后期通过多阶段长文本训练达到了128K。</li>\n<li><strong>GQA技术应用</strong>: LLaMA3 8B和70B模型均采用了GQA技术。</li>\n</ul>\n<p>💡启发点：通过扩展词表和上下文长度，Llama 3显著提升了处理复杂任务的能力。</p>\n<h2 id=\"训练数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练数据\"><span>训练数据</span></a></h2>\n<p>Llama 3采用了精心设计的预训练语料库，扩展到15T Tokens，代码数据扩充了4倍，显著提升了模型在代码能力和逻辑推理能力方面的表现。数据来源包括30多种语言，超过5%的非英语token。这些措施不仅提高了英语内容处理效率，也增强了多语言处理能力。</p>\n<h3 id=\"数据过滤流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据过滤流程\"><span>数据过滤流程</span></a></h3>\n<p>Meta开发了一系列数据过滤工具，包括启发式过滤器、NSFW过滤器、语义重复数据删除技术及预测数据质量的文本分类器。这些工具确保了高质量数据的选择。</p>\n<h2 id=\"训练流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练流程\"><span>训练流程</span></a></h2>\n<p>Llama-3系列包括两个模型：预训练模型Llama-3和微调后的模型Llama-3-Instruct。</p>\n<h3 id=\"整体流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#整体流程\"><span>整体流程</span></a></h3>\n<ol>\n<li>✅ 初始预训练</li>\n<li>⚠ 长上下文预训练</li>\n<li>❗ 退火（Annealing）</li>\n</ol>\n<p>后训练阶段包括监督式微调（SFT）、拒绝采样、RLHF和直接微调等步骤。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：LLaMA-3没有采用MOE结构，这可能导致在性能上无法与同规模的密集型模型相比。随着模型规模的扩大，如何降低推理成本将成为一个需要关注的问题。</p>\n</blockquote>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>Llama 3 8B</th>\n<th>Llama 3 70B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>上下文长度</td>\n<td>8k</td>\n<td>8k</td>\n</tr>\n<tr>\n<td>GQA</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Token数量</td>\n<td>15T+</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何进一步优化多语言处理能力。</li>\n<li>探讨降低模型推理成本的方法。</li>\n<li>开发更高效的数据过滤技术以提升数据质量。</li>\n</ul>\n<blockquote>\n<p>原文出处: &quot;The Llama 3 Herd of Models&quot;</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>: 人工智能模型</p>\n<p><strong>标签</strong>: Llama3, 多语言处理, 机器学习, 模型优化</p>\n<p><strong>日期</strong>: 2025年4月12日</p>\n<hr>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>Llama 3系列模型是Meta公司推出的最新人工智能模型，包含Llama3和Llama3.1。该系列模型在多语言处理、长文本处理和工具使用方面进行了显著的改进。本文将深入解析Llama 3的模型结构、训练数据及训练流程。\n<img src=\"/img/user/附件/Pasted image 20250424223641.png\" alt=\"Pasted image 20250424223641.png\"></p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>与之前的LLaMA2相比，LLaMA3进行了以下改进：</p>\n<ul>\n<li><strong>词表扩展</strong>: tokenizer由sentencepiece更换为tiktoken，词表大小从32k扩展到128k。</li>\n<li><strong>上下文长度</strong>: 上下文长度扩展到了8k，预训练后期通过多阶段长文本训练达到了128K。</li>\n<li><strong>GQA技术应用</strong>: LLaMA3 8B和70B模型均采用了GQA技术。</li>\n</ul>\n<p>💡启发点：通过扩展词表和上下文长度，Llama 3显著提升了处理复杂任务的能力。</p>\n<h2 id=\"训练数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练数据\"><span>训练数据</span></a></h2>\n<p>Llama 3采用了精心设计的预训练语料库，扩展到15T Tokens，代码数据扩充了4倍，显著提升了模型在代码能力和逻辑推理能力方面的表现。数据来源包括30多种语言，超过5%的非英语token。这些措施不仅提高了英语内容处理效率，也增强了多语言处理能力。</p>\n<h3 id=\"数据过滤流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据过滤流程\"><span>数据过滤流程</span></a></h3>\n<p>Meta开发了一系列数据过滤工具，包括启发式过滤器、NSFW过滤器、语义重复数据删除技术及预测数据质量的文本分类器。这些工具确保了高质量数据的选择。</p>\n<h2 id=\"训练流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练流程\"><span>训练流程</span></a></h2>\n<p>Llama-3系列包括两个模型：预训练模型Llama-3和微调后的模型Llama-3-Instruct。</p>\n<h3 id=\"整体流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#整体流程\"><span>整体流程</span></a></h3>\n<ol>\n<li>✅ 初始预训练</li>\n<li>⚠ 长上下文预训练</li>\n<li>❗ 退火（Annealing）</li>\n</ol>\n<p>后训练阶段包括监督式微调（SFT）、拒绝采样、RLHF和直接微调等步骤。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：LLaMA-3没有采用MOE结构，这可能导致在性能上无法与同规模的密集型模型相比。随着模型规模的扩大，如何降低推理成本将成为一个需要关注的问题。</p>\n</blockquote>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>Llama 3 8B</th>\n<th>Llama 3 70B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>上下文长度</td>\n<td>8k</td>\n<td>8k</td>\n</tr>\n<tr>\n<td>GQA</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Token数量</td>\n<td>15T+</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何进一步优化多语言处理能力。</li>\n<li>探讨降低模型推理成本的方法。</li>\n<li>开发更高效的数据过滤技术以提升数据质量。</li>\n</ul>\n<blockquote>\n<p>原文出处: &quot;The Llama 3 Herd of Models&quot;</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n**分类**: 人工智能模型\n\n**标签**: Llama3, 多语言处理, 机器学习, 模型优化\n\n**日期**: 2025年4月12日\n\n---\n\n\n## 内容概述\nLlama 3系列模型是Meta公司推出的最新人工智能模型，包含Llama3和Llama3.1。该系列模型在多语言处理、长文本处理和工具使用方面进行了显著的改进。本文将深入解析Llama 3的模型结构、训练数据及训练流程。\n![Pasted image 20250424223641.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424223641.png)\n\n\n## 模型结构\n与之前的LLaMA2相比，LLaMA3进行了以下改进：\n- **词表扩展**: tokenizer由sentencepiece更换为tiktoken，词表大小从32k扩展到128k。\n- **上下文长度**: 上下文长度扩展到了8k，预训练后期通过多阶段长文本训练达到了128K。\n- **GQA技术应用**: LLaMA3 8B和70B模型均采用了GQA技术。\n\n💡启发点：通过扩展词表和上下文长度，Llama 3显著提升了处理复杂任务的能力。\n\n\n## 训练数据\nLlama 3采用了精心设计的预训练语料库，扩展到15T Tokens，代码数据扩充了4倍，显著提升了模型在代码能力和逻辑推理能力方面的表现。数据来源包括30多种语言，超过5%的非英语token。这些措施不仅提高了英语内容处理效率，也增强了多语言处理能力。\n\n### 数据过滤流程\nMeta开发了一系列数据过滤工具，包括启发式过滤器、NSFW过滤器、语义重复数据删除技术及预测数据质量的文本分类器。这些工具确保了高质量数据的选择。\n\n\n## 训练流程\nLlama-3系列包括两个模型：预训练模型Llama-3和微调后的模型Llama-3-Instruct。\n\n### 整体流程\n1. ✅ 初始预训练\n2. ⚠ 长上下文预训练\n3. ❗ 退火（Annealing）\n\n后训练阶段包括监督式微调（SFT）、拒绝采样、RLHF和直接微调等步骤。\n\n\n## 常见错误\n> 警告：LLaMA-3没有采用MOE结构，这可能导致在性能上无法与同规模的密集型模型相比。随着模型规模的扩大，如何降低推理成本将成为一个需要关注的问题。\n\n\n## 数据表格\n| 参数       | Llama 3 8B | Llama 3 70B |\n|------------|------------|-------------|\n| 上下文长度 | 8k         | 8k          |\n| GQA        | Yes        | Yes         |\n| Token数量  | 15T+       | -           |\n\n\n## 行动清单\n- 研究如何进一步优化多语言处理能力。\n- 探讨降低模型推理成本的方法。\n- 开发更高效的数据过滤技术以提升数据质量。\n\n> 原文出处: \"The Llama 3 Herd of Models\"","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容概述","slug":"内容概述","link":"#内容概述","children":[]},{"level":2,"title":"模型结构","slug":"模型结构","link":"#模型结构","children":[]},{"level":2,"title":"训练数据","slug":"训练数据","link":"#训练数据","children":[{"level":3,"title":"数据过滤流程","slug":"数据过滤流程","link":"#数据过滤流程","children":[]}]},{"level":2,"title":"训练流程","slug":"训练流程","link":"#训练流程","children":[{"level":3,"title":"整体流程","slug":"整体流程","link":"#整体流程","children":[]}]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"数据表格","slug":"数据表格","link":"#数据表格","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
