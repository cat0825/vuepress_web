{"content":"<h2 id=\"关于大语言模型学习导航\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关于大语言模型学习导航\"><span><strong>关于大语言模型学习导航</strong></span></a></h2>\n<ul>\n<li>[[大语言模型学习|大语言模型学习]]\n<ul>\n<li>[[Attention注意力机制|Attention注意力机制]]\n<ul>\n<li>[[大语言模型学习/Attention注意力机制/Attention机制详解与应用|Attention机制详解与应用]]</li>\n<li>[[大语言模型学习/Attention注意力机制/DCA：长文本处理的新突破（Dual Chunk Attention）|DCA：长文本处理的新突破（Dual Chunk Attention）]]</li>\n<li>[[大语言模型学习/Attention注意力机制/KV Cache技术详解：优化Transformer自回归生成效率|KV Cache技术详解：优化Transformer自回归生成效率]]</li>\n<li>[[大语言模型学习/Attention注意力机制/Transformer中的Attention详解与应用指南|Transformer中的Attention详解与应用指南]]</li>\n<li>[[大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted Sparse Attention的创新方法|【长上下文模型优化】基于Shifted Sparse Attention的创新方法]]</li>\n<li>[[大语言模型学习/Attention注意力机制/优化Attention计算复杂度的技术探讨|优化Attention计算复杂度的技术探讨]]</li>\n<li>[[大语言模型学习/Attention注意力机制/深度学习中的注意力机制优化：从MHA到MLA|深度学习中的注意力机制优化：从MHA到MLA]]</li>\n</ul>\n</li>\n<li>[[FFN、Add &amp; LN 的作用与应用|FFN、Add &amp; LN 的作用与应用]]\n<ul>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/Transformer核心模块解析：FFN、Add &amp; LN 的作用与应用|Transformer核心模块解析：FFN、Add &amp; LN 的作用与应用]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较|深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析|激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/激活函数详解与比较：从Sigmoid到Swish|激活函数详解与比较：从Sigmoid到Swish]]</li>\n</ul>\n</li>\n<li>[[Positional Encoding位置编码|Positional Encoding位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/NTK插值方法解析与优化：从NTK-aware到NTK-by-parts|NTK插值方法解析与优化：从NTK-aware到NTK-by-parts]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/YaRN方法解析：扩展RoPE嵌入与注意力优化的实践|YaRN方法解析：扩展RoPE嵌入与注意力优化的实践]]</li>\n<li>[[大语言模型学习/词嵌入/介绍|位置编码介绍]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/位置内插法扩展语言模型上下文长度|位置内插法扩展语言模型上下文长度]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/数字输入优化与外推方法解析|数字输入优化与外推方法解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/旋转位置编码与ALiBi：深度学习中的位置嵌入优化|旋转位置编码与ALiBi：深度学习中的位置嵌入优化]]</li>\n<li>[[相对位置编码|相对位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/DeBERTa的相对位置编码与绝对位置编码解析|DeBERTa的相对位置编码与绝对位置编码解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/T5模型与相对位置编码优化解析|T5模型与相对位置编码优化解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/相对位置编码与XLNet位置编码详解 深入理解Transformer机制|相对位置编码与XLNet位置编码详解 深入理解Transformer机制]]</li>\n</ul>\n</li>\n<li>[[绝对位置编码|绝对位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/绝对位置编码/BERT与RNN位置编码的对比与应用|BERT与RNN位置编码的对比与应用]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/绝对位置编码/Transformer绝对位置编码详解与改进分析|Transformer绝对位置编码详解与改进分析]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[Pre-training 预训练|Pre-training 预训练]]\n<ul>\n<li>[[大语言模型学习/Pre-training 预训练/推理耗时|大语言模型学习/Pre-training 预训练/推理耗时]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据多样性与模型优化探索|数据多样性与模型优化探索]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据清洗|数据清洗]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据爬取|数据爬取]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据配比与训练顺序优化指南|数据配比与训练顺序优化指南]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/模型打分与数据去重|模型打分与数据去重]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/深度学习中的显存优化与梯度处理方法|深度学习中的显存优化与梯度处理方法]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/混合精度训练|混合精度训练]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/继续预训练|继续预训练]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/训练容灾及训练监控|训练容灾及训练监控]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练定义以及数据来源|预训练定义以及数据来源]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练评估|预训练评估]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练评估2|预训练评估2]]</li>\n<li>[[预训练过程|预训练过程]]\n<ul>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/训练Tokenizer|训练Tokenizer]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/预训练的Scaling Law|预训练的Scaling Law]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/预训练策略|预训练策略]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/高效深度学习模型训练框架选择与优化指南|高效深度学习模型训练框架选择与优化指南]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[RL强化学习基础|RL强化学习基础]]\n<ul>\n<li>[[大语言模型学习/RL强化学习基础/SARSA-λ与Q-learning对比|SARSA-λ与Q-learning对比]]</li>\n<li>[[大语言模型学习/RL强化学习基础/SARSA算法|SARSA算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/价值迭代算法|价值迭代算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习分类|强化学习分类]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习的独特性|强化学习的独特性]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习问题,流程|强化学习问题,流程]]</li>\n<li>[[大语言模型学习/RL强化学习基础/时序差分算法|时序差分算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/深度Q网络|深度Q网络]]</li>\n<li>[[大语言模型学习/RL强化学习基础/策略迭代算法|策略迭代算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/蒙特卡洛方法|蒙特卡洛方法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/贝尔曼方程|贝尔曼方程]]</li>\n<li>[[大语言模型学习/RL强化学习基础/马尔可夫决策过程|马尔可夫决策过程]]</li>\n</ul>\n</li>\n<li>[[Structure &amp; Decoding Policy 结构和解码策略|Structure &amp; Decoding Policy 结构和解码策略]]\n<ul>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/大模型结构与混合专家（LLM &amp; MoE）解析|大模型结构与混合专家（LLM &amp; MoE）解析]]</li>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略|深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略]]</li>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/解码采样策略：Greedy Search与Beam Search的实现与优化|解码采样策略：Greedy Search与Beam Search的实现与优化]]</li>\n</ul>\n</li>\n<li>[[分词|分词]]\n<ul>\n<li>[[大语言模型学习/分词/BBPE：字节级别的BPE分词技术解析与应用|BBPE：字节级别的BPE分词技术解析与应用]]</li>\n<li>[[大语言模型学习/分词/WordPiece分词算法解析与实践|WordPiece分词算法解析与实践]]</li>\n<li>[[大语言模型学习/分词/使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践|使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践]]</li>\n<li>[[大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践|使用Unigram语言模型（ULM）优化分词算法：核心思路与实践]]</li>\n<li>[[大语言模型学习/分词/分词算法的比较|分词算法的比较]]</li>\n<li>[[大语言模型学习/分词/常用分词库|常用分词库]]</li>\n</ul>\n</li>\n<li>[[后训练|后训练]]\n<ul>\n<li>[[SFT监督微调|SFT监督微调]]\n<ul>\n<li>[[SFT数据及处理|SFT数据及处理]]\n<ul>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/开源数据集|开源数据集]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据多样性探索|数据多样性探索]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据生产合成与质量过滤|数据生产合成与质量过滤]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据飞轮在SFT中的应用与优化|数据飞轮在SFT中的应用与优化]]</li>\n</ul>\n</li>\n<li>[[STF训练|STF训练]]\n<ul>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升|多轮对话专项提升]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2|多轮对话专项提升2]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练启动脚本|训练启动脚本]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略|训练技巧和训练策略]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练框架及参数设置|训练框架及参数设置]]</li>\n</ul>\n</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别|监督微调与预训练的区别]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[词嵌入|词嵌入]]\n<ul>\n<li>[[大语言模型学习/词嵌入/FastText|FastText]]</li>\n<li>[[大语言模型学习/词嵌入/oneHot|oneHot]]</li>\n<li>[[大语言模型学习/词嵌入/Word2Vec|Word2Vec]]</li>\n<li>[[大语言模型学习/词嵌入/介绍|词嵌入介绍]]</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/Welcome🎉.md","filePathRelative":"notes_bak/Welcome🎉.md","frontmatter":{"dg-publish":true,"dg-home":true,"permalink":"/welcome/","tags":["gardenEntry"],"dgPassFrontmatter":true,"noteIcon":null,"created":"2024-12-24T05:56:28.000Z","updated":"2025-04-30T14:32:50.692Z","title":"Welcome🎉","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"关于大语言模型学习导航\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关于大语言模型学习导航\"><span><strong>关于大语言模型学习导航</strong></span></a></h2>\n<ul>\n<li>[[大语言模型学习|大语言模型学习]]\n<ul>\n<li>[[Attention注意力机制|Attention注意力机制]]\n<ul>\n<li>[[大语言模型学习/Attention注意力机制/Attention机制详解与应用|Attention机制详解与应用]]</li>\n<li>[[大语言模型学习/Attention注意力机制/DCA：长文本处理的新突破（Dual Chunk Attention）|DCA：长文本处理的新突破（Dual Chunk Attention）]]</li>\n<li>[[大语言模型学习/Attention注意力机制/KV Cache技术详解：优化Transformer自回归生成效率|KV Cache技术详解：优化Transformer自回归生成效率]]</li>\n<li>[[大语言模型学习/Attention注意力机制/Transformer中的Attention详解与应用指南|Transformer中的Attention详解与应用指南]]</li>\n<li>[[大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted Sparse Attention的创新方法|【长上下文模型优化】基于Shifted Sparse Attention的创新方法]]</li>\n<li>[[大语言模型学习/Attention注意力机制/优化Attention计算复杂度的技术探讨|优化Attention计算复杂度的技术探讨]]</li>\n<li>[[大语言模型学习/Attention注意力机制/深度学习中的注意力机制优化：从MHA到MLA|深度学习中的注意力机制优化：从MHA到MLA]]</li>\n</ul>\n</li>\n<li>[[FFN、Add &amp; LN 的作用与应用|FFN、Add &amp; LN 的作用与应用]]\n<ul>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/Transformer核心模块解析：FFN、Add &amp; LN 的作用与应用|Transformer核心模块解析：FFN、Add &amp; LN 的作用与应用]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较|深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析|激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/激活函数详解与比较：从Sigmoid到Swish|激活函数详解与比较：从Sigmoid到Swish]]</li>\n</ul>\n</li>\n<li>[[Positional Encoding位置编码|Positional Encoding位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/NTK插值方法解析与优化：从NTK-aware到NTK-by-parts|NTK插值方法解析与优化：从NTK-aware到NTK-by-parts]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/YaRN方法解析：扩展RoPE嵌入与注意力优化的实践|YaRN方法解析：扩展RoPE嵌入与注意力优化的实践]]</li>\n<li>[[大语言模型学习/词嵌入/介绍|位置编码介绍]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/位置内插法扩展语言模型上下文长度|位置内插法扩展语言模型上下文长度]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/数字输入优化与外推方法解析|数字输入优化与外推方法解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/旋转位置编码与ALiBi：深度学习中的位置嵌入优化|旋转位置编码与ALiBi：深度学习中的位置嵌入优化]]</li>\n<li>[[相对位置编码|相对位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/DeBERTa的相对位置编码与绝对位置编码解析|DeBERTa的相对位置编码与绝对位置编码解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/T5模型与相对位置编码优化解析|T5模型与相对位置编码优化解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/相对位置编码与XLNet位置编码详解 深入理解Transformer机制|相对位置编码与XLNet位置编码详解 深入理解Transformer机制]]</li>\n</ul>\n</li>\n<li>[[绝对位置编码|绝对位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/绝对位置编码/BERT与RNN位置编码的对比与应用|BERT与RNN位置编码的对比与应用]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/绝对位置编码/Transformer绝对位置编码详解与改进分析|Transformer绝对位置编码详解与改进分析]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[Pre-training 预训练|Pre-training 预训练]]\n<ul>\n<li>[[大语言模型学习/Pre-training 预训练/推理耗时|大语言模型学习/Pre-training 预训练/推理耗时]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据多样性与模型优化探索|数据多样性与模型优化探索]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据清洗|数据清洗]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据爬取|数据爬取]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据配比与训练顺序优化指南|数据配比与训练顺序优化指南]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/模型打分与数据去重|模型打分与数据去重]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/深度学习中的显存优化与梯度处理方法|深度学习中的显存优化与梯度处理方法]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/混合精度训练|混合精度训练]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/继续预训练|继续预训练]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/训练容灾及训练监控|训练容灾及训练监控]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练定义以及数据来源|预训练定义以及数据来源]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练评估|预训练评估]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练评估2|预训练评估2]]</li>\n<li>[[预训练过程|预训练过程]]\n<ul>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/训练Tokenizer|训练Tokenizer]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/预训练的Scaling Law|预训练的Scaling Law]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/预训练策略|预训练策略]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/高效深度学习模型训练框架选择与优化指南|高效深度学习模型训练框架选择与优化指南]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[RL强化学习基础|RL强化学习基础]]\n<ul>\n<li>[[大语言模型学习/RL强化学习基础/SARSA-λ与Q-learning对比|SARSA-λ与Q-learning对比]]</li>\n<li>[[大语言模型学习/RL强化学习基础/SARSA算法|SARSA算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/价值迭代算法|价值迭代算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习分类|强化学习分类]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习的独特性|强化学习的独特性]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习问题,流程|强化学习问题,流程]]</li>\n<li>[[大语言模型学习/RL强化学习基础/时序差分算法|时序差分算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/深度Q网络|深度Q网络]]</li>\n<li>[[大语言模型学习/RL强化学习基础/策略迭代算法|策略迭代算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/蒙特卡洛方法|蒙特卡洛方法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/贝尔曼方程|贝尔曼方程]]</li>\n<li>[[大语言模型学习/RL强化学习基础/马尔可夫决策过程|马尔可夫决策过程]]</li>\n</ul>\n</li>\n<li>[[Structure &amp; Decoding Policy 结构和解码策略|Structure &amp; Decoding Policy 结构和解码策略]]\n<ul>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/大模型结构与混合专家（LLM &amp; MoE）解析|大模型结构与混合专家（LLM &amp; MoE）解析]]</li>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略|深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略]]</li>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/解码采样策略：Greedy Search与Beam Search的实现与优化|解码采样策略：Greedy Search与Beam Search的实现与优化]]</li>\n</ul>\n</li>\n<li>[[分词|分词]]\n<ul>\n<li>[[大语言模型学习/分词/BBPE：字节级别的BPE分词技术解析与应用|BBPE：字节级别的BPE分词技术解析与应用]]</li>\n<li>[[大语言模型学习/分词/WordPiece分词算法解析与实践|WordPiece分词算法解析与实践]]</li>\n<li>[[大语言模型学习/分词/使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践|使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践]]</li>\n<li>[[大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践|使用Unigram语言模型（ULM）优化分词算法：核心思路与实践]]</li>\n<li>[[大语言模型学习/分词/分词算法的比较|分词算法的比较]]</li>\n<li>[[大语言模型学习/分词/常用分词库|常用分词库]]</li>\n</ul>\n</li>\n<li>[[后训练|后训练]]\n<ul>\n<li>[[SFT监督微调|SFT监督微调]]\n<ul>\n<li>[[SFT数据及处理|SFT数据及处理]]\n<ul>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/开源数据集|开源数据集]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据多样性探索|数据多样性探索]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据生产合成与质量过滤|数据生产合成与质量过滤]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据飞轮在SFT中的应用与优化|数据飞轮在SFT中的应用与优化]]</li>\n</ul>\n</li>\n<li>[[STF训练|STF训练]]\n<ul>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升|多轮对话专项提升]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2|多轮对话专项提升2]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练启动脚本|训练启动脚本]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略|训练技巧和训练策略]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练框架及参数设置|训练框架及参数设置]]</li>\n</ul>\n</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别|监督微调与预训练的区别]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[词嵌入|词嵌入]]\n<ul>\n<li>[[大语言模型学习/词嵌入/FastText|FastText]]</li>\n<li>[[大语言模型学习/词嵌入/oneHot|oneHot]]</li>\n<li>[[大语言模型学习/词嵌入/Word2Vec|Word2Vec]]</li>\n<li>[[大语言模型学习/词嵌入/介绍|词嵌入介绍]]</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</template>","contentStripped":"<h2 id=\"关于大语言模型学习导航\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关于大语言模型学习导航\"><span><strong>关于大语言模型学习导航</strong></span></a></h2>\n<ul>\n<li>[[大语言模型学习|大语言模型学习]]\n<ul>\n<li>[[Attention注意力机制|Attention注意力机制]]\n<ul>\n<li>[[大语言模型学习/Attention注意力机制/Attention机制详解与应用|Attention机制详解与应用]]</li>\n<li>[[大语言模型学习/Attention注意力机制/DCA：长文本处理的新突破（Dual Chunk Attention）|DCA：长文本处理的新突破（Dual Chunk Attention）]]</li>\n<li>[[大语言模型学习/Attention注意力机制/KV Cache技术详解：优化Transformer自回归生成效率|KV Cache技术详解：优化Transformer自回归生成效率]]</li>\n<li>[[大语言模型学习/Attention注意力机制/Transformer中的Attention详解与应用指南|Transformer中的Attention详解与应用指南]]</li>\n<li>[[大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted Sparse Attention的创新方法|【长上下文模型优化】基于Shifted Sparse Attention的创新方法]]</li>\n<li>[[大语言模型学习/Attention注意力机制/优化Attention计算复杂度的技术探讨|优化Attention计算复杂度的技术探讨]]</li>\n<li>[[大语言模型学习/Attention注意力机制/深度学习中的注意力机制优化：从MHA到MLA|深度学习中的注意力机制优化：从MHA到MLA]]</li>\n</ul>\n</li>\n<li>[[FFN、Add &amp; LN 的作用与应用|FFN、Add &amp; LN 的作用与应用]]\n<ul>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/Transformer核心模块解析：FFN、Add &amp; LN 的作用与应用|Transformer核心模块解析：FFN、Add &amp; LN 的作用与应用]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较|深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析|激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析]]</li>\n<li>[[大语言模型学习/FFN、Add &amp; LN 的作用与应用/激活函数详解与比较：从Sigmoid到Swish|激活函数详解与比较：从Sigmoid到Swish]]</li>\n</ul>\n</li>\n<li>[[Positional Encoding位置编码|Positional Encoding位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/NTK插值方法解析与优化：从NTK-aware到NTK-by-parts|NTK插值方法解析与优化：从NTK-aware到NTK-by-parts]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/YaRN方法解析：扩展RoPE嵌入与注意力优化的实践|YaRN方法解析：扩展RoPE嵌入与注意力优化的实践]]</li>\n<li>[[大语言模型学习/词嵌入/介绍|位置编码介绍]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/位置内插法扩展语言模型上下文长度|位置内插法扩展语言模型上下文长度]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/数字输入优化与外推方法解析|数字输入优化与外推方法解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/旋转位置编码与ALiBi：深度学习中的位置嵌入优化|旋转位置编码与ALiBi：深度学习中的位置嵌入优化]]</li>\n<li>[[相对位置编码|相对位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/DeBERTa的相对位置编码与绝对位置编码解析|DeBERTa的相对位置编码与绝对位置编码解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/T5模型与相对位置编码优化解析|T5模型与相对位置编码优化解析]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/相对位置编码/相对位置编码与XLNet位置编码详解 深入理解Transformer机制|相对位置编码与XLNet位置编码详解 深入理解Transformer机制]]</li>\n</ul>\n</li>\n<li>[[绝对位置编码|绝对位置编码]]\n<ul>\n<li>[[大语言模型学习/Positional Encoding位置编码/绝对位置编码/BERT与RNN位置编码的对比与应用|BERT与RNN位置编码的对比与应用]]</li>\n<li>[[大语言模型学习/Positional Encoding位置编码/绝对位置编码/Transformer绝对位置编码详解与改进分析|Transformer绝对位置编码详解与改进分析]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[Pre-training 预训练|Pre-training 预训练]]\n<ul>\n<li>[[大语言模型学习/Pre-training 预训练/推理耗时|大语言模型学习/Pre-training 预训练/推理耗时]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据多样性与模型优化探索|数据多样性与模型优化探索]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据清洗|数据清洗]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据爬取|数据爬取]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/数据配比与训练顺序优化指南|数据配比与训练顺序优化指南]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/模型打分与数据去重|模型打分与数据去重]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/深度学习中的显存优化与梯度处理方法|深度学习中的显存优化与梯度处理方法]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/混合精度训练|混合精度训练]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/继续预训练|继续预训练]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/训练容灾及训练监控|训练容灾及训练监控]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练定义以及数据来源|预训练定义以及数据来源]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练评估|预训练评估]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练评估2|预训练评估2]]</li>\n<li>[[预训练过程|预训练过程]]\n<ul>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/训练Tokenizer|训练Tokenizer]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/预训练的Scaling Law|预训练的Scaling Law]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/预训练策略|预训练策略]]</li>\n<li>[[大语言模型学习/Pre-training 预训练/预训练过程/高效深度学习模型训练框架选择与优化指南|高效深度学习模型训练框架选择与优化指南]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[RL强化学习基础|RL强化学习基础]]\n<ul>\n<li>[[大语言模型学习/RL强化学习基础/SARSA-λ与Q-learning对比|SARSA-λ与Q-learning对比]]</li>\n<li>[[大语言模型学习/RL强化学习基础/SARSA算法|SARSA算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/价值迭代算法|价值迭代算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习分类|强化学习分类]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习的独特性|强化学习的独特性]]</li>\n<li>[[大语言模型学习/RL强化学习基础/强化学习问题,流程|强化学习问题,流程]]</li>\n<li>[[大语言模型学习/RL强化学习基础/时序差分算法|时序差分算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/深度Q网络|深度Q网络]]</li>\n<li>[[大语言模型学习/RL强化学习基础/策略迭代算法|策略迭代算法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/蒙特卡洛方法|蒙特卡洛方法]]</li>\n<li>[[大语言模型学习/RL强化学习基础/贝尔曼方程|贝尔曼方程]]</li>\n<li>[[大语言模型学习/RL强化学习基础/马尔可夫决策过程|马尔可夫决策过程]]</li>\n</ul>\n</li>\n<li>[[Structure &amp; Decoding Policy 结构和解码策略|Structure &amp; Decoding Policy 结构和解码策略]]\n<ul>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/大模型结构与混合专家（LLM &amp; MoE）解析|大模型结构与混合专家（LLM &amp; MoE）解析]]</li>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略|深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略]]</li>\n<li>[[大语言模型学习/Structure &amp; Decoding Policy 结构和解码策略/解码采样策略：Greedy Search与Beam Search的实现与优化|解码采样策略：Greedy Search与Beam Search的实现与优化]]</li>\n</ul>\n</li>\n<li>[[分词|分词]]\n<ul>\n<li>[[大语言模型学习/分词/BBPE：字节级别的BPE分词技术解析与应用|BBPE：字节级别的BPE分词技术解析与应用]]</li>\n<li>[[大语言模型学习/分词/WordPiece分词算法解析与实践|WordPiece分词算法解析与实践]]</li>\n<li>[[大语言模型学习/分词/使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践|使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践]]</li>\n<li>[[大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践|使用Unigram语言模型（ULM）优化分词算法：核心思路与实践]]</li>\n<li>[[大语言模型学习/分词/分词算法的比较|分词算法的比较]]</li>\n<li>[[大语言模型学习/分词/常用分词库|常用分词库]]</li>\n</ul>\n</li>\n<li>[[后训练|后训练]]\n<ul>\n<li>[[SFT监督微调|SFT监督微调]]\n<ul>\n<li>[[SFT数据及处理|SFT数据及处理]]\n<ul>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/开源数据集|开源数据集]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据多样性探索|数据多样性探索]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据生产合成与质量过滤|数据生产合成与质量过滤]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据飞轮在SFT中的应用与优化|数据飞轮在SFT中的应用与优化]]</li>\n</ul>\n</li>\n<li>[[STF训练|STF训练]]\n<ul>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升|多轮对话专项提升]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2|多轮对话专项提升2]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练启动脚本|训练启动脚本]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略|训练技巧和训练策略]]</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/STF训练/训练框架及参数设置|训练框架及参数设置]]</li>\n</ul>\n</li>\n<li>[[大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别|监督微调与预训练的区别]]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[[词嵌入|词嵌入]]\n<ul>\n<li>[[大语言模型学习/词嵌入/FastText|FastText]]</li>\n<li>[[大语言模型学习/词嵌入/oneHot|oneHot]]</li>\n<li>[[大语言模型学习/词嵌入/Word2Vec|Word2Vec]]</li>\n<li>[[大语言模型学习/词嵌入/介绍|词嵌入介绍]]</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## **关于大语言模型学习导航**\n- [[大语言模型学习\\|大语言模型学习]]\n  - [[Attention注意力机制\\|Attention注意力机制]]\n    - [[大语言模型学习/Attention注意力机制/Attention机制详解与应用\\|Attention机制详解与应用]]\n    - [[大语言模型学习/Attention注意力机制/DCA：长文本处理的新突破（Dual Chunk Attention）\\|DCA：长文本处理的新突破（Dual Chunk Attention）]]\n    - [[大语言模型学习/Attention注意力机制/KV Cache技术详解：优化Transformer自回归生成效率\\|KV Cache技术详解：优化Transformer自回归生成效率]]\n    - [[大语言模型学习/Attention注意力机制/Transformer中的Attention详解与应用指南\\|Transformer中的Attention详解与应用指南]]\n    - [[大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted Sparse Attention的创新方法\\|【长上下文模型优化】基于Shifted Sparse Attention的创新方法]]\n    - [[大语言模型学习/Attention注意力机制/优化Attention计算复杂度的技术探讨\\|优化Attention计算复杂度的技术探讨]]\n    - [[大语言模型学习/Attention注意力机制/深度学习中的注意力机制优化：从MHA到MLA\\|深度学习中的注意力机制优化：从MHA到MLA]]\n  - [[FFN、Add & LN 的作用与应用\\|FFN、Add & LN 的作用与应用]]\n    - [[大语言模型学习/FFN、Add & LN 的作用与应用/Transformer核心模块解析：FFN、Add & LN 的作用与应用\\|Transformer核心模块解析：FFN、Add & LN 的作用与应用]]\n    - [[大语言模型学习/FFN、Add & LN 的作用与应用/深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较\\|深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较]]\n    - [[大语言模型学习/FFN、Add & LN 的作用与应用/激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析\\|激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析]]\n    - [[大语言模型学习/FFN、Add & LN 的作用与应用/激活函数详解与比较：从Sigmoid到Swish\\|激活函数详解与比较：从Sigmoid到Swish]]\n  - [[Positional Encoding位置编码\\|Positional Encoding位置编码]]\n    - [[大语言模型学习/Positional Encoding位置编码/NTK插值方法解析与优化：从NTK-aware到NTK-by-parts\\|NTK插值方法解析与优化：从NTK-aware到NTK-by-parts]]\n    - [[大语言模型学习/Positional Encoding位置编码/YaRN方法解析：扩展RoPE嵌入与注意力优化的实践\\|YaRN方法解析：扩展RoPE嵌入与注意力优化的实践]]\n    - [[大语言模型学习/词嵌入/介绍\\|位置编码介绍]]\n    - [[大语言模型学习/Positional Encoding位置编码/位置内插法扩展语言模型上下文长度\\|位置内插法扩展语言模型上下文长度]]\n    - [[大语言模型学习/Positional Encoding位置编码/数字输入优化与外推方法解析\\|数字输入优化与外推方法解析]]\n    - [[大语言模型学习/Positional Encoding位置编码/旋转位置编码与ALiBi：深度学习中的位置嵌入优化\\|旋转位置编码与ALiBi：深度学习中的位置嵌入优化]]\n    - [[相对位置编码\\|相对位置编码]]\n      - [[大语言模型学习/Positional Encoding位置编码/相对位置编码/DeBERTa的相对位置编码与绝对位置编码解析\\|DeBERTa的相对位置编码与绝对位置编码解析]]\n      - [[大语言模型学习/Positional Encoding位置编码/相对位置编码/T5模型与相对位置编码优化解析\\|T5模型与相对位置编码优化解析]]\n      - [[大语言模型学习/Positional Encoding位置编码/相对位置编码/相对位置编码与XLNet位置编码详解 深入理解Transformer机制\\|相对位置编码与XLNet位置编码详解 深入理解Transformer机制]]\n    - [[绝对位置编码\\|绝对位置编码]]\n      - [[大语言模型学习/Positional Encoding位置编码/绝对位置编码/BERT与RNN位置编码的对比与应用\\|BERT与RNN位置编码的对比与应用]]\n      - [[大语言模型学习/Positional Encoding位置编码/绝对位置编码/Transformer绝对位置编码详解与改进分析\\|Transformer绝对位置编码详解与改进分析]]\n  - [[Pre-training 预训练\\|Pre-training 预训练]]\n    - [[大语言模型学习/Pre-training 预训练/推理耗时\\|大语言模型学习/Pre-training 预训练/推理耗时]]\n    - [[大语言模型学习/Pre-training 预训练/数据多样性与模型优化探索\\|数据多样性与模型优化探索]]\n    - [[大语言模型学习/Pre-training 预训练/数据清洗\\|数据清洗]]\n    - [[大语言模型学习/Pre-training 预训练/数据爬取\\|数据爬取]]\n    - [[大语言模型学习/Pre-training 预训练/数据配比与训练顺序优化指南\\|数据配比与训练顺序优化指南]]\n    - [[大语言模型学习/Pre-training 预训练/模型打分与数据去重\\|模型打分与数据去重]]\n    - [[大语言模型学习/Pre-training 预训练/深度学习中的显存优化与梯度处理方法\\|深度学习中的显存优化与梯度处理方法]]\n    - [[大语言模型学习/Pre-training 预训练/混合精度训练\\|混合精度训练]]\n    - [[大语言模型学习/Pre-training 预训练/继续预训练\\|继续预训练]]\n    - [[大语言模型学习/Pre-training 预训练/训练容灾及训练监控\\|训练容灾及训练监控]]\n    - [[大语言模型学习/Pre-training 预训练/预训练定义以及数据来源\\|预训练定义以及数据来源]]\n    - [[大语言模型学习/Pre-training 预训练/预训练评估\\|预训练评估]]\n    - [[大语言模型学习/Pre-training 预训练/预训练评估2\\|预训练评估2]]\n    - [[预训练过程\\|预训练过程]]\n      - [[大语言模型学习/Pre-training 预训练/预训练过程/训练Tokenizer\\|训练Tokenizer]]\n      - [[大语言模型学习/Pre-training 预训练/预训练过程/预训练的Scaling Law\\|预训练的Scaling Law]]\n      - [[大语言模型学习/Pre-training 预训练/预训练过程/预训练策略\\|预训练策略]]\n      - [[大语言模型学习/Pre-training 预训练/预训练过程/高效深度学习模型训练框架选择与优化指南\\|高效深度学习模型训练框架选择与优化指南]]\n  - [[RL强化学习基础\\|RL强化学习基础]]\n    - [[大语言模型学习/RL强化学习基础/SARSA-λ与Q-learning对比\\|SARSA-λ与Q-learning对比]]\n    - [[大语言模型学习/RL强化学习基础/SARSA算法\\|SARSA算法]]\n    - [[大语言模型学习/RL强化学习基础/价值迭代算法\\|价值迭代算法]]\n    - [[大语言模型学习/RL强化学习基础/强化学习分类\\|强化学习分类]]\n    - [[大语言模型学习/RL强化学习基础/强化学习的独特性\\|强化学习的独特性]]\n    - [[大语言模型学习/RL强化学习基础/强化学习问题,流程\\|强化学习问题,流程]]\n    - [[大语言模型学习/RL强化学习基础/时序差分算法\\|时序差分算法]]\n    - [[大语言模型学习/RL强化学习基础/深度Q网络\\|深度Q网络]]\n    - [[大语言模型学习/RL强化学习基础/策略迭代算法\\|策略迭代算法]]\n    - [[大语言模型学习/RL强化学习基础/蒙特卡洛方法\\|蒙特卡洛方法]]\n    - [[大语言模型学习/RL强化学习基础/贝尔曼方程\\|贝尔曼方程]]\n    - [[大语言模型学习/RL强化学习基础/马尔可夫决策过程\\|马尔可夫决策过程]]\n  - [[Structure & Decoding Policy 结构和解码策略\\|Structure & Decoding Policy 结构和解码策略]]\n    - [[大语言模型学习/Structure & Decoding Policy 结构和解码策略/大模型结构与混合专家（LLM & MoE）解析\\|大模型结构与混合专家（LLM & MoE）解析]]\n    - [[大语言模型学习/Structure & Decoding Policy 结构和解码策略/深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略\\|深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略]]\n    - [[大语言模型学习/Structure & Decoding Policy 结构和解码策略/解码采样策略：Greedy Search与Beam Search的实现与优化\\|解码采样策略：Greedy Search与Beam Search的实现与优化]]\n  - [[分词\\|分词]]\n    - [[大语言模型学习/分词/BBPE：字节级别的BPE分词技术解析与应用\\|BBPE：字节级别的BPE分词技术解析与应用]]\n    - [[大语言模型学习/分词/WordPiece分词算法解析与实践\\|WordPiece分词算法解析与实践]]\n    - [[大语言模型学习/分词/使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践\\|使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践]]\n    - [[大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践\\|使用Unigram语言模型（ULM）优化分词算法：核心思路与实践]]\n    - [[大语言模型学习/分词/分词算法的比较\\|分词算法的比较]]\n    - [[大语言模型学习/分词/常用分词库\\|常用分词库]]\n  - [[后训练\\|后训练]]\n    - [[SFT监督微调\\|SFT监督微调]]\n      - [[SFT数据及处理\\|SFT数据及处理]]\n        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/开源数据集\\|开源数据集]]\n        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据多样性探索\\|数据多样性探索]]\n        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据生产合成与质量过滤\\|数据生产合成与质量过滤]]\n        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据飞轮在SFT中的应用与优化\\|数据飞轮在SFT中的应用与优化]]\n      - [[STF训练\\|STF训练]]\n        - [[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升\\|多轮对话专项提升]]\n        - [[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2\\|多轮对话专项提升2]]\n        - [[大语言模型学习/后训练/SFT监督微调/STF训练/训练启动脚本\\|训练启动脚本]]\n        - [[大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略\\|训练技巧和训练策略]]\n        - [[大语言模型学习/后训练/SFT监督微调/STF训练/训练框架及参数设置\\|训练框架及参数设置]]\n      - [[大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别\\|监督微调与预训练的区别]]\n  - [[词嵌入\\|词嵌入]]\n    - [[大语言模型学习/词嵌入/FastText\\|FastText]]\n    - [[大语言模型学习/词嵌入/oneHot\\|oneHot]]\n    - [[大语言模型学习/词嵌入/Word2Vec\\|Word2Vec]]\n    - [[大语言模型学习/词嵌入/介绍\\|词嵌入介绍]]","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"关于大语言模型学习导航","slug":"关于大语言模型学习导航","link":"#关于大语言模型学习导航","children":[]}]}}
