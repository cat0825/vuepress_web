---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/预训练定义以及数据来源","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/预训练定义以及数据来源/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-08T11:56:22.000+08:00","updated":"2025-04-13T13:06:02.000+08:00"}
---



**分类**：人工智能  
**标签**：预训练、大模型、自监督学习、数据处理、Next Token Prediction  
**日期**：2023-10-10  

---



## 核心观点总结
大模型的预训练旨在通过在大规模数据集上进行自监督学习，捕捉通用特征和模式，从而提升模型的适应性和泛化能力。预训练依赖于高质量、多领域的数据，使用Next Token Prediction（NTP）作为主要训练目标，帮助模型掌握语言和其他复杂技能。

---



## 重点内容解析

### 1. **预训练的定义与目标**
- **目标**：
  - 通过在大规模数据集上学习，捕捉通用特征和模式。
  - 减少对标注数据的依赖，加速适应新任务。
- **训练方法**：
  - 自监督学习（区别于无监督学习）。
  - 使用 **Next Token Prediction (NTP)** 作为训练目标，其公式为：
    $$
    L = - \sum_{n=1}^N \log p(x_n | x_1, x_2, ..., x_{n-1}; \theta)
    $$
  - 模型根据上下文预测下一个最可能的单词，并通过对数似然损失优化。

💡 **启发点**：NTP方法不仅适用于语言，还能扩展到代码、数学等领域的预测任务。

---


### 2. **预训练数据的来源与规模**
- **数据量级**：
  - 初始预训练：约 10T tokens。
  - 进一步微调：至少 100B tokens。
- **数据来源**：
  - **Common Crawl**：开放网页数据平台。
  - **GitHub**：代码相关数据。
  - **电子书与教育资料**：涵盖多领域知识。
  - **内部数据**：企业自有业务数据。
- **多语种支持**：
  - 通用模型需覆盖中英文，小语种根据需求选择性收集。

| 数据来源         | 特点                           |
|------------------|--------------------------------|
| Common Crawl     | 网页数据，覆盖广泛             |
| GitHub           | 专注于代码与技术               |
| 教育资料与论文   | 提供高质量知识内容             |
| 内部数据         | 企业特定领域的专属语料         |

📈 **趋势预测**：未来，高质量多模态数据（如图像、视频与文本结合）将成为预训练的重要方向。

---


### 3. **数据处理的挑战**
- **问题**：
  - 高质量数据（如论文、书籍）通常以PDF格式存在，解析复杂。
- **解决方案**：
  ✅ 使用专业PDF解析服务，避免依赖低效的Python库。  
  ✅ 训练OCR模型，前提是有足够高质量的PDF-文本对齐数据。  
  ⚠ 注意：直接用大模型（如GPT-4）解析PDF可能成本过高。  

💡 **启发点**：高效的数据处理工具是预训练成功的关键。

---



## 常见错误与注意事项
> **⚠ 警告**：  
> - 数据质量直接影响模型性能，低质量或偏向单一领域的数据可能导致模型在实际应用中表现不佳。  
> - 多语种处理需确保语料分布均衡，否则可能影响小语种任务的表现。

---



## [思考] 延伸问题
1. 如何优化多模态数据（如图像与文本）的联合预训练方法？  
2. 在多语种模型中，如何权衡不同语种的数据比例以提升性能？  
3. 对于特定领域的垂直应用，是否需要重新设计预训练目标？

---

> **来源**：本文内容基于大模型预训练技术文档整理与总结。

---



## 行动清单
1. ✅ 调研现有的开源数据集（如FineWeb、Pile、RedPajama），并尝试整合到自己的项目中。  
2. ✅ 学习如何使用专业PDF解析服务或OCR技术提升数据处理效率。  
3. ❗ 探索多模态预训练方法，关注图像、文本、音频等多种形式的数据整合。

---



## 后续追踪
- 跟踪最新发布的大规模开源模型及其预训练技术进展。  
- 深入研究自监督学习在非语言任务（如代码生成、逻辑推理）中的应用潜力。



## 数据资源概览
以下是一些常用的开源数据集及其获取地址：

| 数据集名称                     | 地址                                                                 |
|--------------------------------|----------------------------------------------------------------------|
| Skywork/SkyPile-150B           | [点击访问](https://huggingface.co/datasets/Skywork/SkyPile-150B)     |
| Wikipedia中文20230720          | [点击访问](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered) |
| C4                             | [点击访问](https://github.com/allenai/allennlp/discussions/5056)     |
| RedPajama                      | [点击访问](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2) |
| EleutherAI/the_pile_deduplicated | [点击访问](https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated) |
| WuDaoCorporaText               | [点击访问](https://data.baai.ac.cn/details/WuDaoCorporaText)         |
| PRM800K                        | [点击访问](https://github.com/openai/prm800k?tab=readme-ov-file)     |
| YeungNLP/firefly-pretrain-dataset | [点击访问](https://huggingface.co/datasets/YeungNLP/firefly-pretrain-dataset) |

💡 **启发点**：这些数据集涵盖了多语言、多领域的语料，适合用于构建通用型预训练模型。

---



## 数据采样与分布策略
在继续预训练中，通用数据的采样策略对模型性能影响显著。以下是一个具体分配案例：

### 数据分布比例
- **总Tokens量**：100B  
- **语言分布**：中文 : 英文 : 代码 = 20% : 70% : 10%


### 中文语料采样
| 数据集名称          | Tokens数量 (单位：B) | 备注                      |
|---------------------|---------------------|---------------------------|
| cc                 | 4.024               |                          |
| baidu_baike_v3     | 0.804               |                          |
| wiki_zw            | 0.1602              |                          |
| qikan              | 0.1602              |                          |
| recipe             | 0.0182              |                          |


### 英文语料采样
| 数据集名称          | Tokens数量 (单位：B) | 备注                      |
|---------------------|---------------------|---------------------------|
| c4                 | 32.675              |                          |
| arxiv_v2           | 3.2652              |                          |
| wiki_en            | 3.6792              |                          |


### Code语料采样
| 数据集名称          | Tokens数量 (单位：B) | 备注                      |
|---------------------|---------------------------|
| code               | 4.9042              |                          |
| github70v1         | 1.2168              |                          |

---

> 原始内容来源：[Skywork/SkyPile-150B](https://huggingface.co/datasets/Skywork/SkyPile-150B)、[Wikipedia中文20230720](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)、[C4](https://github.com/allenai/allennlp/discussions/5056) 等。
