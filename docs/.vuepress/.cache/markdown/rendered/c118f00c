{"content":"<h2 id=\"megatron-lm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm\"><span>Megatron-LM</span></a></h2>\n<p>Megatron-LM 是一个基于 PyTorch 的分布式训练框架，用来训练基于 Transformer 的大型语言模型。大型模型能够提供更精准和强大的语义理解与推理能力。随着计算资源的普及和数据集的增大，模型参数的数量呈指数级增长。然而，训练这样规模庞大的模型面临着一些挑战：</p>\n<h3 id=\"megatron-lm-的优缺点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm-的优缺点\"><span>Megatron-LM 的优缺点</span></a></h3>\n<p>Megatron-LM 是预训练必用框架，训练速度比 DeepSpeed 快，但也有诸多缺点：</p>\n<ul>\n<li>Megatron-LM 作为框架完全没有分层分模块，也没有太多抽象设计，导致模型跟框架无法解耦，需要手动切割模型并且适配，只适合 GPT/LLama 系列模型。</li>\n<li>通信有明显精度损失，特别是 ringallReduce。</li>\n<li>Megatron 推理时，kv cache 存在低级 bug，容易导致 RLHF 训练失败。</li>\n<li>混合精度训练时，bf16 没有 master weight。</li>\n<li>显存乱申请，加上 PyTorch allocator 拉垮，经常出现 OOM（Out of Memory）。</li>\n</ul>\n<p>虽然有这么多缺点，但是 Megatron-LM 仍然是千卡集群以上最佳的选择，没有之一。千卡以内可以选择 FSDP 或者 TorchTitan。</p>\n<h3 id=\"megatron-lm-的特点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm-的特点\"><span>Megatron-LM 的特点</span></a></h3>\n<p>Megatron-LM 支持数据并行、张量并行、流水线并行和专家并行，简称 4D 并行。</p>\n<h3 id=\"提醒\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#提醒\"><span>提醒</span></a></h3>\n<p>可以看看李沐的视频。</p>\n<h2 id=\"大型模型训练的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#大型模型训练的挑战\"><span>大型模型训练的挑战</span></a></h2>\n<p>训练大型模型面临的挑战主要包括显存限制、计算挑战以及并行策略挑战。</p>\n<h3 id=\"显存限制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存限制\"><span>显存限制</span></a></h3>\n<p>即便是目前最大的 GPU 主内存也难以容纳这些模型的参数。举例来说，一个 1750 亿参数的 GPT-3 模型需要约 700GB 的参数空间，对应的梯度约为 700GB，而优化器状态还需额外的 1400GB，总计需求高达 2.8TB。</p>\n<h3 id=\"计算挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算挑战\"><span>计算挑战</span></a></h3>\n<p>即使我们设法将模型适应单个 GPU（例如通过在主机内存和设备内存之间进行参数交换），模型所需的大量计算操作也会导致训练时间大幅延长。举个例子，使用一块 NVIDIA V100 GPU 来训练拥有 1750 亿参数的 GPT-3 模型，大约需要耗时 288 年。</p>\n<h3 id=\"并行策略挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#并行策略挑战\"><span>并行策略挑战</span></a></h3>\n<p>不同的并行策略对应不同的通信模式和通信量，这也是一个需要考虑的挑战。</p>\n<p>在现代的深度学习训练中，数据并行是一种常用的策略，通过将模型的训练任务分散到多个计算单元上来提高效率和加速训练过程。本文将详细介绍数据并行的概念、其优缺点，以及在分布式计算中常用的 All-Reduce 操作。</p>\n<h2 id=\"数据并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据并行\"><span>数据并行</span></a></h2>\n<p>数据并行（Data Parallelism, DP）是一种将模型训练任务分散到多个计算单元（如 GPU）的策略。假设有 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 张卡（worker），每张卡都保存一个完整模型的副本。在每一次迭代（iteration/step）中，输入数据集 batch 被分割成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 个等大小的 micro-batch，每张卡根据拿到的 micro-batch 数据独立计算梯度。随后，worker 定期聚合它们的梯度（调用 All-Reduce 计算梯度均值），每张卡再独立进行参数更新。</p>\n<p>对于无法放进单个 Worker 的大模型，可以在模型的较小片段上使用数据并行。这种方法在扩展性上通常表现出色，但存在两个限制：\n<img src=\"/img/user/附件/Pasted image 20250429221511.png\" alt=\"Pasted image 20250429221511.png\"><img src=\"/img/user/附件/Pasted image 20250429221518.png\" alt=\"Pasted image 20250429221518.png\"></p>\n<h3 id=\"限制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#限制\"><span>限制</span></a></h3>\n<ol>\n<li>\n<p><strong>降低 GPU 利用率</strong>：在一定点之后，每个 GPU 的批量大小变得太小，这会降低 GPU 的利用率，并增加通信成本。</p>\n</li>\n<li>\n<p><strong>设备数限制</strong>：可用于训练的 GPU 设备数量受到批量大小的限制，这限制了可以使用的最大设备数。</p>\n</li>\n</ol>\n<h2 id=\"all-reduce-操作\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#all-reduce-操作\"><span>All-Reduce 操作</span></a></h2>\n<p>All-Reduce 是一种在并行计算和分布式计算中常用的通信操作，用于在多个计算节点（例如 GPU 或 CPU）之间汇总数据。</p>\n<h3 id=\"工作原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#工作原理\"><span>工作原理</span></a></h3>\n<ol>\n<li>\n<p><strong>数据分发</strong>：每个节点在完成了自己的计算后会生成一组梯度（或者其他需要汇总的数据），这些数据需要在所有节点之间进行汇总。</p>\n</li>\n<li>\n<p><strong>数据汇总</strong>：All-Reduce 操作将每个节点上的数据在所有节点之间进行相加或其他类型的归约操作（例如，求和、求平均、求最大值等）。这意味着最终每个节点都会得到相同的结果，该结果是所有节点数据的汇总。</p>\n</li>\n<li>\n<p><strong>结果广播</strong>：完成汇总操作后，汇总结果会被广播回每个节点，使得每个节点都持有相同的汇总数据。这一步确保了所有节点在下一步的计算中都能使用一致的数据。</p>\n</li>\n</ol>\n<h2 id=\"模型并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型并行\"><span>模型并行</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250429221654.png\" alt=\"Pasted image 20250429221654.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250429221701.png\" alt=\"Pasted image 20250429221701.png\"></p>\n<h2 id=\"model-parallelism-mp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#model-parallelism-mp\"><span>Model Parallelism MP</span></a></h2>\n<p>对于数据并行的限制，可以采用一些内存管理技术，比如激活检查点（Activation Checkpointing）。</p>\n<h3 id=\"activation-checkpointing-gradient-checkpointing\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#activation-checkpointing-gradient-checkpointing\"><span>Activation Checkpointing（gradient_checkpointing）</span></a></h3>\n<p>激活检查点是一种在前向传播过程中计算节点的激活值并保存的方法。计算下一个节点完成后丢弃中间节点的激活值，反向传播时如果有保存下来的梯度就直接使用，如果没有就使用保存下来的前一个节点的梯度重新计算当前节点的梯度再使用。</p>\n<p>此外，还可以使用模型并行来划分模型的不同阶段，从而解决GPU内存容量和计算限制的问题，使得权重和关联的优化器状态不需要同时存储在一个GPU上。一个模型的内存和计算被分布在多个计算节点上，主要的方式分为流水线并行和张量并行。</p>\n<h2 id=\"张量并行-intra-layer-tensor-parallelism\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行-intra-layer-tensor-parallelism\"><span>张量并行 Intra-Layer Tensor Parallelism</span></a></h2>\n<p>有的tensor/layer很大，一张卡放不下，将tensor分割成多块，一张卡存一块。这也可以被理解为将大矩阵运算拆分成多个小矩阵运算，然后分布到不同的设备上进行计算。\n<img src=\"/img/user/附件/Pasted image 20250429221727.png\" alt=\"Pasted image 20250429221727.png\"></p>\n<h3 id=\"代码块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码块\"><span>代码块</span></a></h3>\n<p>高度的模型并行会产生很多的小矩阵乘法(GEMMS)。对于一个GEMMs <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">Y = X A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span></span></span></span>，按照对权重矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span>的分块方式，张量并行又可以分成行并行和列并行。神经网络中的线性层（Linear层），可以将其看作是将输入矩阵分块进行计算，然后将计算结果合并成输出矩阵。这个过程涉及矩阵乘法和加法操作，其中矩阵乘法涉及到权重矩阵和输入数据之间的乘法，然后再加上偏置向量；对于非线性层（例如激活函数层），通常不需要进行额外的设计。这些层的计算过程是基于输入数据应用某种非线性函数，例如ReLU（修正线性单元）、Sigmoid、Tanh等。这些函数在数学上是已知的，只需要将输入数据传递给这些函数，然后得到输出。整体来看，神经网络的计算可以被抽象为一系列的矩阵和向量操作，其中线性层涉及矩阵乘法和加法，而非线性层涉及特定的函数计算。这些操作在深度学习框架中会被高度优化，以提高计算效率和训练速度。</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 假设模型有三层：L0, L1, L2 每层有两个神经元 两张卡</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">----</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> b0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> c0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">----</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> b1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> c1</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"gemms行并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms行并行\"><span>GEMMs行并行</span></a></h3>\n<p>先分析Row Parallelism，就是把权重矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span>按照行切分成两部分，对应的把<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>按照列切分成两部分：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>X</mi><mi>A</mi><mo>=</mo><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>X</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>X</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.16em\" columnalign=\"center center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>A</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>A</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><mo>=</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo>=</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>=</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X A = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\begin{bmatrix} A_1 &amp; A_2 \\end{bmatrix} = X_1 A_1 + X_2 A_2 = Y_1 + Y_2 = Y\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.41em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.95em;\"><span></span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">]</span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85em;\"><span style=\"top:-3.01em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85em;\"><span style=\"top:-3.01em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">]</span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span></p>\n<p>这样<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">X_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>A</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">A_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>可以在第一个GPU上计算，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">X_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>A</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">A_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>可以在第二个GPU上进行计算。</p>\n<p>通过这种方式，可以有效地将大型模型的计算任务分配到多个GPU上，从而提高计算效率，同时解决单个GPU内存不足的问题。这种技术在处理大型深度学习模型时尤为重要，因为它能够显著提高训练速度，并允许更大规模的模型在有限硬件资源下进行训练。\n<img src=\"/img/user/附件/Pasted image 20250429221755.png\" alt=\"Pasted image 20250429221755.png\"><img src=\"/img/user/附件/Pasted image 20250429221811.png\" alt=\"Pasted image 20250429221811.png\"></p>\n<h2 id=\"gemms列并行与transformer中的张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms列并行与transformer中的张量并行\"><span>GEMMs列并行与Transformer中的张量并行</span></a></h2>\n<p>在现代深度学习模型中，尤其是Transformer架构中，矩阵计算的并行化是提升模型训练和推理速度的关键。本文将探讨GEMMs列并行和Transformer中的张量并行技术。</p>\n<h2 id=\"gemms列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms列并行\"><span>GEMMs列并行</span></a></h2>\n<p>GEMMs列并行是一种将矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 按照列来分割的技术。这种方法可以有效地将计算任务分配到多个GPU上，从而实现并行计算。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250429221945.png\" alt=\"Pasted image 20250429221945.png\"></p>\n<h2 id=\"transformer中的张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformer中的张量并行\"><span>Transformer中的张量并行</span></a></h2>\n<p>Transformer架构包含了Self-Attention和Feed Forward Network，这些操作涉及大量的矩阵计算，非常适合在GPU上进行并行操作，以加速模型的训练和推理过程。\n<img src=\"/img/user/附件/Pasted image 20250429221953.png\" alt=\"Pasted image 20250429221953.png\"></p>\n<h3 id=\"masked-multi-head-self-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#masked-multi-head-self-attention\"><span>Masked Multi-Head Self Attention</span></a></h3>\n<p>Masked Multi-Head Self Attention涉及到大量的矩阵乘法操作，这些操作可以被高效地并行执行，从而提高计算速度。</p>\n<h3 id=\"feed-forward-neural-network\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#feed-forward-neural-network\"><span>Feed Forward Neural Network</span></a></h3>\n<p>Feed Forward Neural Network包含多个全连接层，每个全连接层都涉及矩阵乘法、激活函数（通常是GeLU）和可能的Dropout层。这些操作也是高度并行化的，可以在GPU上迅速执行。</p>\n<p>Megatron的FFN是一个两层MLP，第一层是维度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 变成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">4H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span>，第二层是维度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">4H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 变回到 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span>。具体架构如下，紫色块对应于全连接层，每个蓝色块表示一个被复制 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 次的transformer层，红色的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">x L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">xL</span></span></span></span> 代表此蓝色复制 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span> 次。</p>\n<p>张量并行就是要对Transformer进行切分，Megatron把Masked Multi-Head Self Attention和Feed Forward都进行切分以并行化，通过添加一些同步通信操作来创建一个简单的模型并行实现。</p>\n<h2 id=\"切分mlp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#切分mlp\"><span>切分MLP</span></a></h2>\n<p>从MLP开始，MLP的第一部分是GEMM，后面是激活函数GeLU：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Y = \\text{GeLU} ( X A )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<h3 id=\"对比按照行列切分权重的方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#对比按照行列切分权重的方法\"><span>对比按照行列切分权重的方法</span></a></h3>\n<h4 id=\"行并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行并行\"><span>行并行</span></a></h4>\n<p>行并行的方法如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Y = \\text{GeLU} ( X_1 A_1 + X_2 A_2 )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>但是，由于GeLU是非线性计算：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">≠</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{GeLU} ( X_1 A_1 + X_2 A_2 ) \\neq \\text{GeLU} ( X_1 A_1 ) + \\text{GeLU} ( X_2 A_2 )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mrel\"></span></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>所以这种方案需要在GeLU函数之前加上一个同步点，这个同步点让不同GPU之间交换信息。</p>\n<h4 id=\"列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#列并行\"><span>列并行</span></a></h4>\n<p>列并行的方法如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo stretchy=\"false\">]</mo><mo>=</mo><mo stretchy=\"false\">[</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mi>A</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[ Y_1, Y_2 ] = [ \\text{GeLU} ( X A_1 ), \\text{GeLU} ( X A_2 ) ]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>这种方法不需要同步点，可以直接把两个GeLU的输出拼接起来，而不需要额外的通信（比如GeLU这里就不用all-reduce了）。在前向和后向传递的时候做一次all-reduce通信操作。对于第二个MLP使用行并行，可以直接与第一个MLP得到的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 对应计算，不需要额外通信操作。</p>\n<p>总的来说，在MLP层中，对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用“列切割”，对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span> 采用“行切割”。\n<img src=\"/img/user/附件/Pasted image 20250429222007.png\" alt=\"Pasted image 20250429222007.png\"></p>\n<p>在现代深度学习中，模型的规模和复杂度不断增加，如何高效地利用GPU资源成为了一个重要的研究课题。本文将探讨在神经网络计算中，如何通过合理的切分策略来减少GPU之间的通信量，以提高整体计算效率。</p>\n<h2 id=\"gelu计算中的行列切割策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gelu计算中的行列切割策略\"><span>GELU计算中的行列切割策略</span></a></h2>\n<p>在处理矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 时，我们要尽量保证各GPU上的计算相互独立，从而减少通信量。对于 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 来说，需要进行一次GELU的计算，而GELU是非线性的。这意味着，如果对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用行切割，我们必须在进行GELU前，进行一次AllReduce，这会产生额外的通信量。但如果对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用列切割，每块GPU就可以继续独立计算。一旦确认好 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 进行列切割，那么相应地，矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span> 就需要进行行切割。</p>\n<h2 id=\"通信量分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信量分析\"><span>通信量分析<img src=\"/img/user/附件/Pasted image 20250429222138.png\" alt=\"Pasted image 20250429222138.png\"></span></a></h2>\n<p>在MLP层中，forward和backward阶段各会产生一次AllReduce。AllReduce的过程分为两个阶段：Reduce-Scatter和All-Gather，每个阶段的通信量都相等。设每个阶段的通信量为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span></span></span></span>，则一次AllReduce产生的总通信量为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">4\\Phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4Φ</span></span></span></span>。MLP层的总通信量为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>4</mn><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">4\\Phi\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4Φ</span></span></span></span></span></p>\n<p>其中：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo>=</mo><mi>b</mi><mo>∗</mo><mi>s</mi><mo>∗</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi = b \\ast s \\ast h\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4653em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">h</span></span></span></span></span></p>\n<h2 id=\"self-attention的切分策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#self-attention的切分策略\"><span>Self-Attention的切分策略</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250429222151.png\" alt=\"Pasted image 20250429222151.png\"></p>\n<h3 id=\"多头注意力并行切分\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多头注意力并行切分\"><span>多头注意力并行切分</span></a></h3>\n<p>利用多头注意力固有的并行特性，可以直接以列并行的方法切分QKV相关的GEMMs。首先考虑head=1的情况，self-attention的计算方式如下图所示（图略）。</p>\n<h3 id=\"当num-heads-2时的情况\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#当num-heads-2时的情况\"><span>当num_heads = 2时的情况<img src=\"/img/user/附件/Pasted image 20250429222201.png\" alt=\"Pasted image 20250429222201.png\"></span></a></h3>\n<p>对于每一块权重，我们沿着列方向维度切割一刀。此时每个 head 上的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">Q</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的维度都变成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>k</mi><mrow><mi>d</mi><mi>i</mi><mi>m</mi></mrow></msub><mi mathvariant=\"normal\">/</mi><mi mathvariant=\"normal\">/</mi><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(d_{model}, k_{dim}//2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">im</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">//2</span><span class=\"mclose\">)</span></span></span></span>。每个 head 上单独做矩阵计算，最后将计算结果concat起来。整个流程如下（图略）。</p>\n<p>因此，我们可以把每个头的参数放到一块GPU上，也可以把多个head放到一块GPU上，但尽量保证head总数能被GPU个数整除。</p>\n<h3 id=\"通信量分析-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信量分析-1\"><span>通信量分析</span></a></h3>\n<p>使用 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 来表示GPU的数量。有几块GPU，就把 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 按行维度切成几份。self-attention 层在 forward 和 backward 中各做一次 AllReduce，总通信量也是：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span></span></span></span></span></p>\n<p>其中：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo>=</mo><mi>b</mi><mo>∗</mo><mi>s</mi><mo>∗</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi = b \\ast s \\ast h\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4653em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">h</span></span></span></span></span></p>\n<h2 id=\"embedding切分策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#embedding切分策略\"><span>Embedding切分策略</span></a></h2>\n<h3 id=\"输入层embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#输入层embedding\"><span>输入层embedding</span></a></h3>\n<p>输入层embedding包括word embedding和positional embedding两部分。对于positional embedding，由于 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\max_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mop\"><span class=\"mop\">max</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 本身不会太长，因此每个GPU上都拷贝一份，对显存的压力不大。但对于word embedding，词表的大小较大，需要将word embedding拆分到各个GPU上。</p>\n<h3 id=\"输出层embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#输出层embedding\"><span>输出层embedding</span></a></h3>\n<p>输出层只有一个word embedding。必须时刻保证输入层和输出层共用一套word embedding。在backward过程中，我们在输出层对word embedding计算一次梯度，在输入层中也会计算一次梯度。在用梯度更新word embedding权重时，必须用两次梯度的总和进行更新。</p>\n<h2 id=\"cross-entropy切分的基本流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#cross-entropy切分的基本流程\"><span>Cross-entropy切分的基本流程<img src=\"/img/user/附件/Pasted image 20250429222312.png\" alt=\"Pasted image 20250429222312.png\"></span></a></h2>\n<p>首先，需要对输出层的<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>和<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>做一次All-Gather操作，把它们concat起来形成<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span>。对<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span>的每一行做softmax运算，就可以得到对于当前位置来说，每个词出现的概率。接着，再用此概率和真值组做cross-entropy即可。</p>\n<p>然而，All-Gather操作会产生额外的通讯量：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s \\times v\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span></p>\n<p>当词表<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span>很大时，这个通讯开销也不容忽视。</p>\n<h2 id=\"优化策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优化策略\"><span>优化策略</span></a></h2>\n<p>针对上述情况，可以采取以下优化策略：</p>\n<h3 id=\"gpu上的局部计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpu上的局部计算\"><span>GPU上的局部计算</span></a></h3>\n<p>每块GPU上，我们可以先按行求和，得到各自GPU上的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mi>P</mi><mi>U</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">GPU\\_sum(e)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">GP</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">m</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span>。</p>\n<h3 id=\"allreduce操作\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#allreduce操作\"><span>AllReduce操作</span></a></h3>\n<p>将每块GPU上的结果做AllReduce操作，得到每行最终的</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo>∑</mo><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sum(e)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.6em;vertical-align:-0.55em;\"></span><span class=\"mop op-symbol large-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>这就是softmax中的分母。此时的通讯量为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span></span></p>\n<h3 id=\"计算局部cross-entropy\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算局部cross-entropy\"><span>计算局部Cross-entropy</span></a></h3>\n<p>在每块GPU上，即可计算各自维护部分的</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mi>e</mi><mrow><mo>∑</mo><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{e}{\\sum(e)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.0436em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1076em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>将其与真值做cross-entropy，得到每行的loss，按行加总起来以后得到GPU上的 scalar Loss。</p>\n<h3 id=\"汇总总loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#汇总总loss\"><span>汇总总Loss</span></a></h3>\n<p>将GPU上的 scalar Loss 做AllReduce操作，得到总Loss。此时通讯量为<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span>。</p>\n<p>通过上述优化，我们把原先的通讯量从</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s \\times v\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span></p>\n<p>大大降至</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>+</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s + N\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></p>\n<h2 id=\"transformerblock中的通信优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformerblock中的通信优化\"><span>TransformerBlock中的通信优化<img src=\"/img/user/附件/Pasted image 20250429222329.png\" alt=\"Pasted image 20250429222329.png\"></span></a></h2>\n<p>一个TransformerBlock需要在前向和后向总共需要4个all-reduce通信操作。</p>\n<h2 id=\"张量并行与序列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行与序列并行\"><span>张量并行与序列并行<img src=\"/img/user/附件/Pasted image 20250429222343.png\" alt=\"Pasted image 20250429222343.png\"></span></a></h2>\n<p>张量并行对AttentionBlock和MLP都进行了并行化，但没有对LN和dropout进行并行化。不过后续在megatron3中用序列并行技术对这两部分进行了并行，这部分就不过多展开了。</p>\n<hr>\n<h1 id=\"流水线并行-inter-layer-pipeline-parallelism\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流水线并行-inter-layer-pipeline-parallelism\"><span>流水线并行 Inter-Layer Pipeline Parallelism</span></a></h1>\n<p>在深度学习模型的训练过程中，如何提高效率一直是研究的重点之一。流水线并行是一种将网络按层切分，划分成多组，并将每组放置在不同的设备上进行计算的方法。通过这种方式，可以在不同设备上并行执行不同的模型阶段，从而提高整体效率。</p>\n<h2 id=\"代码块-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码块-1\"><span>代码块</span></a></h2>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 假设模型有8层 两张卡 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">======================</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> ====================</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L3 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> |</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L4 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L5 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L6 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L7 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">======================</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> ====================</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU0</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> GPU1</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 设想一下，当GPU0在进行（前向/后向）计算时，GPU1在干嘛？闲着 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 当GPU1在进行（前向/后向)计算时，GPU0在干嘛？闲着 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 为了防止”一卡工作，众卡围观“，实践中PP也会把batch数据分割成 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 多个micro-batch，流水线执行</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>目前主流的流水线并行方法包括Gpipe和PipeDream。与这两者相比，Megatron中的流水线并行实现略有不同，采用了Virtual Pipeline的方法。传统的流水线并行通常会在一个设备上放置几个模块，通过在计算和通信之间取得平衡来提高效率。然而，虚拟流水线则采取相反的策略。在设备数量不变的前提下，它将流水线阶段进一步细分，以承载更多的通信量，从而降低空闲时间的比率，以缩短每个步骤的执行时间。</p>\n<h3 id=\"gpipe\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpipe\"><span>Gpipe<img src=\"/img/user/附件/Pasted image 20250429222504.png\" alt=\"Pasted image 20250429222504.png\"></span></a></h3>\n<p>Gpipe方法将transformer层按层切分放到不同的设备上，并将一个batch切分成多个mini-batch。在前向传播时，每个mini-batch从第一个设备流向最后一个设备；在反向传播时，在最后一个设备上计算出梯度并更新对应层的参数，然后将梯度传递给前一个设备进行梯度计算和参数更新。这种方法的劣势之一是空泡率（流水线空闲时间）比较高。</p>\n<h3 id=\"pipedream\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#pipedream\"><span>PipeDream<img src=\"/img/user/附件/Pasted image 20250429222515.png\" alt=\"Pasted image 20250429222515.png\"></span></a></h3>\n<p>PipeDream相对于Gpipe的改进主要体现在内存方面。虽然空泡时间和Gpipe一致，但通过合理安排前向和反向过程的顺序，在步骤中间的稳定阶段形成1前向1反向（1F1B）的模式。在这个阶段，每个设备上最少只需要保存一份micro-batch的激活值，最多需要保存流水线阶段数份激活值，从而可以有效降低空泡占比。</p>\n<h3 id=\"virtual-pipeline\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#virtual-pipeline\"><span>Virtual Pipeline<img src=\"/img/user/附件/Pasted image 20250429222523.png\" alt=\"Pasted image 20250429222523.png\"></span></a></h3>\n<p>传统的pipeline并行通常会在一个设备上放置几个block，这是为了扩展效率考虑，在计算强度和通信强度中间取一个平衡。但Virtual Pipeline却反其道而行之，在设备数量不变的情况下，分出更多的pipeline阶段，以更多的通信量换取空泡比率降低，减小了每一步的执行时间。假设网络共有16层（编号0-15），4个设备，前述Gpipe和PipeDream是分成4个阶段，按编号0-3层放Device1，4-7层放Device2，以此类推。Virtual Pipeline则是减小切分粒度，以virtual_pipeline_stage=2为例，将0-1层放Device1, 2-3层放在Device2，...，6-7层放到Device4，8-9层继续放在Device1，10-11层放在Device2，...，14-15层放在Device4。在稳定的时候也是1F1B的形式。按照这种方式，设备之间的点对点通信次数（量）直接翻了virtual_pipeline_stage倍，但空泡比率降低了。\n<img src=\"/img/user/附件/Pasted image 20250429222538.png\" alt=\"Pasted image 20250429222538.png\"></p>\n<h2 id=\"_3d并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3d并行\"><span>3D并行</span></a></h2>\n<h2 id=\"数据并行-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据并行-1\"><span>数据并行</span></a></h2>\n<h3 id=\"计算效率高-实现简单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算效率高-实现简单\"><span>计算效率高， 实现简单</span></a></h3>\n<p>数据并行是一种计算效率极高且实现简单的并行技术。在这种模式下，每个计算节点都保存完整的模型、梯度和优化器状态，因此显存效率不高。然而，当增加并行度时，单卡的计算量保持恒定，可以实现近乎完美的线性扩展。不过，规约梯度的通信开销与模型大小成正相关。</p>\n<h2 id=\"张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行\"><span>张量并行</span></a></h2>\n<h3 id=\"因模型结构而异-实现难度大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#因模型结构而异-实现难度大\"><span>因模型结构而异， 实现难度大</span></a></h3>\n<p>张量并行的实现难度较大，因为它依赖于模型的具体结构。随着并行度增加，显存占用成比例地减少，这是减少单层神经网络中间激活的唯一方法。然而，频繁的通信限制了两个通信阶段之间的计算量，导致计算效率很低。</p>\n<h2 id=\"流水线并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流水线并行\"><span>流水线并行</span></a></h2>\n<h3 id=\"通信成本最低\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信成本最低\"><span>通信成本最低</span></a></h3>\n<p>流水线并行具有最低的通信成本。虽然减少的显存与流水线并行度成正相关，但流水线并行不会减少每层中间激活的显存占用。其计算效率得益于成本更低的点对点（P2P）通信，通信量与流水线各个阶段边界的激活值大小成正相关。</p>\n<h2 id=\"显存和通信效率比较\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存和通信效率比较\"><span>显存和通信效率比较</span></a></h2>\n<ul>\n<li>显存效率：模型并行 &gt; 流水线并行 &gt; 数据并行</li>\n<li>通信效率：流水线并行 &gt; 数据并行 &gt; 模型并行</li>\n</ul>\n<h2 id=\"_3d并行技术的混合应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3d并行技术的混合应用\"><span>3D并行技术的混合应用</span></a></h2>\n<p>3D并行技术是混合数据并行（DP）、张量并行（TP）和流水线并行（PP）组成的。在实际应用中，可以通过四路张量并行、四路流水线并行、两路数据并行以及32个worker来实现高效的3D并行。\n<img src=\"/img/user/附件/Pasted image 20250429222809.png\" alt=\"Pasted image 20250429222809.png\"><img src=\"/img/user/附件/Pasted image 20250429222815.png\" alt=\"Pasted image 20250429222815.png\"><img src=\"/img/user/附件/Pasted image 20250429222822.png\" alt=\"Pasted image 20250429222822.png\"></p>\n<h2 id=\"_4d并行技术在llama3中的应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4d并行技术在llama3中的应用\"><span>4D并行技术在LLaMA3中的应用</span></a></h2>\n<p>在LLaMA3的技术报告中，他们还使用了4D并行技术。除了PP、TP和DP，还加上了CP（context parallel），进一步提高了模型训练的效率。\n<img src=\"/img/user/附件/Pasted image 20250429222830.png\" alt=\"Pasted image 20250429222830.png\">\n<img src=\"/img/user/附件/Pasted image 20250429222839.png\" alt=\"Pasted image 20250429222839.png\"></p>\n<hr>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/训练推理优化/训练框架/Megatron-LM.md","filePathRelative":"notes_bak/大语言模型学习/训练推理优化/训练框架/Megatron-LM.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/训练推理优化/训练框架/Megatron-LM","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/训练推理优化/训练框架/Megatron-LM/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-29T14:12:46.000Z","updated":"2025-04-30T10:18:07.000Z","title":"Megatron-LM","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"megatron-lm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm\"><span>Megatron-LM</span></a></h2>\n<p>Megatron-LM 是一个基于 PyTorch 的分布式训练框架，用来训练基于 Transformer 的大型语言模型。大型模型能够提供更精准和强大的语义理解与推理能力。随着计算资源的普及和数据集的增大，模型参数的数量呈指数级增长。然而，训练这样规模庞大的模型面临着一些挑战：</p>\n<h3 id=\"megatron-lm-的优缺点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm-的优缺点\"><span>Megatron-LM 的优缺点</span></a></h3>\n<p>Megatron-LM 是预训练必用框架，训练速度比 DeepSpeed 快，但也有诸多缺点：</p>\n<ul>\n<li>Megatron-LM 作为框架完全没有分层分模块，也没有太多抽象设计，导致模型跟框架无法解耦，需要手动切割模型并且适配，只适合 GPT/LLama 系列模型。</li>\n<li>通信有明显精度损失，特别是 ringallReduce。</li>\n<li>Megatron 推理时，kv cache 存在低级 bug，容易导致 RLHF 训练失败。</li>\n<li>混合精度训练时，bf16 没有 master weight。</li>\n<li>显存乱申请，加上 PyTorch allocator 拉垮，经常出现 OOM（Out of Memory）。</li>\n</ul>\n<p>虽然有这么多缺点，但是 Megatron-LM 仍然是千卡集群以上最佳的选择，没有之一。千卡以内可以选择 FSDP 或者 TorchTitan。</p>\n<h3 id=\"megatron-lm-的特点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm-的特点\"><span>Megatron-LM 的特点</span></a></h3>\n<p>Megatron-LM 支持数据并行、张量并行、流水线并行和专家并行，简称 4D 并行。</p>\n<h3 id=\"提醒\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#提醒\"><span>提醒</span></a></h3>\n<p>可以看看李沐的视频。</p>\n<h2 id=\"大型模型训练的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#大型模型训练的挑战\"><span>大型模型训练的挑战</span></a></h2>\n<p>训练大型模型面临的挑战主要包括显存限制、计算挑战以及并行策略挑战。</p>\n<h3 id=\"显存限制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存限制\"><span>显存限制</span></a></h3>\n<p>即便是目前最大的 GPU 主内存也难以容纳这些模型的参数。举例来说，一个 1750 亿参数的 GPT-3 模型需要约 700GB 的参数空间，对应的梯度约为 700GB，而优化器状态还需额外的 1400GB，总计需求高达 2.8TB。</p>\n<h3 id=\"计算挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算挑战\"><span>计算挑战</span></a></h3>\n<p>即使我们设法将模型适应单个 GPU（例如通过在主机内存和设备内存之间进行参数交换），模型所需的大量计算操作也会导致训练时间大幅延长。举个例子，使用一块 NVIDIA V100 GPU 来训练拥有 1750 亿参数的 GPT-3 模型，大约需要耗时 288 年。</p>\n<h3 id=\"并行策略挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#并行策略挑战\"><span>并行策略挑战</span></a></h3>\n<p>不同的并行策略对应不同的通信模式和通信量，这也是一个需要考虑的挑战。</p>\n<p>在现代的深度学习训练中，数据并行是一种常用的策略，通过将模型的训练任务分散到多个计算单元上来提高效率和加速训练过程。本文将详细介绍数据并行的概念、其优缺点，以及在分布式计算中常用的 All-Reduce 操作。</p>\n<h2 id=\"数据并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据并行\"><span>数据并行</span></a></h2>\n<p>数据并行（Data Parallelism, DP）是一种将模型训练任务分散到多个计算单元（如 GPU）的策略。假设有 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 张卡（worker），每张卡都保存一个完整模型的副本。在每一次迭代（iteration/step）中，输入数据集 batch 被分割成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 个等大小的 micro-batch，每张卡根据拿到的 micro-batch 数据独立计算梯度。随后，worker 定期聚合它们的梯度（调用 All-Reduce 计算梯度均值），每张卡再独立进行参数更新。</p>\n<p>对于无法放进单个 Worker 的大模型，可以在模型的较小片段上使用数据并行。这种方法在扩展性上通常表现出色，但存在两个限制：\n<img src=\"/img/user/附件/Pasted image 20250429221511.png\" alt=\"Pasted image 20250429221511.png\"><img src=\"/img/user/附件/Pasted image 20250429221518.png\" alt=\"Pasted image 20250429221518.png\"></p>\n<h3 id=\"限制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#限制\"><span>限制</span></a></h3>\n<ol>\n<li>\n<p><strong>降低 GPU 利用率</strong>：在一定点之后，每个 GPU 的批量大小变得太小，这会降低 GPU 的利用率，并增加通信成本。</p>\n</li>\n<li>\n<p><strong>设备数限制</strong>：可用于训练的 GPU 设备数量受到批量大小的限制，这限制了可以使用的最大设备数。</p>\n</li>\n</ol>\n<h2 id=\"all-reduce-操作\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#all-reduce-操作\"><span>All-Reduce 操作</span></a></h2>\n<p>All-Reduce 是一种在并行计算和分布式计算中常用的通信操作，用于在多个计算节点（例如 GPU 或 CPU）之间汇总数据。</p>\n<h3 id=\"工作原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#工作原理\"><span>工作原理</span></a></h3>\n<ol>\n<li>\n<p><strong>数据分发</strong>：每个节点在完成了自己的计算后会生成一组梯度（或者其他需要汇总的数据），这些数据需要在所有节点之间进行汇总。</p>\n</li>\n<li>\n<p><strong>数据汇总</strong>：All-Reduce 操作将每个节点上的数据在所有节点之间进行相加或其他类型的归约操作（例如，求和、求平均、求最大值等）。这意味着最终每个节点都会得到相同的结果，该结果是所有节点数据的汇总。</p>\n</li>\n<li>\n<p><strong>结果广播</strong>：完成汇总操作后，汇总结果会被广播回每个节点，使得每个节点都持有相同的汇总数据。这一步确保了所有节点在下一步的计算中都能使用一致的数据。</p>\n</li>\n</ol>\n<h2 id=\"模型并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型并行\"><span>模型并行</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250429221654.png\" alt=\"Pasted image 20250429221654.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250429221701.png\" alt=\"Pasted image 20250429221701.png\"></p>\n<h2 id=\"model-parallelism-mp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#model-parallelism-mp\"><span>Model Parallelism MP</span></a></h2>\n<p>对于数据并行的限制，可以采用一些内存管理技术，比如激活检查点（Activation Checkpointing）。</p>\n<h3 id=\"activation-checkpointing-gradient-checkpointing\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#activation-checkpointing-gradient-checkpointing\"><span>Activation Checkpointing（gradient_checkpointing）</span></a></h3>\n<p>激活检查点是一种在前向传播过程中计算节点的激活值并保存的方法。计算下一个节点完成后丢弃中间节点的激活值，反向传播时如果有保存下来的梯度就直接使用，如果没有就使用保存下来的前一个节点的梯度重新计算当前节点的梯度再使用。</p>\n<p>此外，还可以使用模型并行来划分模型的不同阶段，从而解决GPU内存容量和计算限制的问题，使得权重和关联的优化器状态不需要同时存储在一个GPU上。一个模型的内存和计算被分布在多个计算节点上，主要的方式分为流水线并行和张量并行。</p>\n<h2 id=\"张量并行-intra-layer-tensor-parallelism\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行-intra-layer-tensor-parallelism\"><span>张量并行 Intra-Layer Tensor Parallelism</span></a></h2>\n<p>有的tensor/layer很大，一张卡放不下，将tensor分割成多块，一张卡存一块。这也可以被理解为将大矩阵运算拆分成多个小矩阵运算，然后分布到不同的设备上进行计算。\n<img src=\"/img/user/附件/Pasted image 20250429221727.png\" alt=\"Pasted image 20250429221727.png\"></p>\n<h3 id=\"代码块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码块\"><span>代码块</span></a></h3>\n<p>高度的模型并行会产生很多的小矩阵乘法(GEMMS)。对于一个GEMMs <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">Y = X A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span></span></span></span>，按照对权重矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span>的分块方式，张量并行又可以分成行并行和列并行。神经网络中的线性层（Linear层），可以将其看作是将输入矩阵分块进行计算，然后将计算结果合并成输出矩阵。这个过程涉及矩阵乘法和加法操作，其中矩阵乘法涉及到权重矩阵和输入数据之间的乘法，然后再加上偏置向量；对于非线性层（例如激活函数层），通常不需要进行额外的设计。这些层的计算过程是基于输入数据应用某种非线性函数，例如ReLU（修正线性单元）、Sigmoid、Tanh等。这些函数在数学上是已知的，只需要将输入数据传递给这些函数，然后得到输出。整体来看，神经网络的计算可以被抽象为一系列的矩阵和向量操作，其中线性层涉及矩阵乘法和加法，而非线性层涉及特定的函数计算。这些操作在深度学习框架中会被高度优化，以提高计算效率和训练速度。</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 假设模型有三层：L0, L1, L2 每层有两个神经元 两张卡</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">----</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> b0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> c0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">----</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> b1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> c1</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"gemms行并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms行并行\"><span>GEMMs行并行</span></a></h3>\n<p>先分析Row Parallelism，就是把权重矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span>按照行切分成两部分，对应的把<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>按照列切分成两部分：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>X</mi><mi>A</mi><mo>=</mo><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>X</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>X</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.16em\" columnalign=\"center center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>A</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>A</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><mo>=</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo>=</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>=</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X A = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\begin{bmatrix} A_1 &amp; A_2 \\end{bmatrix} = X_1 A_1 + X_2 A_2 = Y_1 + Y_2 = Y\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.41em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.95em;\"><span></span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">]</span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85em;\"><span style=\"top:-3.01em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85em;\"><span style=\"top:-3.01em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">]</span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span></p>\n<p>这样<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">X_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>A</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">A_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>可以在第一个GPU上计算，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">X_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>A</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">A_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>可以在第二个GPU上进行计算。</p>\n<p>通过这种方式，可以有效地将大型模型的计算任务分配到多个GPU上，从而提高计算效率，同时解决单个GPU内存不足的问题。这种技术在处理大型深度学习模型时尤为重要，因为它能够显著提高训练速度，并允许更大规模的模型在有限硬件资源下进行训练。\n<img src=\"/img/user/附件/Pasted image 20250429221755.png\" alt=\"Pasted image 20250429221755.png\"><img src=\"/img/user/附件/Pasted image 20250429221811.png\" alt=\"Pasted image 20250429221811.png\"></p>\n<h2 id=\"gemms列并行与transformer中的张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms列并行与transformer中的张量并行\"><span>GEMMs列并行与Transformer中的张量并行</span></a></h2>\n<p>在现代深度学习模型中，尤其是Transformer架构中，矩阵计算的并行化是提升模型训练和推理速度的关键。本文将探讨GEMMs列并行和Transformer中的张量并行技术。</p>\n<h2 id=\"gemms列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms列并行\"><span>GEMMs列并行</span></a></h2>\n<p>GEMMs列并行是一种将矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 按照列来分割的技术。这种方法可以有效地将计算任务分配到多个GPU上，从而实现并行计算。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250429221945.png\" alt=\"Pasted image 20250429221945.png\"></p>\n<h2 id=\"transformer中的张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformer中的张量并行\"><span>Transformer中的张量并行</span></a></h2>\n<p>Transformer架构包含了Self-Attention和Feed Forward Network，这些操作涉及大量的矩阵计算，非常适合在GPU上进行并行操作，以加速模型的训练和推理过程。\n<img src=\"/img/user/附件/Pasted image 20250429221953.png\" alt=\"Pasted image 20250429221953.png\"></p>\n<h3 id=\"masked-multi-head-self-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#masked-multi-head-self-attention\"><span>Masked Multi-Head Self Attention</span></a></h3>\n<p>Masked Multi-Head Self Attention涉及到大量的矩阵乘法操作，这些操作可以被高效地并行执行，从而提高计算速度。</p>\n<h3 id=\"feed-forward-neural-network\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#feed-forward-neural-network\"><span>Feed Forward Neural Network</span></a></h3>\n<p>Feed Forward Neural Network包含多个全连接层，每个全连接层都涉及矩阵乘法、激活函数（通常是GeLU）和可能的Dropout层。这些操作也是高度并行化的，可以在GPU上迅速执行。</p>\n<p>Megatron的FFN是一个两层MLP，第一层是维度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 变成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">4H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span>，第二层是维度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">4H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 变回到 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span>。具体架构如下，紫色块对应于全连接层，每个蓝色块表示一个被复制 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 次的transformer层，红色的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">x L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">xL</span></span></span></span> 代表此蓝色复制 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span> 次。</p>\n<p>张量并行就是要对Transformer进行切分，Megatron把Masked Multi-Head Self Attention和Feed Forward都进行切分以并行化，通过添加一些同步通信操作来创建一个简单的模型并行实现。</p>\n<h2 id=\"切分mlp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#切分mlp\"><span>切分MLP</span></a></h2>\n<p>从MLP开始，MLP的第一部分是GEMM，后面是激活函数GeLU：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Y = \\text{GeLU} ( X A )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<h3 id=\"对比按照行列切分权重的方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#对比按照行列切分权重的方法\"><span>对比按照行列切分权重的方法</span></a></h3>\n<h4 id=\"行并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行并行\"><span>行并行</span></a></h4>\n<p>行并行的方法如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Y = \\text{GeLU} ( X_1 A_1 + X_2 A_2 )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>但是，由于GeLU是非线性计算：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">≠</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{GeLU} ( X_1 A_1 + X_2 A_2 ) \\neq \\text{GeLU} ( X_1 A_1 ) + \\text{GeLU} ( X_2 A_2 )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mrel\"></span></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>所以这种方案需要在GeLU函数之前加上一个同步点，这个同步点让不同GPU之间交换信息。</p>\n<h4 id=\"列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#列并行\"><span>列并行</span></a></h4>\n<p>列并行的方法如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo stretchy=\"false\">]</mo><mo>=</mo><mo stretchy=\"false\">[</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mi>A</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[ Y_1, Y_2 ] = [ \\text{GeLU} ( X A_1 ), \\text{GeLU} ( X A_2 ) ]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>这种方法不需要同步点，可以直接把两个GeLU的输出拼接起来，而不需要额外的通信（比如GeLU这里就不用all-reduce了）。在前向和后向传递的时候做一次all-reduce通信操作。对于第二个MLP使用行并行，可以直接与第一个MLP得到的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 对应计算，不需要额外通信操作。</p>\n<p>总的来说，在MLP层中，对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用“列切割”，对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span> 采用“行切割”。\n<img src=\"/img/user/附件/Pasted image 20250429222007.png\" alt=\"Pasted image 20250429222007.png\"></p>\n<p>在现代深度学习中，模型的规模和复杂度不断增加，如何高效地利用GPU资源成为了一个重要的研究课题。本文将探讨在神经网络计算中，如何通过合理的切分策略来减少GPU之间的通信量，以提高整体计算效率。</p>\n<h2 id=\"gelu计算中的行列切割策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gelu计算中的行列切割策略\"><span>GELU计算中的行列切割策略</span></a></h2>\n<p>在处理矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 时，我们要尽量保证各GPU上的计算相互独立，从而减少通信量。对于 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 来说，需要进行一次GELU的计算，而GELU是非线性的。这意味着，如果对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用行切割，我们必须在进行GELU前，进行一次AllReduce，这会产生额外的通信量。但如果对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用列切割，每块GPU就可以继续独立计算。一旦确认好 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 进行列切割，那么相应地，矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span> 就需要进行行切割。</p>\n<h2 id=\"通信量分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信量分析\"><span>通信量分析<img src=\"/img/user/附件/Pasted image 20250429222138.png\" alt=\"Pasted image 20250429222138.png\"></span></a></h2>\n<p>在MLP层中，forward和backward阶段各会产生一次AllReduce。AllReduce的过程分为两个阶段：Reduce-Scatter和All-Gather，每个阶段的通信量都相等。设每个阶段的通信量为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span></span></span></span>，则一次AllReduce产生的总通信量为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">4\\Phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4Φ</span></span></span></span>。MLP层的总通信量为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>4</mn><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">4\\Phi\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4Φ</span></span></span></span></span></p>\n<p>其中：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo>=</mo><mi>b</mi><mo>∗</mo><mi>s</mi><mo>∗</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi = b \\ast s \\ast h\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4653em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">h</span></span></span></span></span></p>\n<h2 id=\"self-attention的切分策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#self-attention的切分策略\"><span>Self-Attention的切分策略</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250429222151.png\" alt=\"Pasted image 20250429222151.png\"></p>\n<h3 id=\"多头注意力并行切分\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多头注意力并行切分\"><span>多头注意力并行切分</span></a></h3>\n<p>利用多头注意力固有的并行特性，可以直接以列并行的方法切分QKV相关的GEMMs。首先考虑head=1的情况，self-attention的计算方式如下图所示（图略）。</p>\n<h3 id=\"当num-heads-2时的情况\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#当num-heads-2时的情况\"><span>当num_heads = 2时的情况<img src=\"/img/user/附件/Pasted image 20250429222201.png\" alt=\"Pasted image 20250429222201.png\"></span></a></h3>\n<p>对于每一块权重，我们沿着列方向维度切割一刀。此时每个 head 上的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">Q</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的维度都变成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>k</mi><mrow><mi>d</mi><mi>i</mi><mi>m</mi></mrow></msub><mi mathvariant=\"normal\">/</mi><mi mathvariant=\"normal\">/</mi><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(d_{model}, k_{dim}//2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">im</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">//2</span><span class=\"mclose\">)</span></span></span></span>。每个 head 上单独做矩阵计算，最后将计算结果concat起来。整个流程如下（图略）。</p>\n<p>因此，我们可以把每个头的参数放到一块GPU上，也可以把多个head放到一块GPU上，但尽量保证head总数能被GPU个数整除。</p>\n<h3 id=\"通信量分析-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信量分析-1\"><span>通信量分析</span></a></h3>\n<p>使用 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 来表示GPU的数量。有几块GPU，就把 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 按行维度切成几份。self-attention 层在 forward 和 backward 中各做一次 AllReduce，总通信量也是：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span></span></span></span></span></p>\n<p>其中：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo>=</mo><mi>b</mi><mo>∗</mo><mi>s</mi><mo>∗</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi = b \\ast s \\ast h\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4653em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">h</span></span></span></span></span></p>\n<h2 id=\"embedding切分策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#embedding切分策略\"><span>Embedding切分策略</span></a></h2>\n<h3 id=\"输入层embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#输入层embedding\"><span>输入层embedding</span></a></h3>\n<p>输入层embedding包括word embedding和positional embedding两部分。对于positional embedding，由于 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\max_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mop\"><span class=\"mop\">max</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 本身不会太长，因此每个GPU上都拷贝一份，对显存的压力不大。但对于word embedding，词表的大小较大，需要将word embedding拆分到各个GPU上。</p>\n<h3 id=\"输出层embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#输出层embedding\"><span>输出层embedding</span></a></h3>\n<p>输出层只有一个word embedding。必须时刻保证输入层和输出层共用一套word embedding。在backward过程中，我们在输出层对word embedding计算一次梯度，在输入层中也会计算一次梯度。在用梯度更新word embedding权重时，必须用两次梯度的总和进行更新。</p>\n<h2 id=\"cross-entropy切分的基本流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#cross-entropy切分的基本流程\"><span>Cross-entropy切分的基本流程<img src=\"/img/user/附件/Pasted image 20250429222312.png\" alt=\"Pasted image 20250429222312.png\"></span></a></h2>\n<p>首先，需要对输出层的<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>和<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>做一次All-Gather操作，把它们concat起来形成<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span>。对<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span>的每一行做softmax运算，就可以得到对于当前位置来说，每个词出现的概率。接着，再用此概率和真值组做cross-entropy即可。</p>\n<p>然而，All-Gather操作会产生额外的通讯量：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s \\times v\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span></p>\n<p>当词表<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span>很大时，这个通讯开销也不容忽视。</p>\n<h2 id=\"优化策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优化策略\"><span>优化策略</span></a></h2>\n<p>针对上述情况，可以采取以下优化策略：</p>\n<h3 id=\"gpu上的局部计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpu上的局部计算\"><span>GPU上的局部计算</span></a></h3>\n<p>每块GPU上，我们可以先按行求和，得到各自GPU上的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mi>P</mi><mi>U</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">GPU\\_sum(e)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">GP</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">m</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span>。</p>\n<h3 id=\"allreduce操作\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#allreduce操作\"><span>AllReduce操作</span></a></h3>\n<p>将每块GPU上的结果做AllReduce操作，得到每行最终的</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo>∑</mo><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sum(e)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.6em;vertical-align:-0.55em;\"></span><span class=\"mop op-symbol large-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>这就是softmax中的分母。此时的通讯量为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span></span></p>\n<h3 id=\"计算局部cross-entropy\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算局部cross-entropy\"><span>计算局部Cross-entropy</span></a></h3>\n<p>在每块GPU上，即可计算各自维护部分的</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mi>e</mi><mrow><mo>∑</mo><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{e}{\\sum(e)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.0436em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1076em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>将其与真值做cross-entropy，得到每行的loss，按行加总起来以后得到GPU上的 scalar Loss。</p>\n<h3 id=\"汇总总loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#汇总总loss\"><span>汇总总Loss</span></a></h3>\n<p>将GPU上的 scalar Loss 做AllReduce操作，得到总Loss。此时通讯量为<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span>。</p>\n<p>通过上述优化，我们把原先的通讯量从</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s \\times v\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span></p>\n<p>大大降至</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>+</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s + N\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></p>\n<h2 id=\"transformerblock中的通信优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformerblock中的通信优化\"><span>TransformerBlock中的通信优化<img src=\"/img/user/附件/Pasted image 20250429222329.png\" alt=\"Pasted image 20250429222329.png\"></span></a></h2>\n<p>一个TransformerBlock需要在前向和后向总共需要4个all-reduce通信操作。</p>\n<h2 id=\"张量并行与序列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行与序列并行\"><span>张量并行与序列并行<img src=\"/img/user/附件/Pasted image 20250429222343.png\" alt=\"Pasted image 20250429222343.png\"></span></a></h2>\n<p>张量并行对AttentionBlock和MLP都进行了并行化，但没有对LN和dropout进行并行化。不过后续在megatron3中用序列并行技术对这两部分进行了并行，这部分就不过多展开了。</p>\n<hr>\n<h1 id=\"流水线并行-inter-layer-pipeline-parallelism\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流水线并行-inter-layer-pipeline-parallelism\"><span>流水线并行 Inter-Layer Pipeline Parallelism</span></a></h1>\n<p>在深度学习模型的训练过程中，如何提高效率一直是研究的重点之一。流水线并行是一种将网络按层切分，划分成多组，并将每组放置在不同的设备上进行计算的方法。通过这种方式，可以在不同设备上并行执行不同的模型阶段，从而提高整体效率。</p>\n<h2 id=\"代码块-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码块-1\"><span>代码块</span></a></h2>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 假设模型有8层 两张卡 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">======================</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> ====================</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L3 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> |</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L4 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L5 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L6 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L7 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">======================</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> ====================</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU0</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> GPU1</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 设想一下，当GPU0在进行（前向/后向）计算时，GPU1在干嘛？闲着 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 当GPU1在进行（前向/后向)计算时，GPU0在干嘛？闲着 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 为了防止”一卡工作，众卡围观“，实践中PP也会把batch数据分割成 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 多个micro-batch，流水线执行</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>目前主流的流水线并行方法包括Gpipe和PipeDream。与这两者相比，Megatron中的流水线并行实现略有不同，采用了Virtual Pipeline的方法。传统的流水线并行通常会在一个设备上放置几个模块，通过在计算和通信之间取得平衡来提高效率。然而，虚拟流水线则采取相反的策略。在设备数量不变的前提下，它将流水线阶段进一步细分，以承载更多的通信量，从而降低空闲时间的比率，以缩短每个步骤的执行时间。</p>\n<h3 id=\"gpipe\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpipe\"><span>Gpipe<img src=\"/img/user/附件/Pasted image 20250429222504.png\" alt=\"Pasted image 20250429222504.png\"></span></a></h3>\n<p>Gpipe方法将transformer层按层切分放到不同的设备上，并将一个batch切分成多个mini-batch。在前向传播时，每个mini-batch从第一个设备流向最后一个设备；在反向传播时，在最后一个设备上计算出梯度并更新对应层的参数，然后将梯度传递给前一个设备进行梯度计算和参数更新。这种方法的劣势之一是空泡率（流水线空闲时间）比较高。</p>\n<h3 id=\"pipedream\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#pipedream\"><span>PipeDream<img src=\"/img/user/附件/Pasted image 20250429222515.png\" alt=\"Pasted image 20250429222515.png\"></span></a></h3>\n<p>PipeDream相对于Gpipe的改进主要体现在内存方面。虽然空泡时间和Gpipe一致，但通过合理安排前向和反向过程的顺序，在步骤中间的稳定阶段形成1前向1反向（1F1B）的模式。在这个阶段，每个设备上最少只需要保存一份micro-batch的激活值，最多需要保存流水线阶段数份激活值，从而可以有效降低空泡占比。</p>\n<h3 id=\"virtual-pipeline\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#virtual-pipeline\"><span>Virtual Pipeline<img src=\"/img/user/附件/Pasted image 20250429222523.png\" alt=\"Pasted image 20250429222523.png\"></span></a></h3>\n<p>传统的pipeline并行通常会在一个设备上放置几个block，这是为了扩展效率考虑，在计算强度和通信强度中间取一个平衡。但Virtual Pipeline却反其道而行之，在设备数量不变的情况下，分出更多的pipeline阶段，以更多的通信量换取空泡比率降低，减小了每一步的执行时间。假设网络共有16层（编号0-15），4个设备，前述Gpipe和PipeDream是分成4个阶段，按编号0-3层放Device1，4-7层放Device2，以此类推。Virtual Pipeline则是减小切分粒度，以virtual_pipeline_stage=2为例，将0-1层放Device1, 2-3层放在Device2，...，6-7层放到Device4，8-9层继续放在Device1，10-11层放在Device2，...，14-15层放在Device4。在稳定的时候也是1F1B的形式。按照这种方式，设备之间的点对点通信次数（量）直接翻了virtual_pipeline_stage倍，但空泡比率降低了。\n<img src=\"/img/user/附件/Pasted image 20250429222538.png\" alt=\"Pasted image 20250429222538.png\"></p>\n<h2 id=\"_3d并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3d并行\"><span>3D并行</span></a></h2>\n<h2 id=\"数据并行-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据并行-1\"><span>数据并行</span></a></h2>\n<h3 id=\"计算效率高-实现简单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算效率高-实现简单\"><span>计算效率高， 实现简单</span></a></h3>\n<p>数据并行是一种计算效率极高且实现简单的并行技术。在这种模式下，每个计算节点都保存完整的模型、梯度和优化器状态，因此显存效率不高。然而，当增加并行度时，单卡的计算量保持恒定，可以实现近乎完美的线性扩展。不过，规约梯度的通信开销与模型大小成正相关。</p>\n<h2 id=\"张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行\"><span>张量并行</span></a></h2>\n<h3 id=\"因模型结构而异-实现难度大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#因模型结构而异-实现难度大\"><span>因模型结构而异， 实现难度大</span></a></h3>\n<p>张量并行的实现难度较大，因为它依赖于模型的具体结构。随着并行度增加，显存占用成比例地减少，这是减少单层神经网络中间激活的唯一方法。然而，频繁的通信限制了两个通信阶段之间的计算量，导致计算效率很低。</p>\n<h2 id=\"流水线并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流水线并行\"><span>流水线并行</span></a></h2>\n<h3 id=\"通信成本最低\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信成本最低\"><span>通信成本最低</span></a></h3>\n<p>流水线并行具有最低的通信成本。虽然减少的显存与流水线并行度成正相关，但流水线并行不会减少每层中间激活的显存占用。其计算效率得益于成本更低的点对点（P2P）通信，通信量与流水线各个阶段边界的激活值大小成正相关。</p>\n<h2 id=\"显存和通信效率比较\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存和通信效率比较\"><span>显存和通信效率比较</span></a></h2>\n<ul>\n<li>显存效率：模型并行 &gt; 流水线并行 &gt; 数据并行</li>\n<li>通信效率：流水线并行 &gt; 数据并行 &gt; 模型并行</li>\n</ul>\n<h2 id=\"_3d并行技术的混合应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3d并行技术的混合应用\"><span>3D并行技术的混合应用</span></a></h2>\n<p>3D并行技术是混合数据并行（DP）、张量并行（TP）和流水线并行（PP）组成的。在实际应用中，可以通过四路张量并行、四路流水线并行、两路数据并行以及32个worker来实现高效的3D并行。\n<img src=\"/img/user/附件/Pasted image 20250429222809.png\" alt=\"Pasted image 20250429222809.png\"><img src=\"/img/user/附件/Pasted image 20250429222815.png\" alt=\"Pasted image 20250429222815.png\"><img src=\"/img/user/附件/Pasted image 20250429222822.png\" alt=\"Pasted image 20250429222822.png\"></p>\n<h2 id=\"_4d并行技术在llama3中的应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4d并行技术在llama3中的应用\"><span>4D并行技术在LLaMA3中的应用</span></a></h2>\n<p>在LLaMA3的技术报告中，他们还使用了4D并行技术。除了PP、TP和DP，还加上了CP（context parallel），进一步提高了模型训练的效率。\n<img src=\"/img/user/附件/Pasted image 20250429222830.png\" alt=\"Pasted image 20250429222830.png\">\n<img src=\"/img/user/附件/Pasted image 20250429222839.png\" alt=\"Pasted image 20250429222839.png\"></p>\n<hr>\n</template>","contentStripped":"<h2 id=\"megatron-lm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm\"><span>Megatron-LM</span></a></h2>\n<p>Megatron-LM 是一个基于 PyTorch 的分布式训练框架，用来训练基于 Transformer 的大型语言模型。大型模型能够提供更精准和强大的语义理解与推理能力。随着计算资源的普及和数据集的增大，模型参数的数量呈指数级增长。然而，训练这样规模庞大的模型面临着一些挑战：</p>\n<h3 id=\"megatron-lm-的优缺点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm-的优缺点\"><span>Megatron-LM 的优缺点</span></a></h3>\n<p>Megatron-LM 是预训练必用框架，训练速度比 DeepSpeed 快，但也有诸多缺点：</p>\n<ul>\n<li>Megatron-LM 作为框架完全没有分层分模块，也没有太多抽象设计，导致模型跟框架无法解耦，需要手动切割模型并且适配，只适合 GPT/LLama 系列模型。</li>\n<li>通信有明显精度损失，特别是 ringallReduce。</li>\n<li>Megatron 推理时，kv cache 存在低级 bug，容易导致 RLHF 训练失败。</li>\n<li>混合精度训练时，bf16 没有 master weight。</li>\n<li>显存乱申请，加上 PyTorch allocator 拉垮，经常出现 OOM（Out of Memory）。</li>\n</ul>\n<p>虽然有这么多缺点，但是 Megatron-LM 仍然是千卡集群以上最佳的选择，没有之一。千卡以内可以选择 FSDP 或者 TorchTitan。</p>\n<h3 id=\"megatron-lm-的特点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#megatron-lm-的特点\"><span>Megatron-LM 的特点</span></a></h3>\n<p>Megatron-LM 支持数据并行、张量并行、流水线并行和专家并行，简称 4D 并行。</p>\n<h3 id=\"提醒\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#提醒\"><span>提醒</span></a></h3>\n<p>可以看看李沐的视频。</p>\n<h2 id=\"大型模型训练的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#大型模型训练的挑战\"><span>大型模型训练的挑战</span></a></h2>\n<p>训练大型模型面临的挑战主要包括显存限制、计算挑战以及并行策略挑战。</p>\n<h3 id=\"显存限制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存限制\"><span>显存限制</span></a></h3>\n<p>即便是目前最大的 GPU 主内存也难以容纳这些模型的参数。举例来说，一个 1750 亿参数的 GPT-3 模型需要约 700GB 的参数空间，对应的梯度约为 700GB，而优化器状态还需额外的 1400GB，总计需求高达 2.8TB。</p>\n<h3 id=\"计算挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算挑战\"><span>计算挑战</span></a></h3>\n<p>即使我们设法将模型适应单个 GPU（例如通过在主机内存和设备内存之间进行参数交换），模型所需的大量计算操作也会导致训练时间大幅延长。举个例子，使用一块 NVIDIA V100 GPU 来训练拥有 1750 亿参数的 GPT-3 模型，大约需要耗时 288 年。</p>\n<h3 id=\"并行策略挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#并行策略挑战\"><span>并行策略挑战</span></a></h3>\n<p>不同的并行策略对应不同的通信模式和通信量，这也是一个需要考虑的挑战。</p>\n<p>在现代的深度学习训练中，数据并行是一种常用的策略，通过将模型的训练任务分散到多个计算单元上来提高效率和加速训练过程。本文将详细介绍数据并行的概念、其优缺点，以及在分布式计算中常用的 All-Reduce 操作。</p>\n<h2 id=\"数据并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据并行\"><span>数据并行</span></a></h2>\n<p>数据并行（Data Parallelism, DP）是一种将模型训练任务分散到多个计算单元（如 GPU）的策略。假设有 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 张卡（worker），每张卡都保存一个完整模型的副本。在每一次迭代（iteration/step）中，输入数据集 batch 被分割成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 个等大小的 micro-batch，每张卡根据拿到的 micro-batch 数据独立计算梯度。随后，worker 定期聚合它们的梯度（调用 All-Reduce 计算梯度均值），每张卡再独立进行参数更新。</p>\n<p>对于无法放进单个 Worker 的大模型，可以在模型的较小片段上使用数据并行。这种方法在扩展性上通常表现出色，但存在两个限制：\n<img src=\"/img/user/附件/Pasted image 20250429221511.png\" alt=\"Pasted image 20250429221511.png\"><img src=\"/img/user/附件/Pasted image 20250429221518.png\" alt=\"Pasted image 20250429221518.png\"></p>\n<h3 id=\"限制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#限制\"><span>限制</span></a></h3>\n<ol>\n<li>\n<p><strong>降低 GPU 利用率</strong>：在一定点之后，每个 GPU 的批量大小变得太小，这会降低 GPU 的利用率，并增加通信成本。</p>\n</li>\n<li>\n<p><strong>设备数限制</strong>：可用于训练的 GPU 设备数量受到批量大小的限制，这限制了可以使用的最大设备数。</p>\n</li>\n</ol>\n<h2 id=\"all-reduce-操作\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#all-reduce-操作\"><span>All-Reduce 操作</span></a></h2>\n<p>All-Reduce 是一种在并行计算和分布式计算中常用的通信操作，用于在多个计算节点（例如 GPU 或 CPU）之间汇总数据。</p>\n<h3 id=\"工作原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#工作原理\"><span>工作原理</span></a></h3>\n<ol>\n<li>\n<p><strong>数据分发</strong>：每个节点在完成了自己的计算后会生成一组梯度（或者其他需要汇总的数据），这些数据需要在所有节点之间进行汇总。</p>\n</li>\n<li>\n<p><strong>数据汇总</strong>：All-Reduce 操作将每个节点上的数据在所有节点之间进行相加或其他类型的归约操作（例如，求和、求平均、求最大值等）。这意味着最终每个节点都会得到相同的结果，该结果是所有节点数据的汇总。</p>\n</li>\n<li>\n<p><strong>结果广播</strong>：完成汇总操作后，汇总结果会被广播回每个节点，使得每个节点都持有相同的汇总数据。这一步确保了所有节点在下一步的计算中都能使用一致的数据。</p>\n</li>\n</ol>\n<h2 id=\"模型并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型并行\"><span>模型并行</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250429221654.png\" alt=\"Pasted image 20250429221654.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250429221701.png\" alt=\"Pasted image 20250429221701.png\"></p>\n<h2 id=\"model-parallelism-mp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#model-parallelism-mp\"><span>Model Parallelism MP</span></a></h2>\n<p>对于数据并行的限制，可以采用一些内存管理技术，比如激活检查点（Activation Checkpointing）。</p>\n<h3 id=\"activation-checkpointing-gradient-checkpointing\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#activation-checkpointing-gradient-checkpointing\"><span>Activation Checkpointing（gradient_checkpointing）</span></a></h3>\n<p>激活检查点是一种在前向传播过程中计算节点的激活值并保存的方法。计算下一个节点完成后丢弃中间节点的激活值，反向传播时如果有保存下来的梯度就直接使用，如果没有就使用保存下来的前一个节点的梯度重新计算当前节点的梯度再使用。</p>\n<p>此外，还可以使用模型并行来划分模型的不同阶段，从而解决GPU内存容量和计算限制的问题，使得权重和关联的优化器状态不需要同时存储在一个GPU上。一个模型的内存和计算被分布在多个计算节点上，主要的方式分为流水线并行和张量并行。</p>\n<h2 id=\"张量并行-intra-layer-tensor-parallelism\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行-intra-layer-tensor-parallelism\"><span>张量并行 Intra-Layer Tensor Parallelism</span></a></h2>\n<p>有的tensor/layer很大，一张卡放不下，将tensor分割成多块，一张卡存一块。这也可以被理解为将大矩阵运算拆分成多个小矩阵运算，然后分布到不同的设备上进行计算。\n<img src=\"/img/user/附件/Pasted image 20250429221727.png\" alt=\"Pasted image 20250429221727.png\"></p>\n<h3 id=\"代码块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码块\"><span>代码块</span></a></h3>\n<p>高度的模型并行会产生很多的小矩阵乘法(GEMMS)。对于一个GEMMs <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">Y = X A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span></span></span></span>，按照对权重矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span>的分块方式，张量并行又可以分成行并行和列并行。神经网络中的线性层（Linear层），可以将其看作是将输入矩阵分块进行计算，然后将计算结果合并成输出矩阵。这个过程涉及矩阵乘法和加法操作，其中矩阵乘法涉及到权重矩阵和输入数据之间的乘法，然后再加上偏置向量；对于非线性层（例如激活函数层），通常不需要进行额外的设计。这些层的计算过程是基于输入数据应用某种非线性函数，例如ReLU（修正线性单元）、Sigmoid、Tanh等。这些函数在数学上是已知的，只需要将输入数据传递给这些函数，然后得到输出。整体来看，神经网络的计算可以被抽象为一系列的矩阵和向量操作，其中线性层涉及矩阵乘法和加法，而非线性层涉及特定的函数计算。这些操作在深度学习框架中会被高度优化，以提高计算效率和训练速度。</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 假设模型有三层：L0, L1, L2 每层有两个神经元 两张卡</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU0</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">----</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> b0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> c0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">----</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">--</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> b1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> c1</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"gemms行并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms行并行\"><span>GEMMs行并行</span></a></h3>\n<p>先分析Row Parallelism，就是把权重矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span>按照行切分成两部分，对应的把<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>按照列切分成两部分：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>X</mi><mi>A</mi><mo>=</mo><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>X</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>X</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.16em\" columnalign=\"center center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>A</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>A</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><mo>=</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo>=</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>=</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X A = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\begin{bmatrix} A_1 &amp; A_2 \\end{bmatrix} = X_1 A_1 + X_2 A_2 = Y_1 + Y_2 = Y\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.41em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.95em;\"><span></span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">]</span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85em;\"><span style=\"top:-3.01em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85em;\"><span style=\"top:-3.01em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">]</span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span></p>\n<p>这样<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">X_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>A</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">A_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>可以在第一个GPU上计算，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">X_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>A</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">A_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>可以在第二个GPU上进行计算。</p>\n<p>通过这种方式，可以有效地将大型模型的计算任务分配到多个GPU上，从而提高计算效率，同时解决单个GPU内存不足的问题。这种技术在处理大型深度学习模型时尤为重要，因为它能够显著提高训练速度，并允许更大规模的模型在有限硬件资源下进行训练。\n<img src=\"/img/user/附件/Pasted image 20250429221755.png\" alt=\"Pasted image 20250429221755.png\"><img src=\"/img/user/附件/Pasted image 20250429221811.png\" alt=\"Pasted image 20250429221811.png\"></p>\n<h2 id=\"gemms列并行与transformer中的张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms列并行与transformer中的张量并行\"><span>GEMMs列并行与Transformer中的张量并行</span></a></h2>\n<p>在现代深度学习模型中，尤其是Transformer架构中，矩阵计算的并行化是提升模型训练和推理速度的关键。本文将探讨GEMMs列并行和Transformer中的张量并行技术。</p>\n<h2 id=\"gemms列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gemms列并行\"><span>GEMMs列并行</span></a></h2>\n<p>GEMMs列并行是一种将矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 按照列来分割的技术。这种方法可以有效地将计算任务分配到多个GPU上，从而实现并行计算。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250429221945.png\" alt=\"Pasted image 20250429221945.png\"></p>\n<h2 id=\"transformer中的张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformer中的张量并行\"><span>Transformer中的张量并行</span></a></h2>\n<p>Transformer架构包含了Self-Attention和Feed Forward Network，这些操作涉及大量的矩阵计算，非常适合在GPU上进行并行操作，以加速模型的训练和推理过程。\n<img src=\"/img/user/附件/Pasted image 20250429221953.png\" alt=\"Pasted image 20250429221953.png\"></p>\n<h3 id=\"masked-multi-head-self-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#masked-multi-head-self-attention\"><span>Masked Multi-Head Self Attention</span></a></h3>\n<p>Masked Multi-Head Self Attention涉及到大量的矩阵乘法操作，这些操作可以被高效地并行执行，从而提高计算速度。</p>\n<h3 id=\"feed-forward-neural-network\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#feed-forward-neural-network\"><span>Feed Forward Neural Network</span></a></h3>\n<p>Feed Forward Neural Network包含多个全连接层，每个全连接层都涉及矩阵乘法、激活函数（通常是GeLU）和可能的Dropout层。这些操作也是高度并行化的，可以在GPU上迅速执行。</p>\n<p>Megatron的FFN是一个两层MLP，第一层是维度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 变成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">4H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span>，第二层是维度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">4H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 变回到 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span>。具体架构如下，紫色块对应于全连接层，每个蓝色块表示一个被复制 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 次的transformer层，红色的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">x L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">xL</span></span></span></span> 代表此蓝色复制 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span> 次。</p>\n<p>张量并行就是要对Transformer进行切分，Megatron把Masked Multi-Head Self Attention和Feed Forward都进行切分以并行化，通过添加一些同步通信操作来创建一个简单的模型并行实现。</p>\n<h2 id=\"切分mlp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#切分mlp\"><span>切分MLP</span></a></h2>\n<p>从MLP开始，MLP的第一部分是GEMM，后面是激活函数GeLU：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Y = \\text{GeLU} ( X A )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathnormal\">A</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<h3 id=\"对比按照行列切分权重的方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#对比按照行列切分权重的方法\"><span>对比按照行列切分权重的方法</span></a></h3>\n<h4 id=\"行并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行并行\"><span>行并行</span></a></h4>\n<p>行并行的方法如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Y = \\text{GeLU} ( X_1 A_1 + X_2 A_2 )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>但是，由于GeLU是非线性计算：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">≠</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><msub><mi>A</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{GeLU} ( X_1 A_1 + X_2 A_2 ) \\neq \\text{GeLU} ( X_1 A_1 ) + \\text{GeLU} ( X_2 A_2 )\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mrel\"></span></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>所以这种方案需要在GeLU函数之前加上一个同步点，这个同步点让不同GPU之间交换信息。</p>\n<h4 id=\"列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#列并行\"><span>列并行</span></a></h4>\n<p>列并行的方法如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo stretchy=\"false\">]</mo><mo>=</mo><mo stretchy=\"false\">[</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mi>A</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mtext>GeLU</mtext><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[ Y_1, Y_2 ] = [ \\text{GeLU} ( X A_1 ), \\text{GeLU} ( X A_2 ) ]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\">GeLU</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>这种方法不需要同步点，可以直接把两个GeLU的输出拼接起来，而不需要额外的通信（比如GeLU这里就不用all-reduce了）。在前向和后向传递的时候做一次all-reduce通信操作。对于第二个MLP使用行并行，可以直接与第一个MLP得到的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 对应计算，不需要额外通信操作。</p>\n<p>总的来说，在MLP层中，对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用“列切割”，对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span> 采用“行切割”。\n<img src=\"/img/user/附件/Pasted image 20250429222007.png\" alt=\"Pasted image 20250429222007.png\"></p>\n<p>在现代深度学习中，模型的规模和复杂度不断增加，如何高效地利用GPU资源成为了一个重要的研究课题。本文将探讨在神经网络计算中，如何通过合理的切分策略来减少GPU之间的通信量，以提高整体计算效率。</p>\n<h2 id=\"gelu计算中的行列切割策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gelu计算中的行列切割策略\"><span>GELU计算中的行列切割策略</span></a></h2>\n<p>在处理矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 时，我们要尽量保证各GPU上的计算相互独立，从而减少通信量。对于 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 来说，需要进行一次GELU的计算，而GELU是非线性的。这意味着，如果对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用行切割，我们必须在进行GELU前，进行一次AllReduce，这会产生额外的通信量。但如果对 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 采用列切割，每块GPU就可以继续独立计算。一旦确认好 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span> 进行列切割，那么相应地，矩阵 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span> 就需要进行行切割。</p>\n<h2 id=\"通信量分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信量分析\"><span>通信量分析<img src=\"/img/user/附件/Pasted image 20250429222138.png\" alt=\"Pasted image 20250429222138.png\"></span></a></h2>\n<p>在MLP层中，forward和backward阶段各会产生一次AllReduce。AllReduce的过程分为两个阶段：Reduce-Scatter和All-Gather，每个阶段的通信量都相等。设每个阶段的通信量为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span></span></span></span>，则一次AllReduce产生的总通信量为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">4\\Phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4Φ</span></span></span></span>。MLP层的总通信量为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>4</mn><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">4\\Phi\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4Φ</span></span></span></span></span></p>\n<p>其中：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo>=</mo><mi>b</mi><mo>∗</mo><mi>s</mi><mo>∗</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi = b \\ast s \\ast h\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4653em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">h</span></span></span></span></span></p>\n<h2 id=\"self-attention的切分策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#self-attention的切分策略\"><span>Self-Attention的切分策略</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250429222151.png\" alt=\"Pasted image 20250429222151.png\"></p>\n<h3 id=\"多头注意力并行切分\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多头注意力并行切分\"><span>多头注意力并行切分</span></a></h3>\n<p>利用多头注意力固有的并行特性，可以直接以列并行的方法切分QKV相关的GEMMs。首先考虑head=1的情况，self-attention的计算方式如下图所示（图略）。</p>\n<h3 id=\"当num-heads-2时的情况\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#当num-heads-2时的情况\"><span>当num_heads = 2时的情况<img src=\"/img/user/附件/Pasted image 20250429222201.png\" alt=\"Pasted image 20250429222201.png\"></span></a></h3>\n<p>对于每一块权重，我们沿着列方向维度切割一刀。此时每个 head 上的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">Q</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的维度都变成 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>k</mi><mrow><mi>d</mi><mi>i</mi><mi>m</mi></mrow></msub><mi mathvariant=\"normal\">/</mi><mi mathvariant=\"normal\">/</mi><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(d_{model}, k_{dim}//2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">im</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">//2</span><span class=\"mclose\">)</span></span></span></span>。每个 head 上单独做矩阵计算，最后将计算结果concat起来。整个流程如下（图略）。</p>\n<p>因此，我们可以把每个头的参数放到一块GPU上，也可以把多个head放到一块GPU上，但尽量保证head总数能被GPU个数整除。</p>\n<h3 id=\"通信量分析-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信量分析-1\"><span>通信量分析</span></a></h3>\n<p>使用 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 来表示GPU的数量。有几块GPU，就把 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 按行维度切成几份。self-attention 层在 forward 和 backward 中各做一次 AllReduce，总通信量也是：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span></span></span></span></span></p>\n<p>其中：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo>=</mo><mi>b</mi><mo>∗</mo><mi>s</mi><mo>∗</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">\\Phi = b \\ast s \\ast h\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Φ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4653em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">h</span></span></span></span></span></p>\n<h2 id=\"embedding切分策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#embedding切分策略\"><span>Embedding切分策略</span></a></h2>\n<h3 id=\"输入层embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#输入层embedding\"><span>输入层embedding</span></a></h3>\n<p>输入层embedding包括word embedding和positional embedding两部分。对于positional embedding，由于 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\max_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mop\"><span class=\"mop\">max</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 本身不会太长，因此每个GPU上都拷贝一份，对显存的压力不大。但对于word embedding，词表的大小较大，需要将word embedding拆分到各个GPU上。</p>\n<h3 id=\"输出层embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#输出层embedding\"><span>输出层embedding</span></a></h3>\n<p>输出层只有一个word embedding。必须时刻保证输入层和输出层共用一套word embedding。在backward过程中，我们在输出层对word embedding计算一次梯度，在输入层中也会计算一次梯度。在用梯度更新word embedding权重时，必须用两次梯度的总和进行更新。</p>\n<h2 id=\"cross-entropy切分的基本流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#cross-entropy切分的基本流程\"><span>Cross-entropy切分的基本流程<img src=\"/img/user/附件/Pasted image 20250429222312.png\" alt=\"Pasted image 20250429222312.png\"></span></a></h2>\n<p>首先，需要对输出层的<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>和<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">Y_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>做一次All-Gather操作，把它们concat起来形成<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span>。对<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span>的每一行做softmax运算，就可以得到对于当前位置来说，每个词出现的概率。接着，再用此概率和真值组做cross-entropy即可。</p>\n<p>然而，All-Gather操作会产生额外的通讯量：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s \\times v\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span></p>\n<p>当词表<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span>很大时，这个通讯开销也不容忽视。</p>\n<h2 id=\"优化策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优化策略\"><span>优化策略</span></a></h2>\n<p>针对上述情况，可以采取以下优化策略：</p>\n<h3 id=\"gpu上的局部计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpu上的局部计算\"><span>GPU上的局部计算</span></a></h3>\n<p>每块GPU上，我们可以先按行求和，得到各自GPU上的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mi>P</mi><mi>U</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">GPU\\_sum(e)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">GP</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">m</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span>。</p>\n<h3 id=\"allreduce操作\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#allreduce操作\"><span>AllReduce操作</span></a></h3>\n<p>将每块GPU上的结果做AllReduce操作，得到每行最终的</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo>∑</mo><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sum(e)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.6em;vertical-align:-0.55em;\"></span><span class=\"mop op-symbol large-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>这就是softmax中的分母。此时的通讯量为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span></span></p>\n<h3 id=\"计算局部cross-entropy\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算局部cross-entropy\"><span>计算局部Cross-entropy</span></a></h3>\n<p>在每块GPU上，即可计算各自维护部分的</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mi>e</mi><mrow><mo>∑</mo><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{e}{\\sum(e)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.0436em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1076em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>将其与真值做cross-entropy，得到每行的loss，按行加总起来以后得到GPU上的 scalar Loss。</p>\n<h3 id=\"汇总总loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#汇总总loss\"><span>汇总总Loss</span></a></h3>\n<p>将GPU上的 scalar Loss 做AllReduce操作，得到总Loss。此时通讯量为<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span>。</p>\n<p>通过上述优化，我们把原先的通讯量从</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s \\times v\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span></p>\n<p>大大降至</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mo>×</mo><mi>s</mi><mo>+</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">b \\times s + N\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></p>\n<h2 id=\"transformerblock中的通信优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformerblock中的通信优化\"><span>TransformerBlock中的通信优化<img src=\"/img/user/附件/Pasted image 20250429222329.png\" alt=\"Pasted image 20250429222329.png\"></span></a></h2>\n<p>一个TransformerBlock需要在前向和后向总共需要4个all-reduce通信操作。</p>\n<h2 id=\"张量并行与序列并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行与序列并行\"><span>张量并行与序列并行<img src=\"/img/user/附件/Pasted image 20250429222343.png\" alt=\"Pasted image 20250429222343.png\"></span></a></h2>\n<p>张量并行对AttentionBlock和MLP都进行了并行化，但没有对LN和dropout进行并行化。不过后续在megatron3中用序列并行技术对这两部分进行了并行，这部分就不过多展开了。</p>\n<hr>\n<h1 id=\"流水线并行-inter-layer-pipeline-parallelism\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流水线并行-inter-layer-pipeline-parallelism\"><span>流水线并行 Inter-Layer Pipeline Parallelism</span></a></h1>\n<p>在深度学习模型的训练过程中，如何提高效率一直是研究的重点之一。流水线并行是一种将网络按层切分，划分成多组，并将每组放置在不同的设备上进行计算的方法。通过这种方式，可以在不同设备上并行执行不同的模型阶段，从而提高整体效率。</p>\n<h2 id=\"代码块-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码块-1\"><span>代码块</span></a></h2>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 假设模型有8层 两张卡 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">======================</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> ====================</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L0 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L1 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L2 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L3 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> |</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L4 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L5 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L6 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> L7 </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">|</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">======================</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> ====================</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">GPU0</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> GPU1</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 设想一下，当GPU0在进行（前向/后向）计算时，GPU1在干嘛？闲着 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 当GPU1在进行（前向/后向)计算时，GPU0在干嘛？闲着 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 为了防止”一卡工作，众卡围观“，实践中PP也会把batch数据分割成 ​</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 多个micro-batch，流水线执行</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>目前主流的流水线并行方法包括Gpipe和PipeDream。与这两者相比，Megatron中的流水线并行实现略有不同，采用了Virtual Pipeline的方法。传统的流水线并行通常会在一个设备上放置几个模块，通过在计算和通信之间取得平衡来提高效率。然而，虚拟流水线则采取相反的策略。在设备数量不变的前提下，它将流水线阶段进一步细分，以承载更多的通信量，从而降低空闲时间的比率，以缩短每个步骤的执行时间。</p>\n<h3 id=\"gpipe\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpipe\"><span>Gpipe<img src=\"/img/user/附件/Pasted image 20250429222504.png\" alt=\"Pasted image 20250429222504.png\"></span></a></h3>\n<p>Gpipe方法将transformer层按层切分放到不同的设备上，并将一个batch切分成多个mini-batch。在前向传播时，每个mini-batch从第一个设备流向最后一个设备；在反向传播时，在最后一个设备上计算出梯度并更新对应层的参数，然后将梯度传递给前一个设备进行梯度计算和参数更新。这种方法的劣势之一是空泡率（流水线空闲时间）比较高。</p>\n<h3 id=\"pipedream\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#pipedream\"><span>PipeDream<img src=\"/img/user/附件/Pasted image 20250429222515.png\" alt=\"Pasted image 20250429222515.png\"></span></a></h3>\n<p>PipeDream相对于Gpipe的改进主要体现在内存方面。虽然空泡时间和Gpipe一致，但通过合理安排前向和反向过程的顺序，在步骤中间的稳定阶段形成1前向1反向（1F1B）的模式。在这个阶段，每个设备上最少只需要保存一份micro-batch的激活值，最多需要保存流水线阶段数份激活值，从而可以有效降低空泡占比。</p>\n<h3 id=\"virtual-pipeline\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#virtual-pipeline\"><span>Virtual Pipeline<img src=\"/img/user/附件/Pasted image 20250429222523.png\" alt=\"Pasted image 20250429222523.png\"></span></a></h3>\n<p>传统的pipeline并行通常会在一个设备上放置几个block，这是为了扩展效率考虑，在计算强度和通信强度中间取一个平衡。但Virtual Pipeline却反其道而行之，在设备数量不变的情况下，分出更多的pipeline阶段，以更多的通信量换取空泡比率降低，减小了每一步的执行时间。假设网络共有16层（编号0-15），4个设备，前述Gpipe和PipeDream是分成4个阶段，按编号0-3层放Device1，4-7层放Device2，以此类推。Virtual Pipeline则是减小切分粒度，以virtual_pipeline_stage=2为例，将0-1层放Device1, 2-3层放在Device2，...，6-7层放到Device4，8-9层继续放在Device1，10-11层放在Device2，...，14-15层放在Device4。在稳定的时候也是1F1B的形式。按照这种方式，设备之间的点对点通信次数（量）直接翻了virtual_pipeline_stage倍，但空泡比率降低了。\n<img src=\"/img/user/附件/Pasted image 20250429222538.png\" alt=\"Pasted image 20250429222538.png\"></p>\n<h2 id=\"_3d并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3d并行\"><span>3D并行</span></a></h2>\n<h2 id=\"数据并行-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据并行-1\"><span>数据并行</span></a></h2>\n<h3 id=\"计算效率高-实现简单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算效率高-实现简单\"><span>计算效率高， 实现简单</span></a></h3>\n<p>数据并行是一种计算效率极高且实现简单的并行技术。在这种模式下，每个计算节点都保存完整的模型、梯度和优化器状态，因此显存效率不高。然而，当增加并行度时，单卡的计算量保持恒定，可以实现近乎完美的线性扩展。不过，规约梯度的通信开销与模型大小成正相关。</p>\n<h2 id=\"张量并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#张量并行\"><span>张量并行</span></a></h2>\n<h3 id=\"因模型结构而异-实现难度大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#因模型结构而异-实现难度大\"><span>因模型结构而异， 实现难度大</span></a></h3>\n<p>张量并行的实现难度较大，因为它依赖于模型的具体结构。随着并行度增加，显存占用成比例地减少，这是减少单层神经网络中间激活的唯一方法。然而，频繁的通信限制了两个通信阶段之间的计算量，导致计算效率很低。</p>\n<h2 id=\"流水线并行\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流水线并行\"><span>流水线并行</span></a></h2>\n<h3 id=\"通信成本最低\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通信成本最低\"><span>通信成本最低</span></a></h3>\n<p>流水线并行具有最低的通信成本。虽然减少的显存与流水线并行度成正相关，但流水线并行不会减少每层中间激活的显存占用。其计算效率得益于成本更低的点对点（P2P）通信，通信量与流水线各个阶段边界的激活值大小成正相关。</p>\n<h2 id=\"显存和通信效率比较\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存和通信效率比较\"><span>显存和通信效率比较</span></a></h2>\n<ul>\n<li>显存效率：模型并行 &gt; 流水线并行 &gt; 数据并行</li>\n<li>通信效率：流水线并行 &gt; 数据并行 &gt; 模型并行</li>\n</ul>\n<h2 id=\"_3d并行技术的混合应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3d并行技术的混合应用\"><span>3D并行技术的混合应用</span></a></h2>\n<p>3D并行技术是混合数据并行（DP）、张量并行（TP）和流水线并行（PP）组成的。在实际应用中，可以通过四路张量并行、四路流水线并行、两路数据并行以及32个worker来实现高效的3D并行。\n<img src=\"/img/user/附件/Pasted image 20250429222809.png\" alt=\"Pasted image 20250429222809.png\"><img src=\"/img/user/附件/Pasted image 20250429222815.png\" alt=\"Pasted image 20250429222815.png\"><img src=\"/img/user/附件/Pasted image 20250429222822.png\" alt=\"Pasted image 20250429222822.png\"></p>\n<h2 id=\"_4d并行技术在llama3中的应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4d并行技术在llama3中的应用\"><span>4D并行技术在LLaMA3中的应用</span></a></h2>\n<p>在LLaMA3的技术报告中，他们还使用了4D并行技术。除了PP、TP和DP，还加上了CP（context parallel），进一步提高了模型训练的效率。\n<img src=\"/img/user/附件/Pasted image 20250429222830.png\" alt=\"Pasted image 20250429222830.png\">\n<img src=\"/img/user/附件/Pasted image 20250429222839.png\" alt=\"Pasted image 20250429222839.png\"></p>\n<hr>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## Megatron-LM\nMegatron-LM 是一个基于 PyTorch 的分布式训练框架，用来训练基于 Transformer 的大型语言模型。大型模型能够提供更精准和强大的语义理解与推理能力。随着计算资源的普及和数据集的增大，模型参数的数量呈指数级增长。然而，训练这样规模庞大的模型面临着一些挑战：\n\n### Megatron-LM 的优缺点\nMegatron-LM 是预训练必用框架，训练速度比 DeepSpeed 快，但也有诸多缺点：\n\n- Megatron-LM 作为框架完全没有分层分模块，也没有太多抽象设计，导致模型跟框架无法解耦，需要手动切割模型并且适配，只适合 GPT/LLama 系列模型。\n- 通信有明显精度损失，特别是 ringallReduce。\n- Megatron 推理时，kv cache 存在低级 bug，容易导致 RLHF 训练失败。\n- 混合精度训练时，bf16 没有 master weight。\n- 显存乱申请，加上 PyTorch allocator 拉垮，经常出现 OOM（Out of Memory）。\n\n虽然有这么多缺点，但是 Megatron-LM 仍然是千卡集群以上最佳的选择，没有之一。千卡以内可以选择 FSDP 或者 TorchTitan。\n\n\n### Megatron-LM 的特点\nMegatron-LM 支持数据并行、张量并行、流水线并行和专家并行，简称 4D 并行。\n\n\n### 提醒\n可以看看李沐的视频。\n\n\n## 大型模型训练的挑战\n训练大型模型面临的挑战主要包括显存限制、计算挑战以及并行策略挑战。\n\n### 显存限制\n即便是目前最大的 GPU 主内存也难以容纳这些模型的参数。举例来说，一个 1750 亿参数的 GPT-3 模型需要约 700GB 的参数空间，对应的梯度约为 700GB，而优化器状态还需额外的 1400GB，总计需求高达 2.8TB。\n\n\n### 计算挑战\n即使我们设法将模型适应单个 GPU（例如通过在主机内存和设备内存之间进行参数交换），模型所需的大量计算操作也会导致训练时间大幅延长。举个例子，使用一块 NVIDIA V100 GPU 来训练拥有 1750 亿参数的 GPT-3 模型，大约需要耗时 288 年。\n\n\n### 并行策略挑战\n不同的并行策略对应不同的通信模式和通信量，这也是一个需要考虑的挑战。\n\n\n\n\n\n在现代的深度学习训练中，数据并行是一种常用的策略，通过将模型的训练任务分散到多个计算单元上来提高效率和加速训练过程。本文将详细介绍数据并行的概念、其优缺点，以及在分布式计算中常用的 All-Reduce 操作。\n\n\n## 数据并行\n数据并行（Data Parallelism, DP）是一种将模型训练任务分散到多个计算单元（如 GPU）的策略。假设有 $N$ 张卡（worker），每张卡都保存一个完整模型的副本。在每一次迭代（iteration/step）中，输入数据集 batch 被分割成 $N$ 个等大小的 micro-batch，每张卡根据拿到的 micro-batch 数据独立计算梯度。随后，worker 定期聚合它们的梯度（调用 All-Reduce 计算梯度均值），每张卡再独立进行参数更新。\n\n对于无法放进单个 Worker 的大模型，可以在模型的较小片段上使用数据并行。这种方法在扩展性上通常表现出色，但存在两个限制：\n![Pasted image 20250429221511.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221511.png)![Pasted image 20250429221518.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221518.png)\n\n### 限制\n1. **降低 GPU 利用率**：在一定点之后，每个 GPU 的批量大小变得太小，这会降低 GPU 的利用率，并增加通信成本。\n   \n2. **设备数限制**：可用于训练的 GPU 设备数量受到批量大小的限制，这限制了可以使用的最大设备数。\n\n\n## All-Reduce 操作\nAll-Reduce 是一种在并行计算和分布式计算中常用的通信操作，用于在多个计算节点（例如 GPU 或 CPU）之间汇总数据。\n\n### 工作原理\n1. **数据分发**：每个节点在完成了自己的计算后会生成一组梯度（或者其他需要汇总的数据），这些数据需要在所有节点之间进行汇总。\n   \n2. **数据汇总**：All-Reduce 操作将每个节点上的数据在所有节点之间进行相加或其他类型的归约操作（例如，求和、求平均、求最大值等）。这意味着最终每个节点都会得到相同的结果，该结果是所有节点数据的汇总。\n   \n3. **结果广播**：完成汇总操作后，汇总结果会被广播回每个节点，使得每个节点都持有相同的汇总数据。这一步确保了所有节点在下一步的计算中都能使用一致的数据。\n\n\n## 模型并行\n![Pasted image 20250429221654.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221654.png)\n\n![Pasted image 20250429221701.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221701.png)\n\n\n## Model Parallelism MP\n对于数据并行的限制，可以采用一些内存管理技术，比如激活检查点（Activation Checkpointing）。\n\n### Activation Checkpointing（gradient_checkpointing）\n激活检查点是一种在前向传播过程中计算节点的激活值并保存的方法。计算下一个节点完成后丢弃中间节点的激活值，反向传播时如果有保存下来的梯度就直接使用，如果没有就使用保存下来的前一个节点的梯度重新计算当前节点的梯度再使用。\n\n此外，还可以使用模型并行来划分模型的不同阶段，从而解决GPU内存容量和计算限制的问题，使得权重和关联的优化器状态不需要同时存储在一个GPU上。一个模型的内存和计算被分布在多个计算节点上，主要的方式分为流水线并行和张量并行。\n\n\n## 张量并行 Intra-Layer Tensor Parallelism\n有的tensor/layer很大，一张卡放不下，将tensor分割成多块，一张卡存一块。这也可以被理解为将大矩阵运算拆分成多个小矩阵运算，然后分布到不同的设备上进行计算。\n![Pasted image 20250429221727.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221727.png)\n\n### 代码块\n高度的模型并行会产生很多的小矩阵乘法(GEMMS)。对于一个GEMMs $Y = X A$，按照对权重矩阵$A$的分块方式，张量并行又可以分成行并行和列并行。神经网络中的线性层（Linear层），可以将其看作是将输入矩阵分块进行计算，然后将计算结果合并成输出矩阵。这个过程涉及矩阵乘法和加法操作，其中矩阵乘法涉及到权重矩阵和输入数据之间的乘法，然后再加上偏置向量；对于非线性层（例如激活函数层），通常不需要进行额外的设计。这些层的计算过程是基于输入数据应用某种非线性函数，例如ReLU（修正线性单元）、Sigmoid、Tanh等。这些函数在数学上是已知的，只需要将输入数据传递给这些函数，然后得到输出。整体来看，神经网络的计算可以被抽象为一系列的矩阵和向量操作，其中线性层涉及矩阵乘法和加法，而非线性层涉及特定的函数计算。这些操作在深度学习框架中会被高度优化，以提高计算效率和训练速度。\n\n```Python\n# 假设模型有三层：L0, L1, L2 每层有两个神经元 两张卡\nGPU0:\nL0 | L1 | L2\n---|----|---\na0 | b0 | c0\n\nGPU1:L0 | L1 | L2\n---|----|---\na1 | b1 | c1\n\n```\n\n\n### GEMMs行并行\n先分析Row Parallelism，就是把权重矩阵$A$按照行切分成两部分，对应的把$X$按照列切分成两部分：\n\n$$\nX A = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\begin{bmatrix} A_1 & A_2 \\end{bmatrix} = X_1 A_1 + X_2 A_2 = Y_1 + Y_2 = Y\n$$\n\n这样$X_1$, $A_1$可以在第一个GPU上计算，$X_2$, $A_2$可以在第二个GPU上进行计算。\n\n通过这种方式，可以有效地将大型模型的计算任务分配到多个GPU上，从而提高计算效率，同时解决单个GPU内存不足的问题。这种技术在处理大型深度学习模型时尤为重要，因为它能够显著提高训练速度，并允许更大规模的模型在有限硬件资源下进行训练。\n![Pasted image 20250429221755.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221755.png)![Pasted image 20250429221811.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221811.png)\n\n\n## GEMMs列并行与Transformer中的张量并行\n在现代深度学习模型中，尤其是Transformer架构中，矩阵计算的并行化是提升模型训练和推理速度的关键。本文将探讨GEMMs列并行和Transformer中的张量并行技术。\n\n\n## GEMMs列并行\nGEMMs列并行是一种将矩阵 $A$ 按照列来分割的技术。这种方法可以有效地将计算任务分配到多个GPU上，从而实现并行计算。\n\n![Pasted image 20250429221945.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221945.png)\n\n\n## Transformer中的张量并行\nTransformer架构包含了Self-Attention和Feed Forward Network，这些操作涉及大量的矩阵计算，非常适合在GPU上进行并行操作，以加速模型的训练和推理过程。\n![Pasted image 20250429221953.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429221953.png)\n\n### Masked Multi-Head Self Attention\nMasked Multi-Head Self Attention涉及到大量的矩阵乘法操作，这些操作可以被高效地并行执行，从而提高计算速度。\n\n\n### Feed Forward Neural Network\nFeed Forward Neural Network包含多个全连接层，每个全连接层都涉及矩阵乘法、激活函数（通常是GeLU）和可能的Dropout层。这些操作也是高度并行化的，可以在GPU上迅速执行。\n\nMegatron的FFN是一个两层MLP，第一层是维度从 $H$ 变成 $4H$，第二层是维度从 $4H$ 变回到 $H$。具体架构如下，紫色块对应于全连接层，每个蓝色块表示一个被复制 $N$ 次的transformer层，红色的 $x L$ 代表此蓝色复制 $L$ 次。\n\n张量并行就是要对Transformer进行切分，Megatron把Masked Multi-Head Self Attention和Feed Forward都进行切分以并行化，通过添加一些同步通信操作来创建一个简单的模型并行实现。\n\n\n## 切分MLP\n从MLP开始，MLP的第一部分是GEMM，后面是激活函数GeLU：\n\n$$\nY = \\text{GeLU} ( X A )\n$$\n\n### 对比按照行列切分权重的方法\n\n#### 行并行\n行并行的方法如下：\n\n$$\nY = \\text{GeLU} ( X_1 A_1 + X_2 A_2 )\n$$\n\n但是，由于GeLU是非线性计算：\n\n$$\n\\text{GeLU} ( X_1 A_1 + X_2 A_2 ) \\neq \\text{GeLU} ( X_1 A_1 ) + \\text{GeLU} ( X_2 A_2 )\n$$\n\n所以这种方案需要在GeLU函数之前加上一个同步点，这个同步点让不同GPU之间交换信息。\n\n\n#### 列并行\n列并行的方法如下：\n\n$$\n[ Y_1, Y_2 ] = [ \\text{GeLU} ( X A_1 ), \\text{GeLU} ( X A_2 ) ]\n$$\n\n这种方法不需要同步点，可以直接把两个GeLU的输出拼接起来，而不需要额外的通信（比如GeLU这里就不用all-reduce了）。在前向和后向传递的时候做一次all-reduce通信操作。对于第二个MLP使用行并行，可以直接与第一个MLP得到的 $Y_1$, $Y_2$ 对应计算，不需要额外通信操作。\n\n总的来说，在MLP层中，对 $A$ 采用“列切割”，对 $B$ 采用“行切割”。\n![Pasted image 20250429222007.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222007.png)\n\n在现代深度学习中，模型的规模和复杂度不断增加，如何高效地利用GPU资源成为了一个重要的研究课题。本文将探讨在神经网络计算中，如何通过合理的切分策略来减少GPU之间的通信量，以提高整体计算效率。\n\n\n## GELU计算中的行列切割策略\n在处理矩阵 $A$ 时，我们要尽量保证各GPU上的计算相互独立，从而减少通信量。对于 $A$ 来说，需要进行一次GELU的计算，而GELU是非线性的。这意味着，如果对 $A$ 采用行切割，我们必须在进行GELU前，进行一次AllReduce，这会产生额外的通信量。但如果对 $A$ 采用列切割，每块GPU就可以继续独立计算。一旦确认好 $A$ 进行列切割，那么相应地，矩阵 $B$ 就需要进行行切割。\n\n\n## 通信量分析![Pasted image 20250429222138.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222138.png)\n在MLP层中，forward和backward阶段各会产生一次AllReduce。AllReduce的过程分为两个阶段：Reduce-Scatter和All-Gather，每个阶段的通信量都相等。设每个阶段的通信量为 $\\Phi$，则一次AllReduce产生的总通信量为 $4\\Phi$。MLP层的总通信量为：\n\n$$\n4\\Phi\n$$\n\n其中：\n\n$$\n\\Phi = b \\ast s \\ast h\n$$\n\n\n## Self-Attention的切分策略\n![Pasted image 20250429222151.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222151.png)\n\n### 多头注意力并行切分\n利用多头注意力固有的并行特性，可以直接以列并行的方法切分QKV相关的GEMMs。首先考虑head=1的情况，self-attention的计算方式如下图所示（图略）。\n\n\n### 当num_heads = 2时的情况![Pasted image 20250429222201.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222201.png)\n对于每一块权重，我们沿着列方向维度切割一刀。此时每个 head 上的 $W_Q$, $W_K$, $W_V$ 的维度都变成 $(d_{model}, k_{dim}//2)$。每个 head 上单独做矩阵计算，最后将计算结果concat起来。整个流程如下（图略）。\n\n因此，我们可以把每个头的参数放到一块GPU上，也可以把多个head放到一块GPU上，但尽量保证head总数能被GPU个数整除。\n\n\n### 通信量分析\n使用 $N$ 来表示GPU的数量。有几块GPU，就把 $W$ 按行维度切成几份。self-attention 层在 forward 和 backward 中各做一次 AllReduce，总通信量也是：\n\n$$\n\\Phi\n$$\n\n其中：\n\n$$\n\\Phi = b \\ast s \\ast h\n$$\n\n\n## Embedding切分策略\n\n### 输入层embedding\n输入层embedding包括word embedding和positional embedding两部分。对于positional embedding，由于 $\\max_s$ 本身不会太长，因此每个GPU上都拷贝一份，对显存的压力不大。但对于word embedding，词表的大小较大，需要将word embedding拆分到各个GPU上。\n\n\n### 输出层embedding\n输出层只有一个word embedding。必须时刻保证输入层和输出层共用一套word embedding。在backward过程中，我们在输出层对word embedding计算一次梯度，在输入层中也会计算一次梯度。在用梯度更新word embedding权重时，必须用两次梯度的总和进行更新。\n\n\n## Cross-entropy切分的基本流程![Pasted image 20250429222312.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222312.png)\n首先，需要对输出层的$Y_1$和$Y_2$做一次All-Gather操作，把它们concat起来形成$Y$。对$Y$的每一行做softmax运算，就可以得到对于当前位置来说，每个词出现的概率。接着，再用此概率和真值组做cross-entropy即可。\n\n然而，All-Gather操作会产生额外的通讯量：\n\n$$\nb \\times s \\times v\n$$\n\n当词表$v$很大时，这个通讯开销也不容忽视。\n\n\n## 优化策略\n针对上述情况，可以采取以下优化策略：\n\n### GPU上的局部计算\n每块GPU上，我们可以先按行求和，得到各自GPU上的 $GPU\\_sum(e)$。\n\n\n### AllReduce操作\n将每块GPU上的结果做AllReduce操作，得到每行最终的\n\n$$\n\\sum(e)\n$$\n\n这就是softmax中的分母。此时的通讯量为：\n\n$$\nb \\times s\n$$\n\n\n### 计算局部Cross-entropy\n在每块GPU上，即可计算各自维护部分的\n\n$$\n\\frac{e}{\\sum(e)}\n$$\n\n将其与真值做cross-entropy，得到每行的loss，按行加总起来以后得到GPU上的 scalar Loss。\n\n\n### 汇总总Loss\n将GPU上的 scalar Loss 做AllReduce操作，得到总Loss。此时通讯量为$N$。\n\n通过上述优化，我们把原先的通讯量从\n\n$$\nb \\times s \\times v\n$$\n\n大大降至\n\n$$\nb \\times s + N\n$$\n\n\n## TransformerBlock中的通信优化![Pasted image 20250429222329.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222329.png)\n一个TransformerBlock需要在前向和后向总共需要4个all-reduce通信操作。\n\n\n## 张量并行与序列并行![Pasted image 20250429222343.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222343.png)\n张量并行对AttentionBlock和MLP都进行了并行化，但没有对LN和dropout进行并行化。不过后续在megatron3中用序列并行技术对这两部分进行了并行，这部分就不过多展开了。\n\n---\n\n\n\n# 流水线并行 Inter-Layer Pipeline Parallelism\n在深度学习模型的训练过程中，如何提高效率一直是研究的重点之一。流水线并行是一种将网络按层切分，划分成多组，并将每组放置在不同的设备上进行计算的方法。通过这种方式，可以在不同设备上并行执行不同的模型阶段，从而提高整体效率。\n\n## 代码块\n```Python\n# 假设模型有8层 两张卡 ​\n====================== ===================== ​\n| L0 | L1 | L2 | L3 | | L4 | L5 | L6 | L7 | ​\n====================== ===================== ​\nGPU0 GPU1 ​\n# 设想一下，当GPU0在进行（前向/后向）计算时，GPU1在干嘛？闲着 ​\n# 当GPU1在进行（前向/后向)计算时，GPU0在干嘛？闲着 ​\n# 为了防止”一卡工作，众卡围观“，实践中PP也会把batch数据分割成 ​\n# 多个micro-batch，流水线执行\n\n```\n\n目前主流的流水线并行方法包括Gpipe和PipeDream。与这两者相比，Megatron中的流水线并行实现略有不同，采用了Virtual Pipeline的方法。传统的流水线并行通常会在一个设备上放置几个模块，通过在计算和通信之间取得平衡来提高效率。然而，虚拟流水线则采取相反的策略。在设备数量不变的前提下，它将流水线阶段进一步细分，以承载更多的通信量，从而降低空闲时间的比率，以缩短每个步骤的执行时间。\n\n### Gpipe![Pasted image 20250429222504.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222504.png)\nGpipe方法将transformer层按层切分放到不同的设备上，并将一个batch切分成多个mini-batch。在前向传播时，每个mini-batch从第一个设备流向最后一个设备；在反向传播时，在最后一个设备上计算出梯度并更新对应层的参数，然后将梯度传递给前一个设备进行梯度计算和参数更新。这种方法的劣势之一是空泡率（流水线空闲时间）比较高。\n\n\n### PipeDream![Pasted image 20250429222515.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222515.png)\nPipeDream相对于Gpipe的改进主要体现在内存方面。虽然空泡时间和Gpipe一致，但通过合理安排前向和反向过程的顺序，在步骤中间的稳定阶段形成1前向1反向（1F1B）的模式。在这个阶段，每个设备上最少只需要保存一份micro-batch的激活值，最多需要保存流水线阶段数份激活值，从而可以有效降低空泡占比。\n\n\n### Virtual Pipeline![Pasted image 20250429222523.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222523.png)\n传统的pipeline并行通常会在一个设备上放置几个block，这是为了扩展效率考虑，在计算强度和通信强度中间取一个平衡。但Virtual Pipeline却反其道而行之，在设备数量不变的情况下，分出更多的pipeline阶段，以更多的通信量换取空泡比率降低，减小了每一步的执行时间。假设网络共有16层（编号0-15），4个设备，前述Gpipe和PipeDream是分成4个阶段，按编号0-3层放Device1，4-7层放Device2，以此类推。Virtual Pipeline则是减小切分粒度，以virtual_pipeline_stage=2为例，将0-1层放Device1, 2-3层放在Device2，...，6-7层放到Device4，8-9层继续放在Device1，10-11层放在Device2，...，14-15层放在Device4。在稳定的时候也是1F1B的形式。按照这种方式，设备之间的点对点通信次数（量）直接翻了virtual_pipeline_stage倍，但空泡比率降低了。\n![Pasted image 20250429222538.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222538.png)\n\n\n## 3D并行\n\n\n## 数据并行\n\n### 计算效率高， 实现简单\n数据并行是一种计算效率极高且实现简单的并行技术。在这种模式下，每个计算节点都保存完整的模型、梯度和优化器状态，因此显存效率不高。然而，当增加并行度时，单卡的计算量保持恒定，可以实现近乎完美的线性扩展。不过，规约梯度的通信开销与模型大小成正相关。\n\n\n## 张量并行\n\n### 因模型结构而异， 实现难度大\n张量并行的实现难度较大，因为它依赖于模型的具体结构。随着并行度增加，显存占用成比例地减少，这是减少单层神经网络中间激活的唯一方法。然而，频繁的通信限制了两个通信阶段之间的计算量，导致计算效率很低。\n\n\n## 流水线并行\n\n### 通信成本最低\n流水线并行具有最低的通信成本。虽然减少的显存与流水线并行度成正相关，但流水线并行不会减少每层中间激活的显存占用。其计算效率得益于成本更低的点对点（P2P）通信，通信量与流水线各个阶段边界的激活值大小成正相关。\n\n\n## 显存和通信效率比较\n- 显存效率：模型并行 > 流水线并行 > 数据并行\n- 通信效率：流水线并行 > 数据并行 > 模型并行\n\n\n## 3D并行技术的混合应用\n3D并行技术是混合数据并行（DP）、张量并行（TP）和流水线并行（PP）组成的。在实际应用中，可以通过四路张量并行、四路流水线并行、两路数据并行以及32个worker来实现高效的3D并行。\n![Pasted image 20250429222809.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222809.png)![Pasted image 20250429222815.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222815.png)![Pasted image 20250429222822.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222822.png)\n\n\n## 4D并行技术在LLaMA3中的应用\n在LLaMA3的技术报告中，他们还使用了4D并行技术。除了PP、TP和DP，还加上了CP（context parallel），进一步提高了模型训练的效率。\n![Pasted image 20250429222830.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222830.png)\n![Pasted image 20250429222839.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250429222839.png)\n\n---","excerpt":"","includedFiles":[],"tasklistId":0,"title":"流水线并行 Inter-Layer Pipeline Parallelism","headers":[{"level":2,"title":"Megatron-LM","slug":"megatron-lm","link":"#megatron-lm","children":[{"level":3,"title":"Megatron-LM 的优缺点","slug":"megatron-lm-的优缺点","link":"#megatron-lm-的优缺点","children":[]},{"level":3,"title":"Megatron-LM 的特点","slug":"megatron-lm-的特点","link":"#megatron-lm-的特点","children":[]},{"level":3,"title":"提醒","slug":"提醒","link":"#提醒","children":[]}]},{"level":2,"title":"大型模型训练的挑战","slug":"大型模型训练的挑战","link":"#大型模型训练的挑战","children":[{"level":3,"title":"显存限制","slug":"显存限制","link":"#显存限制","children":[]},{"level":3,"title":"计算挑战","slug":"计算挑战","link":"#计算挑战","children":[]},{"level":3,"title":"并行策略挑战","slug":"并行策略挑战","link":"#并行策略挑战","children":[]}]},{"level":2,"title":"数据并行","slug":"数据并行","link":"#数据并行","children":[{"level":3,"title":"限制","slug":"限制","link":"#限制","children":[]}]},{"level":2,"title":"All-Reduce 操作","slug":"all-reduce-操作","link":"#all-reduce-操作","children":[{"level":3,"title":"工作原理","slug":"工作原理","link":"#工作原理","children":[]}]},{"level":2,"title":"模型并行","slug":"模型并行","link":"#模型并行","children":[]},{"level":2,"title":"Model Parallelism MP","slug":"model-parallelism-mp","link":"#model-parallelism-mp","children":[{"level":3,"title":"Activation Checkpointing（gradient_checkpointing）","slug":"activation-checkpointing-gradient-checkpointing","link":"#activation-checkpointing-gradient-checkpointing","children":[]}]},{"level":2,"title":"张量并行 Intra-Layer Tensor Parallelism","slug":"张量并行-intra-layer-tensor-parallelism","link":"#张量并行-intra-layer-tensor-parallelism","children":[{"level":3,"title":"代码块","slug":"代码块","link":"#代码块","children":[]},{"level":3,"title":"GEMMs行并行","slug":"gemms行并行","link":"#gemms行并行","children":[]}]},{"level":2,"title":"GEMMs列并行与Transformer中的张量并行","slug":"gemms列并行与transformer中的张量并行","link":"#gemms列并行与transformer中的张量并行","children":[]},{"level":2,"title":"GEMMs列并行","slug":"gemms列并行","link":"#gemms列并行","children":[]},{"level":2,"title":"Transformer中的张量并行","slug":"transformer中的张量并行","link":"#transformer中的张量并行","children":[{"level":3,"title":"Masked Multi-Head Self Attention","slug":"masked-multi-head-self-attention","link":"#masked-multi-head-self-attention","children":[]},{"level":3,"title":"Feed Forward Neural Network","slug":"feed-forward-neural-network","link":"#feed-forward-neural-network","children":[]}]},{"level":2,"title":"切分MLP","slug":"切分mlp","link":"#切分mlp","children":[{"level":3,"title":"对比按照行列切分权重的方法","slug":"对比按照行列切分权重的方法","link":"#对比按照行列切分权重的方法","children":[]}]},{"level":2,"title":"GELU计算中的行列切割策略","slug":"gelu计算中的行列切割策略","link":"#gelu计算中的行列切割策略","children":[]},{"level":2,"title":"通信量分析","slug":"通信量分析","link":"#通信量分析","children":[]},{"level":2,"title":"Self-Attention的切分策略","slug":"self-attention的切分策略","link":"#self-attention的切分策略","children":[{"level":3,"title":"多头注意力并行切分","slug":"多头注意力并行切分","link":"#多头注意力并行切分","children":[]},{"level":3,"title":"当num_heads = 2时的情况","slug":"当num-heads-2时的情况","link":"#当num-heads-2时的情况","children":[]},{"level":3,"title":"通信量分析","slug":"通信量分析-1","link":"#通信量分析-1","children":[]}]},{"level":2,"title":"Embedding切分策略","slug":"embedding切分策略","link":"#embedding切分策略","children":[{"level":3,"title":"输入层embedding","slug":"输入层embedding","link":"#输入层embedding","children":[]},{"level":3,"title":"输出层embedding","slug":"输出层embedding","link":"#输出层embedding","children":[]}]},{"level":2,"title":"Cross-entropy切分的基本流程","slug":"cross-entropy切分的基本流程","link":"#cross-entropy切分的基本流程","children":[]},{"level":2,"title":"优化策略","slug":"优化策略","link":"#优化策略","children":[{"level":3,"title":"GPU上的局部计算","slug":"gpu上的局部计算","link":"#gpu上的局部计算","children":[]},{"level":3,"title":"AllReduce操作","slug":"allreduce操作","link":"#allreduce操作","children":[]},{"level":3,"title":"计算局部Cross-entropy","slug":"计算局部cross-entropy","link":"#计算局部cross-entropy","children":[]},{"level":3,"title":"汇总总Loss","slug":"汇总总loss","link":"#汇总总loss","children":[]}]},{"level":2,"title":"TransformerBlock中的通信优化","slug":"transformerblock中的通信优化","link":"#transformerblock中的通信优化","children":[]},{"level":2,"title":"张量并行与序列并行","slug":"张量并行与序列并行","link":"#张量并行与序列并行","children":[]},{"level":2,"title":"代码块","slug":"代码块-1","link":"#代码块-1","children":[{"level":3,"title":"Gpipe","slug":"gpipe","link":"#gpipe","children":[]},{"level":3,"title":"PipeDream","slug":"pipedream","link":"#pipedream","children":[]},{"level":3,"title":"Virtual Pipeline","slug":"virtual-pipeline","link":"#virtual-pipeline","children":[]}]},{"level":2,"title":"3D并行","slug":"_3d并行","link":"#_3d并行","children":[]},{"level":2,"title":"数据并行","slug":"数据并行-1","link":"#数据并行-1","children":[{"level":3,"title":"计算效率高， 实现简单","slug":"计算效率高-实现简单","link":"#计算效率高-实现简单","children":[]}]},{"level":2,"title":"张量并行","slug":"张量并行","link":"#张量并行","children":[{"level":3,"title":"因模型结构而异， 实现难度大","slug":"因模型结构而异-实现难度大","link":"#因模型结构而异-实现难度大","children":[]}]},{"level":2,"title":"流水线并行","slug":"流水线并行","link":"#流水线并行","children":[{"level":3,"title":"通信成本最低","slug":"通信成本最低","link":"#通信成本最低","children":[]}]},{"level":2,"title":"显存和通信效率比较","slug":"显存和通信效率比较","link":"#显存和通信效率比较","children":[]},{"level":2,"title":"3D并行技术的混合应用","slug":"_3d并行技术的混合应用","link":"#_3d并行技术的混合应用","children":[]},{"level":2,"title":"4D并行技术在LLaMA3中的应用","slug":"_4d并行技术在llama3中的应用","link":"#_4d并行技术在llama3中的应用","children":[]}]}}
