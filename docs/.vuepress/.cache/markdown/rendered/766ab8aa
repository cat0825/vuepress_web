{"content":"<p>元数据：</p>\n<ul>\n<li>分类：人工智能</li>\n<li>标签：ChatGLM2, 解码器, 人工智能, 模型训练, 自回归</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"模型结构概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构概述\"><span>模型结构概述</span></a></h2>\n<p>ChatGLM2-6B 采用了完全的 Decoder-only 架构，结合了以下技术特性：</p>\n<ul>\n<li><strong>RoPE</strong>（旋转位置编码）用于增强位置编码能力。</li>\n<li>激活函数从 <strong>Gelu</strong> 转换为 <strong>SwiGLU</strong>。</li>\n<li>使用 <strong>Pre-RMSnorm</strong> 规范化技术。</li>\n<li>多头注意力机制（MHA）优化为多查询注意力（MQA），提高推理效率。</li>\n<li>基于 <strong>FalshAttention</strong> 的上下文长度从 2K 增加到 32K，并在对话阶段采用 8K 上下文进行训练。</li>\n</ul>\n<h2 id=\"训练目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练目标\"><span>训练目标</span></a></h2>\n<p>ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.5 万亿中英标识符的预训练，并进行了人类偏好对齐训练。</p>\n<h2 id=\"解码器架构的选择\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#解码器架构的选择\"><span>解码器架构的选择</span></a></h2>\n<h3 id=\"为什么选择-decoder-only-架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#为什么选择-decoder-only-架构\"><span>为什么选择 Decoder-only 架构？</span></a></h3>\n<p>多轮对话中，使用 prefix-decoder 需要构造多个数据来训练，而 decoder-only 架构通过使用 casual mask 可以直接处理整个多轮对话数据。此改动还消除了二维编码的需要。</p>\n<p>💡 <strong>启发点</strong>：Decoder-only 架构展现了强大的自回归生成能力。</p>\n<h3 id=\"样本构建与损失计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#样本构建与损失计算\"><span>样本构建与损失计算</span></a></h3>\n<p>在处理多轮对话时，例如三轮对话 Q1A1，Q2A2，Q3A3：</p>\n<ul>\n<li>\n<p><strong>PrefixLM 需要构建的样本</strong>：</p>\n<ul>\n<li>Q1 -&gt; A1</li>\n<li>Q1A1Q2 -&gt; A2</li>\n<li>Q1A1Q2A2Q3 -&gt; A3</li>\n</ul>\n</li>\n<li>\n<p><strong>Decoder-only 样本构建与损失计算</strong>：</p>\n<ul>\n<li>样本构建：Q1 A1 Q2 A2 Q3 A3</li>\n<li>损失计算：只需计算 A1、A2 和 A3 部分</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在使用 prefix-decoder 架构时，容易造成数据冗余和训练复杂度增加。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 深入研究 SwiGLU 激活函数的优势。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探讨 RoPE 在其他模型中的应用。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 分析多查询注意力（MQA）的具体实现细节。</label></li>\n</ul>\n<blockquote>\n<p>原始出处：<a href=\"https://github.com/THUDM/ChatGLM2-6B\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/GLM系列/GLM2.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/GLM系列/GLM2.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/GLM系列/GLM2","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/GLM系列/GLM2/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-25T02:48:16.000Z","updated":"2025-04-25T02:50:09.000Z","title":"GLM2","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>元数据：</p>\n<ul>\n<li>分类：人工智能</li>\n<li>标签：ChatGLM2, 解码器, 人工智能, 模型训练, 自回归</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"模型结构概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构概述\"><span>模型结构概述</span></a></h2>\n<p>ChatGLM2-6B 采用了完全的 Decoder-only 架构，结合了以下技术特性：</p>\n<ul>\n<li><strong>RoPE</strong>（旋转位置编码）用于增强位置编码能力。</li>\n<li>激活函数从 <strong>Gelu</strong> 转换为 <strong>SwiGLU</strong>。</li>\n<li>使用 <strong>Pre-RMSnorm</strong> 规范化技术。</li>\n<li>多头注意力机制（MHA）优化为多查询注意力（MQA），提高推理效率。</li>\n<li>基于 <strong>FalshAttention</strong> 的上下文长度从 2K 增加到 32K，并在对话阶段采用 8K 上下文进行训练。</li>\n</ul>\n<h2 id=\"训练目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练目标\"><span>训练目标</span></a></h2>\n<p>ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.5 万亿中英标识符的预训练，并进行了人类偏好对齐训练。</p>\n<h2 id=\"解码器架构的选择\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#解码器架构的选择\"><span>解码器架构的选择</span></a></h2>\n<h3 id=\"为什么选择-decoder-only-架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#为什么选择-decoder-only-架构\"><span>为什么选择 Decoder-only 架构？</span></a></h3>\n<p>多轮对话中，使用 prefix-decoder 需要构造多个数据来训练，而 decoder-only 架构通过使用 casual mask 可以直接处理整个多轮对话数据。此改动还消除了二维编码的需要。</p>\n<p>💡 <strong>启发点</strong>：Decoder-only 架构展现了强大的自回归生成能力。</p>\n<h3 id=\"样本构建与损失计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#样本构建与损失计算\"><span>样本构建与损失计算</span></a></h3>\n<p>在处理多轮对话时，例如三轮对话 Q1A1，Q2A2，Q3A3：</p>\n<ul>\n<li>\n<p><strong>PrefixLM 需要构建的样本</strong>：</p>\n<ul>\n<li>Q1 -&gt; A1</li>\n<li>Q1A1Q2 -&gt; A2</li>\n<li>Q1A1Q2A2Q3 -&gt; A3</li>\n</ul>\n</li>\n<li>\n<p><strong>Decoder-only 样本构建与损失计算</strong>：</p>\n<ul>\n<li>样本构建：Q1 A1 Q2 A2 Q3 A3</li>\n<li>损失计算：只需计算 A1、A2 和 A3 部分</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在使用 prefix-decoder 架构时，容易造成数据冗余和训练复杂度增加。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 深入研究 SwiGLU 激活函数的优势。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探讨 RoPE 在其他模型中的应用。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 分析多查询注意力（MQA）的具体实现细节。</label></li>\n</ul>\n<blockquote>\n<p>原始出处：<a href=\"https://github.com/THUDM/ChatGLM2-6B\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n</blockquote>\n</template>","contentStripped":"<p>元数据：</p>\n<ul>\n<li>分类：人工智能</li>\n<li>标签：ChatGLM2, 解码器, 人工智能, 模型训练, 自回归</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"模型结构概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构概述\"><span>模型结构概述</span></a></h2>\n<p>ChatGLM2-6B 采用了完全的 Decoder-only 架构，结合了以下技术特性：</p>\n<ul>\n<li><strong>RoPE</strong>（旋转位置编码）用于增强位置编码能力。</li>\n<li>激活函数从 <strong>Gelu</strong> 转换为 <strong>SwiGLU</strong>。</li>\n<li>使用 <strong>Pre-RMSnorm</strong> 规范化技术。</li>\n<li>多头注意力机制（MHA）优化为多查询注意力（MQA），提高推理效率。</li>\n<li>基于 <strong>FalshAttention</strong> 的上下文长度从 2K 增加到 32K，并在对话阶段采用 8K 上下文进行训练。</li>\n</ul>\n<h2 id=\"训练目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练目标\"><span>训练目标</span></a></h2>\n<p>ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.5 万亿中英标识符的预训练，并进行了人类偏好对齐训练。</p>\n<h2 id=\"解码器架构的选择\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#解码器架构的选择\"><span>解码器架构的选择</span></a></h2>\n<h3 id=\"为什么选择-decoder-only-架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#为什么选择-decoder-only-架构\"><span>为什么选择 Decoder-only 架构？</span></a></h3>\n<p>多轮对话中，使用 prefix-decoder 需要构造多个数据来训练，而 decoder-only 架构通过使用 casual mask 可以直接处理整个多轮对话数据。此改动还消除了二维编码的需要。</p>\n<p>💡 <strong>启发点</strong>：Decoder-only 架构展现了强大的自回归生成能力。</p>\n<h3 id=\"样本构建与损失计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#样本构建与损失计算\"><span>样本构建与损失计算</span></a></h3>\n<p>在处理多轮对话时，例如三轮对话 Q1A1，Q2A2，Q3A3：</p>\n<ul>\n<li>\n<p><strong>PrefixLM 需要构建的样本</strong>：</p>\n<ul>\n<li>Q1 -&gt; A1</li>\n<li>Q1A1Q2 -&gt; A2</li>\n<li>Q1A1Q2A2Q3 -&gt; A3</li>\n</ul>\n</li>\n<li>\n<p><strong>Decoder-only 样本构建与损失计算</strong>：</p>\n<ul>\n<li>样本构建：Q1 A1 Q2 A2 Q3 A3</li>\n<li>损失计算：只需计算 A1、A2 和 A3 部分</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在使用 prefix-decoder 架构时，容易造成数据冗余和训练复杂度增加。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 深入研究 SwiGLU 激活函数的优势。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探讨 RoPE 在其他模型中的应用。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 分析多查询注意力（MQA）的具体实现细节。</label></li>\n</ul>\n<blockquote>\n<p>原始出处：<a href=\"https://github.com/THUDM/ChatGLM2-6B\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"元数据：\n\n- 分类：人工智能\n- 标签：ChatGLM2, 解码器, 人工智能, 模型训练, 自回归\n- 日期：2025年4月12日\n\n## 模型结构概述\nChatGLM2-6B 采用了完全的 Decoder-only 架构，结合了以下技术特性：\n- **RoPE**（旋转位置编码）用于增强位置编码能力。\n- 激活函数从 **Gelu** 转换为 **SwiGLU**。\n- 使用 **Pre-RMSnorm** 规范化技术。\n- 多头注意力机制（MHA）优化为多查询注意力（MQA），提高推理效率。\n- 基于 **FalshAttention** 的上下文长度从 2K 增加到 32K，并在对话阶段采用 8K 上下文进行训练。\n\n\n## 训练目标\nChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.5 万亿中英标识符的预训练，并进行了人类偏好对齐训练。\n\n\n## 解码器架构的选择\n\n### 为什么选择 Decoder-only 架构？\n多轮对话中，使用 prefix-decoder 需要构造多个数据来训练，而 decoder-only 架构通过使用 casual mask 可以直接处理整个多轮对话数据。此改动还消除了二维编码的需要。\n\n💡 **启发点**：Decoder-only 架构展现了强大的自回归生成能力。\n\n\n### 样本构建与损失计算\n在处理多轮对话时，例如三轮对话 Q1A1，Q2A2，Q3A3：\n\n- **PrefixLM 需要构建的样本**：\n  - Q1 -> A1\n  - Q1A1Q2 -> A2\n  - Q1A1Q2A2Q3 -> A3\n\n- **Decoder-only 样本构建与损失计算**：\n  - 样本构建：Q1 A1 Q2 A2 Q3 A3\n  - 损失计算：只需计算 A1、A2 和 A3 部分\n\n\n## 常见错误\n> ⚠ 在使用 prefix-decoder 架构时，容易造成数据冗余和训练复杂度增加。\n\n\n## 行动清单\n- [ ] 深入研究 SwiGLU 激活函数的优势。\n- [ ] 探讨 RoPE 在其他模型中的应用。\n- [ ] 分析多查询注意力（MQA）的具体实现细节。\n\n> 原始出处：[GitHub](https://github.com/THUDM/ChatGLM2-6B)","excerpt":"","includedFiles":[],"tasklistId":3,"title":"","headers":[{"level":2,"title":"模型结构概述","slug":"模型结构概述","link":"#模型结构概述","children":[]},{"level":2,"title":"训练目标","slug":"训练目标","link":"#训练目标","children":[]},{"level":2,"title":"解码器架构的选择","slug":"解码器架构的选择","link":"#解码器架构的选择","children":[{"level":3,"title":"为什么选择 Decoder-only 架构？","slug":"为什么选择-decoder-only-架构","link":"#为什么选择-decoder-only-架构","children":[]},{"level":3,"title":"样本构建与损失计算","slug":"样本构建与损失计算","link":"#样本构建与损失计算","children":[]}]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
