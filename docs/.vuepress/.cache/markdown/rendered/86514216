{"content":"<h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自动推断</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<ul>\n<li>深度学习</li>\n<li>强化学习</li>\n<li>人类反馈</li>\n</ul>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<hr>\n<h2 id=\"研究背景\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#研究背景\"><span>研究背景</span></a></h2>\n<p>深度学习中的人类偏好学习（RLHF）首次在2017年的论文《Deep Reinforcement Learning from Human Preferences》中被提出。最初的目的是解决复杂强化学习任务中环境奖励函数设计的问题。强化学习在许多任务中面临目标复杂、难以定义奖励函数的问题，导致难以将人类实际目标传达给智能体。不正确的、有偏的奖励函数会导致智能体过分利用奖励函数，产生reward hacking问题，即实际学到的行为与人类期望不符合，甚至有害。这种奖励函数的设计需要大量专业人士的精力，而现有方法如逆强化学习和模仿学习在处理复杂行为时存在局限性，直接使用人类反馈作为奖励函数成本过高。</p>\n<h2 id=\"研究目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#研究目标\"><span>研究目标</span></a></h2>\n<p>为了解决没有明确定义奖励函数的强化学习问题，需要满足以下几点：</p>\n<ol>\n<li>✅ 能够解决那些人类只能识别期望行为，但不一定能提供示范（demonstration）的任务。</li>\n<li>⚠ 允许非专家用户对智能体进行教导。</li>\n<li>❗ 能够扩展到大型问题。</li>\n<li>在用户反馈方面经济高效。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在设计奖励函数时，容易产生偏见或错误，导致智能体行为偏离期望。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>使用人类反馈作为奖励函数是一种创新，能够有效地传达人类的期望，即使在复杂任务中。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>调查现有RLHF技术的应用领域。</li>\n<li>评估RLHF在不同任务中的效果。</li>\n<li>研究如何降低人类反馈成本。</li>\n</ul>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索RLHF在其他领域的应用。</li>\n<li>开发更高效的用户反馈机制。</li>\n</ul>\n<blockquote>\n<p>引用: Deep Reinforcement Learning from Human Preferences, https://arxiv.org/pdf/1706.03741</p>\n</blockquote>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>[思考]</span></a></h2>\n<ol>\n<li>如何确保用户反馈的准确性和一致性？</li>\n<li>RLHF能否应用于其他机器学习领域？</li>\n<li>在没有专家参与的情况下，如何保证智能体的训练质量？</li>\n</ol>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RLHF流程.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RLHF流程.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RLHF流程","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RLHF流程/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-16T02:25:36.000Z","updated":"2025-04-16T02:27:06.000Z","title":"RLHF流程","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自动推断</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<ul>\n<li>深度学习</li>\n<li>强化学习</li>\n<li>人类反馈</li>\n</ul>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<hr>\n<h2 id=\"研究背景\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#研究背景\"><span>研究背景</span></a></h2>\n<p>深度学习中的人类偏好学习（RLHF）首次在2017年的论文《Deep Reinforcement Learning from Human Preferences》中被提出。最初的目的是解决复杂强化学习任务中环境奖励函数设计的问题。强化学习在许多任务中面临目标复杂、难以定义奖励函数的问题，导致难以将人类实际目标传达给智能体。不正确的、有偏的奖励函数会导致智能体过分利用奖励函数，产生reward hacking问题，即实际学到的行为与人类期望不符合，甚至有害。这种奖励函数的设计需要大量专业人士的精力，而现有方法如逆强化学习和模仿学习在处理复杂行为时存在局限性，直接使用人类反馈作为奖励函数成本过高。</p>\n<h2 id=\"研究目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#研究目标\"><span>研究目标</span></a></h2>\n<p>为了解决没有明确定义奖励函数的强化学习问题，需要满足以下几点：</p>\n<ol>\n<li>✅ 能够解决那些人类只能识别期望行为，但不一定能提供示范（demonstration）的任务。</li>\n<li>⚠ 允许非专家用户对智能体进行教导。</li>\n<li>❗ 能够扩展到大型问题。</li>\n<li>在用户反馈方面经济高效。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在设计奖励函数时，容易产生偏见或错误，导致智能体行为偏离期望。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>使用人类反馈作为奖励函数是一种创新，能够有效地传达人类的期望，即使在复杂任务中。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>调查现有RLHF技术的应用领域。</li>\n<li>评估RLHF在不同任务中的效果。</li>\n<li>研究如何降低人类反馈成本。</li>\n</ul>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索RLHF在其他领域的应用。</li>\n<li>开发更高效的用户反馈机制。</li>\n</ul>\n<blockquote>\n<p>引用: Deep Reinforcement Learning from Human Preferences, https://arxiv.org/pdf/1706.03741</p>\n</blockquote>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>[思考]</span></a></h2>\n<ol>\n<li>如何确保用户反馈的准确性和一致性？</li>\n<li>RLHF能否应用于其他机器学习领域？</li>\n<li>在没有专家参与的情况下，如何保证智能体的训练质量？</li>\n</ol>\n</template>","contentStripped":"<h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自动推断</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<ul>\n<li>深度学习</li>\n<li>强化学习</li>\n<li>人类反馈</li>\n</ul>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<hr>\n<h2 id=\"研究背景\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#研究背景\"><span>研究背景</span></a></h2>\n<p>深度学习中的人类偏好学习（RLHF）首次在2017年的论文《Deep Reinforcement Learning from Human Preferences》中被提出。最初的目的是解决复杂强化学习任务中环境奖励函数设计的问题。强化学习在许多任务中面临目标复杂、难以定义奖励函数的问题，导致难以将人类实际目标传达给智能体。不正确的、有偏的奖励函数会导致智能体过分利用奖励函数，产生reward hacking问题，即实际学到的行为与人类期望不符合，甚至有害。这种奖励函数的设计需要大量专业人士的精力，而现有方法如逆强化学习和模仿学习在处理复杂行为时存在局限性，直接使用人类反馈作为奖励函数成本过高。</p>\n<h2 id=\"研究目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#研究目标\"><span>研究目标</span></a></h2>\n<p>为了解决没有明确定义奖励函数的强化学习问题，需要满足以下几点：</p>\n<ol>\n<li>✅ 能够解决那些人类只能识别期望行为，但不一定能提供示范（demonstration）的任务。</li>\n<li>⚠ 允许非专家用户对智能体进行教导。</li>\n<li>❗ 能够扩展到大型问题。</li>\n<li>在用户反馈方面经济高效。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在设计奖励函数时，容易产生偏见或错误，导致智能体行为偏离期望。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>使用人类反馈作为奖励函数是一种创新，能够有效地传达人类的期望，即使在复杂任务中。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>调查现有RLHF技术的应用领域。</li>\n<li>评估RLHF在不同任务中的效果。</li>\n<li>研究如何降低人类反馈成本。</li>\n</ul>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索RLHF在其他领域的应用。</li>\n<li>开发更高效的用户反馈机制。</li>\n</ul>\n<blockquote>\n<p>引用: Deep Reinforcement Learning from Human Preferences, https://arxiv.org/pdf/1706.03741</p>\n</blockquote>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>[思考]</span></a></h2>\n<ol>\n<li>如何确保用户反馈的准确性和一致性？</li>\n<li>RLHF能否应用于其他机器学习领域？</li>\n<li>在没有专家参与的情况下，如何保证智能体的训练质量？</li>\n</ol>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 分类\n自动推断\n\n\n## 标签\n- 深度学习\n- 强化学习\n- 人类反馈\n\n\n## 日期\n2025年4月12日\n\n---\n\n\n## 研究背景\n深度学习中的人类偏好学习（RLHF）首次在2017年的论文《Deep Reinforcement Learning from Human Preferences》中被提出。最初的目的是解决复杂强化学习任务中环境奖励函数设计的问题。强化学习在许多任务中面临目标复杂、难以定义奖励函数的问题，导致难以将人类实际目标传达给智能体。不正确的、有偏的奖励函数会导致智能体过分利用奖励函数，产生reward hacking问题，即实际学到的行为与人类期望不符合，甚至有害。这种奖励函数的设计需要大量专业人士的精力，而现有方法如逆强化学习和模仿学习在处理复杂行为时存在局限性，直接使用人类反馈作为奖励函数成本过高。\n\n\n## 研究目标\n为了解决没有明确定义奖励函数的强化学习问题，需要满足以下几点：\n\n1. ✅ 能够解决那些人类只能识别期望行为，但不一定能提供示范（demonstration）的任务。\n2. ⚠ 允许非专家用户对智能体进行教导。\n3. ❗ 能够扩展到大型问题。\n4. 在用户反馈方面经济高效。\n\n\n## 常见错误\n> 在设计奖励函数时，容易产生偏见或错误，导致智能体行为偏离期望。\n\n\n## 💡启发点\n使用人类反馈作为奖励函数是一种创新，能够有效地传达人类的期望，即使在复杂任务中。\n\n\n## 行动清单\n- 调查现有RLHF技术的应用领域。\n- 评估RLHF在不同任务中的效果。\n- 研究如何降低人类反馈成本。\n\n\n## 后续追踪\n- 探索RLHF在其他领域的应用。\n- 开发更高效的用户反馈机制。\n\n> 引用: Deep Reinforcement Learning from Human Preferences, https://arxiv.org/pdf/1706.03741\n\n\n## [思考]\n1. 如何确保用户反馈的准确性和一致性？\n2. RLHF能否应用于其他机器学习领域？\n3. 在没有专家参与的情况下，如何保证智能体的训练质量？","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"分类","slug":"分类","link":"#分类","children":[]},{"level":2,"title":"标签","slug":"标签","link":"#标签","children":[]},{"level":2,"title":"日期","slug":"日期","link":"#日期","children":[]},{"level":2,"title":"研究背景","slug":"研究背景","link":"#研究背景","children":[]},{"level":2,"title":"研究目标","slug":"研究目标","link":"#研究目标","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]},{"level":2,"title":"[思考]","slug":"思考","link":"#思考","children":[]}]}}
