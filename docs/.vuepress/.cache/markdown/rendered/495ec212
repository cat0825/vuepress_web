{"content":"<p>元数据：</p>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：PPO, 强化学习, Token奖励</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"ppo训练中的关键技巧\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo训练中的关键技巧\"><span>PPO训练中的关键技巧</span></a></h2>\n<p>在PPO模型训练中，我们会采用一些技巧来优化模型的性能。这些技术包括Token级别的KL惩罚、广义优势估计（GAE）以及加入SFT损失等。这些方法不仅能够改善模型的训练效果，还能保留SFT模型的既有能力。</p>\n<h3 id=\"token-level-kl-penalty\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#token-level-kl-penalty\"><span>Token Level KL-Penalty</span></a></h3>\n<p>KL散度用于计算RL模型与SFT模型在每个token上的响应分布差异。这个散度在训练过程中作为奖励函数中的惩罚项被纳入，具体公式如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>I</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mo stretchy=\"false\">[</mo><mi>E</mi><mi>O</mi><mi>S</mi><mo stretchy=\"false\">]</mo><mo stretchy=\"false\">)</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>β</mi><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(s_t, a_t) = I(s_t = [EOS]) r(x, y) - \\beta KL(t)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">EOS</span><span class=\"mclose\">])</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>其中，KL(t)的计算公式为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>s</mi><mi>t</mi></msub><msub><mo stretchy=\"false\">)</mo><mrow><mi>R</mi><mi>L</mi></mrow></msub></mrow><mrow><msub><mi>π</mi><mrow><mi>S</mi><mi>F</mi><mi>T</mi></mrow></msub><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">KL(t) = \\log\\left(\\frac{\\pi_{\\theta}(a_t | s_t)_{RL}}{\\pi_{SFT}(a_t | s_t)}\\right)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">SFT</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span></span></span></span></span></p>\n<p>💡 启发点：使用KL散度作为惩罚项可以有效控制模型偏离预训练策略的程度。</p>\n<h3 id=\"generalized-advantage-estimation-gae\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#generalized-advantage-estimation-gae\"><span>Generalized Advantage Estimation (GAE)</span></a></h3>\n<p>GAE用于估计逐个token的奖励。通常情况下，我们设置 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span>，这样GAE方法就转变为蒙特卡洛估计方法。</p>\n<h3 id=\"adding-sft-loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#adding-sft-loss\"><span>Adding SFT Loss</span></a></h3>\n<p>在PPO训练中，我们可以加入额外的监督下一个token预测损失，以及KL散度，这样可以保留SFT模型的既有能力。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 计算每个token的KL散度并加入奖励函数。</li>\n<li>⚠ 设置GAE参数以优化奖励估计。</li>\n<li>❗ 在PPO中加入SFT损失以保留模型能力。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在计算KL散度时，确保使用正确的分布比例，否则可能导致训练过程不稳定。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何有效设置KL散度的惩罚系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>。</li>\n<li>实验不同GAE参数对模型性能的影响。</li>\n<li>探索其他可能的损失函数组合以优化PPO训练。</li>\n</ul>\n<blockquote>\n<p>来源：原始内容来自某技术文档或研究论文。</p>\n</blockquote>\n<h1 id=\"ppo优化与对齐税影响分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo优化与对齐税影响分析\"><span>PPO优化与对齐税影响分析</span></a></h1>\n<h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自动推断：机器学习</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<ul>\n<li>PPO</li>\n<li>对齐税</li>\n<li>强化学习</li>\n<li>NLP</li>\n</ul>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>在现代自然语言处理任务中，PPO（Proximal Policy Optimization）作为一种强化学习算法，常用于优化策略以对齐人类偏好。然而，PPO在优化过程中可能引发所谓的“对齐税”，即尽管对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。为解决这一问题，InstructGPT提出了PPO-ptx方法，通过增加预训练损失（ptx loss）来避免策略遗忘预训练阶段学习到的知识。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250416212538.png\" alt=\"Pasted image 20250416212538.png\"></p>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<ul>\n<li>PPO-ptx通过在PPO优化目标中增加预训练损失，以减轻对齐税的影响。</li>\n<li>KL Reward的系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> 的设置至关重要，需结合目标KL来确定。</li>\n<li>在强化学习中，奖励归一化和优势归一化技术有助于训练稳定性。</li>\n</ul>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"ppo-ptx优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo-ptx优化目标\"><span>PPO-ptx优化目标</span></a></h3>\n<p>PPO-ptx在原有的PPO优化目标基础上增加了预训练数据集上的优化目标，以避免策略遗忘预训练阶段的知识。这种方法旨在减轻对齐税，即虽然RLHF有助于对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。</p>\n<h3 id=\"kl-reward系数设置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#kl-reward系数设置\"><span>KL Reward系数设置</span></a></h3>\n<p>KL Reward中的系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> 非常重要，可以避免策略走得过远，导致过拟合和坍塌。通常需要通过实验确定模型表现较好的KL变化，然后根据目标KL来决定<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>的大小。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250416212547.png\" alt=\"Pasted image 20250416212547.png\"></p>\n<h3 id=\"预训练损失ptx-loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练损失ptx-loss\"><span>预训练损失PTX Loss</span></a></h3>\n<p>InstructGPT中将 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 设为27.8，但在实验中发现通常需要结合策略损失和预训练损失的大小综合设定。在实验中，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma &lt; 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7335em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span> 才能使模型比较好地收敛。</p>\n<h2 id=\"操作步骤-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤-1\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定PPO优化目标，包括加入预训练损失。</li>\n<li>⚠ 设定KL Reward系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> ，需结合目标KL进行实验。</li>\n<li>❗ 调整PTX Loss系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> ，确保模型收敛。</li>\n</ol>\n<h2 id=\"常见错误-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误-1\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在设定KL Reward系数时，不考虑目标KL的变化可能导致策略过拟合。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>PPO-ptx方法通过增加预训练损失有效减轻了对齐税的影响，为优化人类偏好提供了新的视角。</p>\n<h2 id=\"行动清单-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-1\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究更多关于对齐税的影响因素。</li>\n<li>测试不同参数设置下的PPO性能。</li>\n<li>探索其他可能的优化方法以进一步提高模型性能。</li>\n</ul>\n<h2 id=\"数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格示例\"><span>数据表格示例</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>设定值</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></td>\n<td>0.001</td>\n<td>KL散度系数</td>\n</tr>\n<tr>\n<td><span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span></td>\n<td>&lt;1</td>\n<td>PTX Loss系数</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原始出处：论文InstructGPT，Anthropic团队研究报告。</p>\n</blockquote>\n<h1 id=\"强化学习中的奖励利用与泛化问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#强化学习中的奖励利用与泛化问题\"><span>强化学习中的奖励利用与泛化问题</span></a></h1>\n<p>元数据：</p>\n<ul>\n<li>分类：自动推断</li>\n<li>标签：强化学习，奖励黑客，泛化问题，过拟合，测试集合</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点-1\"><span>核心观点</span></a></h2>\n<p>在强化学习（RL）的训练过程中，可能会出现训练集上的奖励（train reward）不断增长，但在测试集上的效果却下降的现象。这主要是由于奖励黑客（Reward hacking）和泛化问题（Generalization issue）导致的。</p>\n<h2 id=\"重点段落-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落-1\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>奖励黑客问题</strong>：\n当训练集上的奖励增长时，可能是因为奖励模型被“黑”了。这意味着虽然表面上看训练集的表现提高了，但实际上在人工评估时效果却下降。</p>\n</li>\n<li>\n<p><strong>泛化问题</strong>：\n如果训练集上的人工评估结果也在上涨，那么奖励黑客没有发生。然而，如果测试集上的效果下降，这表明模型可能过拟合了训练集，导致泛化问题。</p>\n</li>\n<li>\n<p><strong>解决方案</strong>：\n需要在模型训练过程中监控测试集的表现，确保模型不仅在训练集上表现良好，也能在未见过的数据上保持良好的性能。</p>\n</li>\n</ol>\n<h2 id=\"操作步骤-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤-2\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确保训练过程中监控测试集的表现。</li>\n<li>⚠ 注意奖励模型可能被“黑”的风险。</li>\n<li>❗ 识别和处理模型的过拟合现象。</li>\n</ol>\n<h2 id=\"常见错误-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误-2\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：过度依赖训练集上的表现而忽视测试集的效果可能导致模型无法泛化。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>在强化学习中，不仅要关注训练集的表现，还要特别注意测试集的效果，以避免过拟合和奖励黑客问题。</li>\n</ul>\n<h2 id=\"行动清单-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-2\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实施更严格的测试集监控机制。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 研究和应用更有效的防止过拟合的方法。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索新的奖励机制以减少奖励黑客的风险。</label></li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于对强化学习中奖励利用与泛化问题的分析和总结。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/PPO训练的trick和问题.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/PPO训练的trick和问题.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/PPO训练的trick和问题","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/PPO训练的trick和问题/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-16T13:24:00.000Z","updated":"2025-04-17T01:02:24.000Z","title":"PPO训练的trick和问题","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>元数据：</p>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：PPO, 强化学习, Token奖励</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"ppo训练中的关键技巧\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo训练中的关键技巧\"><span>PPO训练中的关键技巧</span></a></h2>\n<p>在PPO模型训练中，我们会采用一些技巧来优化模型的性能。这些技术包括Token级别的KL惩罚、广义优势估计（GAE）以及加入SFT损失等。这些方法不仅能够改善模型的训练效果，还能保留SFT模型的既有能力。</p>\n<h3 id=\"token-level-kl-penalty\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#token-level-kl-penalty\"><span>Token Level KL-Penalty</span></a></h3>\n<p>KL散度用于计算RL模型与SFT模型在每个token上的响应分布差异。这个散度在训练过程中作为奖励函数中的惩罚项被纳入，具体公式如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>I</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mo stretchy=\"false\">[</mo><mi>E</mi><mi>O</mi><mi>S</mi><mo stretchy=\"false\">]</mo><mo stretchy=\"false\">)</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>β</mi><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(s_t, a_t) = I(s_t = [EOS]) r(x, y) - \\beta KL(t)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">EOS</span><span class=\"mclose\">])</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>其中，KL(t)的计算公式为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>s</mi><mi>t</mi></msub><msub><mo stretchy=\"false\">)</mo><mrow><mi>R</mi><mi>L</mi></mrow></msub></mrow><mrow><msub><mi>π</mi><mrow><mi>S</mi><mi>F</mi><mi>T</mi></mrow></msub><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">KL(t) = \\log\\left(\\frac{\\pi_{\\theta}(a_t | s_t)_{RL}}{\\pi_{SFT}(a_t | s_t)}\\right)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">SFT</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span></span></span></span></span></p>\n<p>💡 启发点：使用KL散度作为惩罚项可以有效控制模型偏离预训练策略的程度。</p>\n<h3 id=\"generalized-advantage-estimation-gae\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#generalized-advantage-estimation-gae\"><span>Generalized Advantage Estimation (GAE)</span></a></h3>\n<p>GAE用于估计逐个token的奖励。通常情况下，我们设置 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span>，这样GAE方法就转变为蒙特卡洛估计方法。</p>\n<h3 id=\"adding-sft-loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#adding-sft-loss\"><span>Adding SFT Loss</span></a></h3>\n<p>在PPO训练中，我们可以加入额外的监督下一个token预测损失，以及KL散度，这样可以保留SFT模型的既有能力。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 计算每个token的KL散度并加入奖励函数。</li>\n<li>⚠ 设置GAE参数以优化奖励估计。</li>\n<li>❗ 在PPO中加入SFT损失以保留模型能力。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在计算KL散度时，确保使用正确的分布比例，否则可能导致训练过程不稳定。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何有效设置KL散度的惩罚系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>。</li>\n<li>实验不同GAE参数对模型性能的影响。</li>\n<li>探索其他可能的损失函数组合以优化PPO训练。</li>\n</ul>\n<blockquote>\n<p>来源：原始内容来自某技术文档或研究论文。</p>\n</blockquote>\n<h1 id=\"ppo优化与对齐税影响分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo优化与对齐税影响分析\"><span>PPO优化与对齐税影响分析</span></a></h1>\n<h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自动推断：机器学习</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<ul>\n<li>PPO</li>\n<li>对齐税</li>\n<li>强化学习</li>\n<li>NLP</li>\n</ul>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>在现代自然语言处理任务中，PPO（Proximal Policy Optimization）作为一种强化学习算法，常用于优化策略以对齐人类偏好。然而，PPO在优化过程中可能引发所谓的“对齐税”，即尽管对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。为解决这一问题，InstructGPT提出了PPO-ptx方法，通过增加预训练损失（ptx loss）来避免策略遗忘预训练阶段学习到的知识。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250416212538.png\" alt=\"Pasted image 20250416212538.png\"></p>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<ul>\n<li>PPO-ptx通过在PPO优化目标中增加预训练损失，以减轻对齐税的影响。</li>\n<li>KL Reward的系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> 的设置至关重要，需结合目标KL来确定。</li>\n<li>在强化学习中，奖励归一化和优势归一化技术有助于训练稳定性。</li>\n</ul>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"ppo-ptx优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo-ptx优化目标\"><span>PPO-ptx优化目标</span></a></h3>\n<p>PPO-ptx在原有的PPO优化目标基础上增加了预训练数据集上的优化目标，以避免策略遗忘预训练阶段的知识。这种方法旨在减轻对齐税，即虽然RLHF有助于对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。</p>\n<h3 id=\"kl-reward系数设置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#kl-reward系数设置\"><span>KL Reward系数设置</span></a></h3>\n<p>KL Reward中的系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> 非常重要，可以避免策略走得过远，导致过拟合和坍塌。通常需要通过实验确定模型表现较好的KL变化，然后根据目标KL来决定<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>的大小。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250416212547.png\" alt=\"Pasted image 20250416212547.png\"></p>\n<h3 id=\"预训练损失ptx-loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练损失ptx-loss\"><span>预训练损失PTX Loss</span></a></h3>\n<p>InstructGPT中将 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 设为27.8，但在实验中发现通常需要结合策略损失和预训练损失的大小综合设定。在实验中，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma &lt; 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7335em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span> 才能使模型比较好地收敛。</p>\n<h2 id=\"操作步骤-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤-1\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定PPO优化目标，包括加入预训练损失。</li>\n<li>⚠ 设定KL Reward系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> ，需结合目标KL进行实验。</li>\n<li>❗ 调整PTX Loss系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> ，确保模型收敛。</li>\n</ol>\n<h2 id=\"常见错误-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误-1\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在设定KL Reward系数时，不考虑目标KL的变化可能导致策略过拟合。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>PPO-ptx方法通过增加预训练损失有效减轻了对齐税的影响，为优化人类偏好提供了新的视角。</p>\n<h2 id=\"行动清单-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-1\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究更多关于对齐税的影响因素。</li>\n<li>测试不同参数设置下的PPO性能。</li>\n<li>探索其他可能的优化方法以进一步提高模型性能。</li>\n</ul>\n<h2 id=\"数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格示例\"><span>数据表格示例</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>设定值</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></td>\n<td>0.001</td>\n<td>KL散度系数</td>\n</tr>\n<tr>\n<td><span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span></td>\n<td>&lt;1</td>\n<td>PTX Loss系数</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原始出处：论文InstructGPT，Anthropic团队研究报告。</p>\n</blockquote>\n<h1 id=\"强化学习中的奖励利用与泛化问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#强化学习中的奖励利用与泛化问题\"><span>强化学习中的奖励利用与泛化问题</span></a></h1>\n<p>元数据：</p>\n<ul>\n<li>分类：自动推断</li>\n<li>标签：强化学习，奖励黑客，泛化问题，过拟合，测试集合</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点-1\"><span>核心观点</span></a></h2>\n<p>在强化学习（RL）的训练过程中，可能会出现训练集上的奖励（train reward）不断增长，但在测试集上的效果却下降的现象。这主要是由于奖励黑客（Reward hacking）和泛化问题（Generalization issue）导致的。</p>\n<h2 id=\"重点段落-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落-1\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>奖励黑客问题</strong>：\n当训练集上的奖励增长时，可能是因为奖励模型被“黑”了。这意味着虽然表面上看训练集的表现提高了，但实际上在人工评估时效果却下降。</p>\n</li>\n<li>\n<p><strong>泛化问题</strong>：\n如果训练集上的人工评估结果也在上涨，那么奖励黑客没有发生。然而，如果测试集上的效果下降，这表明模型可能过拟合了训练集，导致泛化问题。</p>\n</li>\n<li>\n<p><strong>解决方案</strong>：\n需要在模型训练过程中监控测试集的表现，确保模型不仅在训练集上表现良好，也能在未见过的数据上保持良好的性能。</p>\n</li>\n</ol>\n<h2 id=\"操作步骤-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤-2\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确保训练过程中监控测试集的表现。</li>\n<li>⚠ 注意奖励模型可能被“黑”的风险。</li>\n<li>❗ 识别和处理模型的过拟合现象。</li>\n</ol>\n<h2 id=\"常见错误-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误-2\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：过度依赖训练集上的表现而忽视测试集的效果可能导致模型无法泛化。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>在强化学习中，不仅要关注训练集的表现，还要特别注意测试集的效果，以避免过拟合和奖励黑客问题。</li>\n</ul>\n<h2 id=\"行动清单-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-2\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实施更严格的测试集监控机制。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 研究和应用更有效的防止过拟合的方法。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索新的奖励机制以减少奖励黑客的风险。</label></li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于对强化学习中奖励利用与泛化问题的分析和总结。</p>\n</blockquote>\n</template>","contentStripped":"<p>元数据：</p>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：PPO, 强化学习, Token奖励</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"ppo训练中的关键技巧\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo训练中的关键技巧\"><span>PPO训练中的关键技巧</span></a></h2>\n<p>在PPO模型训练中，我们会采用一些技巧来优化模型的性能。这些技术包括Token级别的KL惩罚、广义优势估计（GAE）以及加入SFT损失等。这些方法不仅能够改善模型的训练效果，还能保留SFT模型的既有能力。</p>\n<h3 id=\"token-level-kl-penalty\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#token-level-kl-penalty\"><span>Token Level KL-Penalty</span></a></h3>\n<p>KL散度用于计算RL模型与SFT模型在每个token上的响应分布差异。这个散度在训练过程中作为奖励函数中的惩罚项被纳入，具体公式如下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>I</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mo stretchy=\"false\">[</mo><mi>E</mi><mi>O</mi><mi>S</mi><mo stretchy=\"false\">]</mo><mo stretchy=\"false\">)</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>β</mi><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(s_t, a_t) = I(s_t = [EOS]) r(x, y) - \\beta KL(t)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">EOS</span><span class=\"mclose\">])</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>其中，KL(t)的计算公式为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>s</mi><mi>t</mi></msub><msub><mo stretchy=\"false\">)</mo><mrow><mi>R</mi><mi>L</mi></mrow></msub></mrow><mrow><msub><mi>π</mi><mrow><mi>S</mi><mi>F</mi><mi>T</mi></mrow></msub><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">KL(t) = \\log\\left(\\frac{\\pi_{\\theta}(a_t | s_t)_{RL}}{\\pi_{SFT}(a_t | s_t)}\\right)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">SFT</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span></span></span></span></span></p>\n<p>💡 启发点：使用KL散度作为惩罚项可以有效控制模型偏离预训练策略的程度。</p>\n<h3 id=\"generalized-advantage-estimation-gae\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#generalized-advantage-estimation-gae\"><span>Generalized Advantage Estimation (GAE)</span></a></h3>\n<p>GAE用于估计逐个token的奖励。通常情况下，我们设置 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span>，这样GAE方法就转变为蒙特卡洛估计方法。</p>\n<h3 id=\"adding-sft-loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#adding-sft-loss\"><span>Adding SFT Loss</span></a></h3>\n<p>在PPO训练中，我们可以加入额外的监督下一个token预测损失，以及KL散度，这样可以保留SFT模型的既有能力。</p>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 计算每个token的KL散度并加入奖励函数。</li>\n<li>⚠ 设置GAE参数以优化奖励估计。</li>\n<li>❗ 在PPO中加入SFT损失以保留模型能力。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在计算KL散度时，确保使用正确的分布比例，否则可能导致训练过程不稳定。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何有效设置KL散度的惩罚系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>。</li>\n<li>实验不同GAE参数对模型性能的影响。</li>\n<li>探索其他可能的损失函数组合以优化PPO训练。</li>\n</ul>\n<blockquote>\n<p>来源：原始内容来自某技术文档或研究论文。</p>\n</blockquote>\n<h1 id=\"ppo优化与对齐税影响分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo优化与对齐税影响分析\"><span>PPO优化与对齐税影响分析</span></a></h1>\n<h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自动推断：机器学习</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<ul>\n<li>PPO</li>\n<li>对齐税</li>\n<li>强化学习</li>\n<li>NLP</li>\n</ul>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>在现代自然语言处理任务中，PPO（Proximal Policy Optimization）作为一种强化学习算法，常用于优化策略以对齐人类偏好。然而，PPO在优化过程中可能引发所谓的“对齐税”，即尽管对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。为解决这一问题，InstructGPT提出了PPO-ptx方法，通过增加预训练损失（ptx loss）来避免策略遗忘预训练阶段学习到的知识。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250416212538.png\" alt=\"Pasted image 20250416212538.png\"></p>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<ul>\n<li>PPO-ptx通过在PPO优化目标中增加预训练损失，以减轻对齐税的影响。</li>\n<li>KL Reward的系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> 的设置至关重要，需结合目标KL来确定。</li>\n<li>在强化学习中，奖励归一化和优势归一化技术有助于训练稳定性。</li>\n</ul>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"ppo-ptx优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ppo-ptx优化目标\"><span>PPO-ptx优化目标</span></a></h3>\n<p>PPO-ptx在原有的PPO优化目标基础上增加了预训练数据集上的优化目标，以避免策略遗忘预训练阶段的知识。这种方法旨在减轻对齐税，即虽然RLHF有助于对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。</p>\n<h3 id=\"kl-reward系数设置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#kl-reward系数设置\"><span>KL Reward系数设置</span></a></h3>\n<p>KL Reward中的系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> 非常重要，可以避免策略走得过远，导致过拟合和坍塌。通常需要通过实验确定模型表现较好的KL变化，然后根据目标KL来决定<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>的大小。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250416212547.png\" alt=\"Pasted image 20250416212547.png\"></p>\n<h3 id=\"预训练损失ptx-loss\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练损失ptx-loss\"><span>预训练损失PTX Loss</span></a></h3>\n<p>InstructGPT中将 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 设为27.8，但在实验中发现通常需要结合策略损失和预训练损失的大小综合设定。在实验中，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma &lt; 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7335em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span> 才能使模型比较好地收敛。</p>\n<h2 id=\"操作步骤-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤-1\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定PPO优化目标，包括加入预训练损失。</li>\n<li>⚠ 设定KL Reward系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span> ，需结合目标KL进行实验。</li>\n<li>❗ 调整PTX Loss系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> ，确保模型收敛。</li>\n</ol>\n<h2 id=\"常见错误-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误-1\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在设定KL Reward系数时，不考虑目标KL的变化可能导致策略过拟合。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>PPO-ptx方法通过增加预训练损失有效减轻了对齐税的影响，为优化人类偏好提供了新的视角。</p>\n<h2 id=\"行动清单-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-1\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究更多关于对齐税的影响因素。</li>\n<li>测试不同参数设置下的PPO性能。</li>\n<li>探索其他可能的优化方法以进一步提高模型性能。</li>\n</ul>\n<h2 id=\"数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格示例\"><span>数据表格示例</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>设定值</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></td>\n<td>0.001</td>\n<td>KL散度系数</td>\n</tr>\n<tr>\n<td><span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span></td>\n<td>&lt;1</td>\n<td>PTX Loss系数</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原始出处：论文InstructGPT，Anthropic团队研究报告。</p>\n</blockquote>\n<h1 id=\"强化学习中的奖励利用与泛化问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#强化学习中的奖励利用与泛化问题\"><span>强化学习中的奖励利用与泛化问题</span></a></h1>\n<p>元数据：</p>\n<ul>\n<li>分类：自动推断</li>\n<li>标签：强化学习，奖励黑客，泛化问题，过拟合，测试集合</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点-1\"><span>核心观点</span></a></h2>\n<p>在强化学习（RL）的训练过程中，可能会出现训练集上的奖励（train reward）不断增长，但在测试集上的效果却下降的现象。这主要是由于奖励黑客（Reward hacking）和泛化问题（Generalization issue）导致的。</p>\n<h2 id=\"重点段落-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落-1\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>奖励黑客问题</strong>：\n当训练集上的奖励增长时，可能是因为奖励模型被“黑”了。这意味着虽然表面上看训练集的表现提高了，但实际上在人工评估时效果却下降。</p>\n</li>\n<li>\n<p><strong>泛化问题</strong>：\n如果训练集上的人工评估结果也在上涨，那么奖励黑客没有发生。然而，如果测试集上的效果下降，这表明模型可能过拟合了训练集，导致泛化问题。</p>\n</li>\n<li>\n<p><strong>解决方案</strong>：\n需要在模型训练过程中监控测试集的表现，确保模型不仅在训练集上表现良好，也能在未见过的数据上保持良好的性能。</p>\n</li>\n</ol>\n<h2 id=\"操作步骤-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤-2\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确保训练过程中监控测试集的表现。</li>\n<li>⚠ 注意奖励模型可能被“黑”的风险。</li>\n<li>❗ 识别和处理模型的过拟合现象。</li>\n</ol>\n<h2 id=\"常见错误-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误-2\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：过度依赖训练集上的表现而忽视测试集的效果可能导致模型无法泛化。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ul>\n<li>在强化学习中，不仅要关注训练集的表现，还要特别注意测试集的效果，以避免过拟合和奖励黑客问题。</li>\n</ul>\n<h2 id=\"行动清单-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-2\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实施更严格的测试集监控机制。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 研究和应用更有效的防止过拟合的方法。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索新的奖励机制以减少奖励黑客的风险。</label></li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于对强化学习中奖励利用与泛化问题的分析和总结。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"元数据：\n- 分类：机器学习\n- 标签：PPO, 强化学习, Token奖励\n- 日期：2025年4月12日\n\n## PPO训练中的关键技巧\n在PPO模型训练中，我们会采用一些技巧来优化模型的性能。这些技术包括Token级别的KL惩罚、广义优势估计（GAE）以及加入SFT损失等。这些方法不仅能够改善模型的训练效果，还能保留SFT模型的既有能力。\n\n### Token Level KL-Penalty\nKL散度用于计算RL模型与SFT模型在每个token上的响应分布差异。这个散度在训练过程中作为奖励函数中的惩罚项被纳入，具体公式如下：\n\n$$\nr(s_t, a_t) = I(s_t = [EOS]) r(x, y) - \\beta KL(t)\n$$\n\n其中，KL(t)的计算公式为：\n\n$$\nKL(t) = \\log\\left(\\frac{\\pi_{\\theta}(a_t | s_t)_{RL}}{\\pi_{SFT}(a_t | s_t)}\\right)\n$$\n\n💡 启发点：使用KL散度作为惩罚项可以有效控制模型偏离预训练策略的程度。\n\n\n### Generalized Advantage Estimation (GAE)\nGAE用于估计逐个token的奖励。通常情况下，我们设置 $\\lambda=1$，这样GAE方法就转变为蒙特卡洛估计方法。\n\n\n### Adding SFT Loss\n在PPO训练中，我们可以加入额外的监督下一个token预测损失，以及KL散度，这样可以保留SFT模型的既有能力。\n\n\n## 操作步骤\n1. ✅ 计算每个token的KL散度并加入奖励函数。\n2. ⚠ 设置GAE参数以优化奖励估计。\n3. ❗ 在PPO中加入SFT损失以保留模型能力。\n\n\n## 常见错误\n> 在计算KL散度时，确保使用正确的分布比例，否则可能导致训练过程不稳定。\n\n\n## 行动清单\n- 研究如何有效设置KL散度的惩罚系数 $\\beta$。\n- 实验不同GAE参数对模型性能的影响。\n- 探索其他可能的损失函数组合以优化PPO训练。\n\n> 来源：原始内容来自某技术文档或研究论文。\n\n\n\n# PPO优化与对齐税影响分析\n\n## 分类\n自动推断：机器学习\n\n\n## 标签\n- PPO\n- 对齐税\n- 强化学习\n- NLP\n\n\n## 日期\n2025年4月12日\n\n\n## 内容概述\n在现代自然语言处理任务中，PPO（Proximal Policy Optimization）作为一种强化学习算法，常用于优化策略以对齐人类偏好。然而，PPO在优化过程中可能引发所谓的“对齐税”，即尽管对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。为解决这一问题，InstructGPT提出了PPO-ptx方法，通过增加预训练损失（ptx loss）来避免策略遗忘预训练阶段学习到的知识。\n\n![Pasted image 20250416212538.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250416212538.png)\n\n\n## 核心观点\n- PPO-ptx通过在PPO优化目标中增加预训练损失，以减轻对齐税的影响。\n- KL Reward的系数 $\\beta$ 的设置至关重要，需结合目标KL来确定。\n- 在强化学习中，奖励归一化和优势归一化技术有助于训练稳定性。\n\n\n## 重点段落\n\n### PPO-ptx优化目标\nPPO-ptx在原有的PPO优化目标基础上增加了预训练数据集上的优化目标，以避免策略遗忘预训练阶段的知识。这种方法旨在减轻对齐税，即虽然RLHF有助于对齐人类偏好，但可能导致模型在某些NLP基准上的性能下降。\n\n\n### KL Reward系数设置\nKL Reward中的系数 $\\beta$ 非常重要，可以避免策略走得过远，导致过拟合和坍塌。通常需要通过实验确定模型表现较好的KL变化，然后根据目标KL来决定$\\beta$的大小。\n\n![Pasted image 20250416212547.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250416212547.png)\n\n\n### 预训练损失PTX Loss\nInstructGPT中将 $\\gamma$ 设为27.8，但在实验中发现通常需要结合策略损失和预训练损失的大小综合设定。在实验中，$\\gamma < 1$ 才能使模型比较好地收敛。\n\n\n## 操作步骤\n1. ✅ 确定PPO优化目标，包括加入预训练损失。\n2. ⚠ 设定KL Reward系数 $\\beta$ ，需结合目标KL进行实验。\n3. ❗ 调整PTX Loss系数 $\\gamma$ ，确保模型收敛。\n\n\n## 常见错误\n> 警告：在设定KL Reward系数时，不考虑目标KL的变化可能导致策略过拟合。\n\n\n## 💡启发点\nPPO-ptx方法通过增加预训练损失有效减轻了对齐税的影响，为优化人类偏好提供了新的视角。\n\n\n## 行动清单\n- 研究更多关于对齐税的影响因素。\n- 测试不同参数设置下的PPO性能。\n- 探索其他可能的优化方法以进一步提高模型性能。\n\n\n## 数据表格示例\n| 参数 | 设定值 | 备注 |\n|------|--------|------|\n| $\\beta$ | 0.001 | KL散度系数 |\n| $\\gamma$ | <1 | PTX Loss系数 |\n\n\n## 来源标注\n> 原始出处：论文InstructGPT，Anthropic团队研究报告。\n\n\n\n# 强化学习中的奖励利用与泛化问题\n元数据：\n\n- 分类：自动推断\n- 标签：强化学习，奖励黑客，泛化问题，过拟合，测试集合\n- 日期：2025年4月12日\n\n## 核心观点\n在强化学习（RL）的训练过程中，可能会出现训练集上的奖励（train reward）不断增长，但在测试集上的效果却下降的现象。这主要是由于奖励黑客（Reward hacking）和泛化问题（Generalization issue）导致的。\n\n\n## 重点段落\n1. **奖励黑客问题**：\n   当训练集上的奖励增长时，可能是因为奖励模型被“黑”了。这意味着虽然表面上看训练集的表现提高了，但实际上在人工评估时效果却下降。\n\n2. **泛化问题**：\n   如果训练集上的人工评估结果也在上涨，那么奖励黑客没有发生。然而，如果测试集上的效果下降，这表明模型可能过拟合了训练集，导致泛化问题。\n\n3. **解决方案**：\n   需要在模型训练过程中监控测试集的表现，确保模型不仅在训练集上表现良好，也能在未见过的数据上保持良好的性能。\n\n\n## 操作步骤\n1. ✅ 确保训练过程中监控测试集的表现。\n2. ⚠ 注意奖励模型可能被“黑”的风险。\n3. ❗ 识别和处理模型的过拟合现象。\n\n\n## 常见错误\n> 警告：过度依赖训练集上的表现而忽视测试集的效果可能导致模型无法泛化。\n\n\n## 💡 启发点\n- 在强化学习中，不仅要关注训练集的表现，还要特别注意测试集的效果，以避免过拟合和奖励黑客问题。\n\n\n## 行动清单\n- [ ] 实施更严格的测试集监控机制。\n- [ ] 研究和应用更有效的防止过拟合的方法。\n- [ ] 探索新的奖励机制以减少奖励黑客的风险。\n\n> 来源：本文内容基于对强化学习中奖励利用与泛化问题的分析和总结。","excerpt":"","includedFiles":[],"tasklistId":3,"title":"PPO优化与对齐税影响分析","headers":[{"level":2,"title":"PPO训练中的关键技巧","slug":"ppo训练中的关键技巧","link":"#ppo训练中的关键技巧","children":[{"level":3,"title":"Token Level KL-Penalty","slug":"token-level-kl-penalty","link":"#token-level-kl-penalty","children":[]},{"level":3,"title":"Generalized Advantage Estimation (GAE)","slug":"generalized-advantage-estimation-gae","link":"#generalized-advantage-estimation-gae","children":[]},{"level":3,"title":"Adding SFT Loss","slug":"adding-sft-loss","link":"#adding-sft-loss","children":[]}]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"分类","slug":"分类","link":"#分类","children":[]},{"level":2,"title":"标签","slug":"标签","link":"#标签","children":[]},{"level":2,"title":"日期","slug":"日期","link":"#日期","children":[]},{"level":2,"title":"内容概述","slug":"内容概述","link":"#内容概述","children":[]},{"level":2,"title":"核心观点","slug":"核心观点","link":"#核心观点","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[{"level":3,"title":"PPO-ptx优化目标","slug":"ppo-ptx优化目标","link":"#ppo-ptx优化目标","children":[]},{"level":3,"title":"KL Reward系数设置","slug":"kl-reward系数设置","link":"#kl-reward系数设置","children":[]},{"level":3,"title":"预训练损失PTX Loss","slug":"预训练损失ptx-loss","link":"#预训练损失ptx-loss","children":[]}]},{"level":2,"title":"操作步骤","slug":"操作步骤-1","link":"#操作步骤-1","children":[]},{"level":2,"title":"常见错误","slug":"常见错误-1","link":"#常见错误-1","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单-1","link":"#行动清单-1","children":[]},{"level":2,"title":"数据表格示例","slug":"数据表格示例","link":"#数据表格示例","children":[]},{"level":2,"title":"来源标注","slug":"来源标注","link":"#来源标注","children":[]},{"level":2,"title":"核心观点","slug":"核心观点-1","link":"#核心观点-1","children":[]},{"level":2,"title":"重点段落","slug":"重点段落-1","link":"#重点段落-1","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤-2","link":"#操作步骤-2","children":[]},{"level":2,"title":"常见错误","slug":"常见错误-2","link":"#常见错误-2","children":[]},{"level":2,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单-2","link":"#行动清单-2","children":[]}]}}
