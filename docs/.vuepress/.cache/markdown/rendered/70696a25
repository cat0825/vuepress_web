{"content":"<p><strong>分类</strong>：人工智能<br>\n<strong>标签</strong>：预训练、大模型、自监督学习、数据处理、Next Token Prediction<br>\n<strong>日期</strong>：2023-10-10</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>大模型的预训练旨在通过在大规模数据集上进行自监督学习，捕捉通用特征和模式，从而提升模型的适应性和泛化能力。预训练依赖于高质量、多领域的数据，使用Next Token Prediction（NTP）作为主要训练目标，帮助模型掌握语言和其他复杂技能。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"_1-预训练的定义与目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-预训练的定义与目标\"><span>1. <strong>预训练的定义与目标</strong></span></a></h3>\n<ul>\n<li><strong>目标</strong>：\n<ul>\n<li>通过在大规模数据集上学习，捕捉通用特征和模式。</li>\n<li>减少对标注数据的依赖，加速适应新任务。</li>\n</ul>\n</li>\n<li><strong>训练方法</strong>：\n<ul>\n<li>自监督学习（区别于无监督学习）。</li>\n<li>使用 <strong>Next Token Prediction (NTP)</strong> 作为训练目标，其公式为：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>n</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L = - \\sum_{n=1}^N \\log p(x_n | x_1, x_2, ..., x_{n-1}; \\theta)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.0954em;vertical-align:-1.2671em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.8829em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2671em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">...</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n<li>模型根据上下文预测下一个最可能的单词，并通过对数似然损失优化。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：NTP方法不仅适用于语言，还能扩展到代码、数学等领域的预测任务。</p>\n<hr>\n<h3 id=\"_2-预训练数据的来源与规模\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-预训练数据的来源与规模\"><span>2. <strong>预训练数据的来源与规模</strong></span></a></h3>\n<ul>\n<li><strong>数据量级</strong>：\n<ul>\n<li>初始预训练：约 10T tokens。</li>\n<li>进一步微调：至少 100B tokens。</li>\n</ul>\n</li>\n<li><strong>数据来源</strong>：\n<ul>\n<li><strong>Common Crawl</strong>：开放网页数据平台。</li>\n<li><strong>GitHub</strong>：代码相关数据。</li>\n<li><strong>电子书与教育资料</strong>：涵盖多领域知识。</li>\n<li><strong>内部数据</strong>：企业自有业务数据。</li>\n</ul>\n</li>\n<li><strong>多语种支持</strong>：\n<ul>\n<li>通用模型需覆盖中英文，小语种根据需求选择性收集。</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>数据来源</th>\n<th>特点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Common Crawl</td>\n<td>网页数据，覆盖广泛</td>\n</tr>\n<tr>\n<td>GitHub</td>\n<td>专注于代码与技术</td>\n</tr>\n<tr>\n<td>教育资料与论文</td>\n<td>提供高质量知识内容</td>\n</tr>\n<tr>\n<td>内部数据</td>\n<td>企业特定领域的专属语料</td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：未来，高质量多模态数据（如图像、视频与文本结合）将成为预训练的重要方向。</p>\n<hr>\n<h3 id=\"_3-数据处理的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-数据处理的挑战\"><span>3. <strong>数据处理的挑战</strong></span></a></h3>\n<ul>\n<li><strong>问题</strong>：\n<ul>\n<li>高质量数据（如论文、书籍）通常以PDF格式存在，解析复杂。</li>\n</ul>\n</li>\n<li><strong>解决方案</strong>：\n✅ 使用专业PDF解析服务，避免依赖低效的Python库。<br>\n✅ 训练OCR模型，前提是有足够高质量的PDF-文本对齐数据。<br>\n⚠ 注意：直接用大模型（如GPT-4）解析PDF可能成本过高。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：高效的数据处理工具是预训练成功的关键。</p>\n<hr>\n<h2 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h2>\n<blockquote>\n<p><strong>⚠ 警告</strong>：</p>\n<ul>\n<li>数据质量直接影响模型性能，低质量或偏向单一领域的数据可能导致模型在实际应用中表现不佳。</li>\n<li>多语种处理需确保语料分布均衡，否则可能影响小语种任务的表现。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何优化多模态数据（如图像与文本）的联合预训练方法？</li>\n<li>在多语种模型中，如何权衡不同语种的数据比例以提升性能？</li>\n<li>对于特定领域的垂直应用，是否需要重新设计预训练目标？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：本文内容基于大模型预训练技术文档整理与总结。</p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>✅ 调研现有的开源数据集（如FineWeb、Pile、RedPajama），并尝试整合到自己的项目中。</li>\n<li>✅ 学习如何使用专业PDF解析服务或OCR技术提升数据处理效率。</li>\n<li>❗ 探索多模态预训练方法，关注图像、文本、音频等多种形式的数据整合。</li>\n</ol>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>跟踪最新发布的大规模开源模型及其预训练技术进展。</li>\n<li>深入研究自监督学习在非语言任务（如代码生成、逻辑推理）中的应用潜力。</li>\n</ul>\n<h2 id=\"数据资源概览\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据资源概览\"><span>数据资源概览</span></a></h2>\n<p>以下是一些常用的开源数据集及其获取地址：</p>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Skywork/SkyPile-150B</td>\n<td><a href=\"https://huggingface.co/datasets/Skywork/SkyPile-150B\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>Wikipedia中文20230720</td>\n<td><a href=\"https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>C4</td>\n<td><a href=\"https://github.com/allenai/allennlp/discussions/5056\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>RedPajama</td>\n<td><a href=\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>EleutherAI/the_pile_deduplicated</td>\n<td><a href=\"https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>WuDaoCorporaText</td>\n<td><a href=\"https://data.baai.ac.cn/details/WuDaoCorporaText\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>PRM800K</td>\n<td><a href=\"https://github.com/openai/prm800k?tab=readme-ov-file\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>YeungNLP/firefly-pretrain-dataset</td>\n<td><a href=\"https://huggingface.co/datasets/YeungNLP/firefly-pretrain-dataset\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n</tbody>\n</table>\n<p>💡 <strong>启发点</strong>：这些数据集涵盖了多语言、多领域的语料，适合用于构建通用型预训练模型。</p>\n<hr>\n<h2 id=\"数据采样与分布策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据采样与分布策略\"><span>数据采样与分布策略</span></a></h2>\n<p>在继续预训练中，通用数据的采样策略对模型性能影响显著。以下是一个具体分配案例：</p>\n<h3 id=\"数据分布比例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据分布比例\"><span>数据分布比例</span></a></h3>\n<ul>\n<li><strong>总Tokens量</strong>：100B</li>\n<li><strong>语言分布</strong>：中文 : 英文 : 代码 = 20% : 70% : 10%</li>\n</ul>\n<h3 id=\"中文语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#中文语料采样\"><span>中文语料采样</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>Tokens数量 (单位：B)</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>cc</td>\n<td>4.024</td>\n<td></td>\n</tr>\n<tr>\n<td>baidu_baike_v3</td>\n<td>0.804</td>\n<td></td>\n</tr>\n<tr>\n<td>wiki_zw</td>\n<td>0.1602</td>\n<td></td>\n</tr>\n<tr>\n<td>qikan</td>\n<td>0.1602</td>\n<td></td>\n</tr>\n<tr>\n<td>recipe</td>\n<td>0.0182</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"英文语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#英文语料采样\"><span>英文语料采样</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>Tokens数量 (单位：B)</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c4</td>\n<td>32.675</td>\n<td></td>\n</tr>\n<tr>\n<td>arxiv_v2</td>\n<td>3.2652</td>\n<td></td>\n</tr>\n<tr>\n<td>wiki_en</td>\n<td>3.6792</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"code语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#code语料采样\"><span>Code语料采样</span></a></h3>\n<p>| 数据集名称          | Tokens数量 (单位：B) | 备注                      |\n|---------------------|---------------------------|\n| code               | 4.9042              |                          |\n| github70v1         | 1.2168              |                          |</p>\n<hr>\n<blockquote>\n<p>原始内容来源：<a href=\"https://huggingface.co/datasets/Skywork/SkyPile-150B\" target=\"_blank\" rel=\"noopener noreferrer\">Skywork/SkyPile-150B</a>、<a href=\"https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia中文20230720</a>、<a href=\"https://github.com/allenai/allennlp/discussions/5056\" target=\"_blank\" rel=\"noopener noreferrer\">C4</a> 等。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Pre-training 预训练/预训练定义以及数据来源 2.md","filePathRelative":"notes_bak/大语言模型学习/Pre-training 预训练/预训练定义以及数据来源 2.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/预训练定义以及数据来源","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/预训练定义以及数据来源/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-08T03:56:22.000Z","updated":"2025-04-13T05:06:02.000Z","title":"预训练定义以及数据来源","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p><strong>分类</strong>：人工智能<br>\n<strong>标签</strong>：预训练、大模型、自监督学习、数据处理、Next Token Prediction<br>\n<strong>日期</strong>：2023-10-10</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>大模型的预训练旨在通过在大规模数据集上进行自监督学习，捕捉通用特征和模式，从而提升模型的适应性和泛化能力。预训练依赖于高质量、多领域的数据，使用Next Token Prediction（NTP）作为主要训练目标，帮助模型掌握语言和其他复杂技能。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"_1-预训练的定义与目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-预训练的定义与目标\"><span>1. <strong>预训练的定义与目标</strong></span></a></h3>\n<ul>\n<li><strong>目标</strong>：\n<ul>\n<li>通过在大规模数据集上学习，捕捉通用特征和模式。</li>\n<li>减少对标注数据的依赖，加速适应新任务。</li>\n</ul>\n</li>\n<li><strong>训练方法</strong>：\n<ul>\n<li>自监督学习（区别于无监督学习）。</li>\n<li>使用 <strong>Next Token Prediction (NTP)</strong> 作为训练目标，其公式为：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>n</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L = - \\sum_{n=1}^N \\log p(x_n | x_1, x_2, ..., x_{n-1}; \\theta)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.0954em;vertical-align:-1.2671em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.8829em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2671em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">...</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n<li>模型根据上下文预测下一个最可能的单词，并通过对数似然损失优化。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：NTP方法不仅适用于语言，还能扩展到代码、数学等领域的预测任务。</p>\n<hr>\n<h3 id=\"_2-预训练数据的来源与规模\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-预训练数据的来源与规模\"><span>2. <strong>预训练数据的来源与规模</strong></span></a></h3>\n<ul>\n<li><strong>数据量级</strong>：\n<ul>\n<li>初始预训练：约 10T tokens。</li>\n<li>进一步微调：至少 100B tokens。</li>\n</ul>\n</li>\n<li><strong>数据来源</strong>：\n<ul>\n<li><strong>Common Crawl</strong>：开放网页数据平台。</li>\n<li><strong>GitHub</strong>：代码相关数据。</li>\n<li><strong>电子书与教育资料</strong>：涵盖多领域知识。</li>\n<li><strong>内部数据</strong>：企业自有业务数据。</li>\n</ul>\n</li>\n<li><strong>多语种支持</strong>：\n<ul>\n<li>通用模型需覆盖中英文，小语种根据需求选择性收集。</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>数据来源</th>\n<th>特点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Common Crawl</td>\n<td>网页数据，覆盖广泛</td>\n</tr>\n<tr>\n<td>GitHub</td>\n<td>专注于代码与技术</td>\n</tr>\n<tr>\n<td>教育资料与论文</td>\n<td>提供高质量知识内容</td>\n</tr>\n<tr>\n<td>内部数据</td>\n<td>企业特定领域的专属语料</td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：未来，高质量多模态数据（如图像、视频与文本结合）将成为预训练的重要方向。</p>\n<hr>\n<h3 id=\"_3-数据处理的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-数据处理的挑战\"><span>3. <strong>数据处理的挑战</strong></span></a></h3>\n<ul>\n<li><strong>问题</strong>：\n<ul>\n<li>高质量数据（如论文、书籍）通常以PDF格式存在，解析复杂。</li>\n</ul>\n</li>\n<li><strong>解决方案</strong>：\n✅ 使用专业PDF解析服务，避免依赖低效的Python库。<br>\n✅ 训练OCR模型，前提是有足够高质量的PDF-文本对齐数据。<br>\n⚠ 注意：直接用大模型（如GPT-4）解析PDF可能成本过高。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：高效的数据处理工具是预训练成功的关键。</p>\n<hr>\n<h2 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h2>\n<blockquote>\n<p><strong>⚠ 警告</strong>：</p>\n<ul>\n<li>数据质量直接影响模型性能，低质量或偏向单一领域的数据可能导致模型在实际应用中表现不佳。</li>\n<li>多语种处理需确保语料分布均衡，否则可能影响小语种任务的表现。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何优化多模态数据（如图像与文本）的联合预训练方法？</li>\n<li>在多语种模型中，如何权衡不同语种的数据比例以提升性能？</li>\n<li>对于特定领域的垂直应用，是否需要重新设计预训练目标？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：本文内容基于大模型预训练技术文档整理与总结。</p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>✅ 调研现有的开源数据集（如FineWeb、Pile、RedPajama），并尝试整合到自己的项目中。</li>\n<li>✅ 学习如何使用专业PDF解析服务或OCR技术提升数据处理效率。</li>\n<li>❗ 探索多模态预训练方法，关注图像、文本、音频等多种形式的数据整合。</li>\n</ol>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>跟踪最新发布的大规模开源模型及其预训练技术进展。</li>\n<li>深入研究自监督学习在非语言任务（如代码生成、逻辑推理）中的应用潜力。</li>\n</ul>\n<h2 id=\"数据资源概览\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据资源概览\"><span>数据资源概览</span></a></h2>\n<p>以下是一些常用的开源数据集及其获取地址：</p>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Skywork/SkyPile-150B</td>\n<td><a href=\"https://huggingface.co/datasets/Skywork/SkyPile-150B\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>Wikipedia中文20230720</td>\n<td><a href=\"https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>C4</td>\n<td><a href=\"https://github.com/allenai/allennlp/discussions/5056\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>RedPajama</td>\n<td><a href=\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>EleutherAI/the_pile_deduplicated</td>\n<td><a href=\"https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>WuDaoCorporaText</td>\n<td><a href=\"https://data.baai.ac.cn/details/WuDaoCorporaText\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>PRM800K</td>\n<td><a href=\"https://github.com/openai/prm800k?tab=readme-ov-file\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>YeungNLP/firefly-pretrain-dataset</td>\n<td><a href=\"https://huggingface.co/datasets/YeungNLP/firefly-pretrain-dataset\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n</tbody>\n</table>\n<p>💡 <strong>启发点</strong>：这些数据集涵盖了多语言、多领域的语料，适合用于构建通用型预训练模型。</p>\n<hr>\n<h2 id=\"数据采样与分布策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据采样与分布策略\"><span>数据采样与分布策略</span></a></h2>\n<p>在继续预训练中，通用数据的采样策略对模型性能影响显著。以下是一个具体分配案例：</p>\n<h3 id=\"数据分布比例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据分布比例\"><span>数据分布比例</span></a></h3>\n<ul>\n<li><strong>总Tokens量</strong>：100B</li>\n<li><strong>语言分布</strong>：中文 : 英文 : 代码 = 20% : 70% : 10%</li>\n</ul>\n<h3 id=\"中文语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#中文语料采样\"><span>中文语料采样</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>Tokens数量 (单位：B)</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>cc</td>\n<td>4.024</td>\n<td></td>\n</tr>\n<tr>\n<td>baidu_baike_v3</td>\n<td>0.804</td>\n<td></td>\n</tr>\n<tr>\n<td>wiki_zw</td>\n<td>0.1602</td>\n<td></td>\n</tr>\n<tr>\n<td>qikan</td>\n<td>0.1602</td>\n<td></td>\n</tr>\n<tr>\n<td>recipe</td>\n<td>0.0182</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"英文语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#英文语料采样\"><span>英文语料采样</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>Tokens数量 (单位：B)</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c4</td>\n<td>32.675</td>\n<td></td>\n</tr>\n<tr>\n<td>arxiv_v2</td>\n<td>3.2652</td>\n<td></td>\n</tr>\n<tr>\n<td>wiki_en</td>\n<td>3.6792</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"code语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#code语料采样\"><span>Code语料采样</span></a></h3>\n<p>| 数据集名称          | Tokens数量 (单位：B) | 备注                      |\n|---------------------|---------------------------|\n| code               | 4.9042              |                          |\n| github70v1         | 1.2168              |                          |</p>\n<hr>\n<blockquote>\n<p>原始内容来源：<a href=\"https://huggingface.co/datasets/Skywork/SkyPile-150B\" target=\"_blank\" rel=\"noopener noreferrer\">Skywork/SkyPile-150B</a>、<a href=\"https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia中文20230720</a>、<a href=\"https://github.com/allenai/allennlp/discussions/5056\" target=\"_blank\" rel=\"noopener noreferrer\">C4</a> 等。</p>\n</blockquote>\n</template>","contentStripped":"<p><strong>分类</strong>：人工智能<br>\n<strong>标签</strong>：预训练、大模型、自监督学习、数据处理、Next Token Prediction<br>\n<strong>日期</strong>：2023-10-10</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>大模型的预训练旨在通过在大规模数据集上进行自监督学习，捕捉通用特征和模式，从而提升模型的适应性和泛化能力。预训练依赖于高质量、多领域的数据，使用Next Token Prediction（NTP）作为主要训练目标，帮助模型掌握语言和其他复杂技能。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"_1-预训练的定义与目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-预训练的定义与目标\"><span>1. <strong>预训练的定义与目标</strong></span></a></h3>\n<ul>\n<li><strong>目标</strong>：\n<ul>\n<li>通过在大规模数据集上学习，捕捉通用特征和模式。</li>\n<li>减少对标注数据的依赖，加速适应新任务。</li>\n</ul>\n</li>\n<li><strong>训练方法</strong>：\n<ul>\n<li>自监督学习（区别于无监督学习）。</li>\n<li>使用 <strong>Next Token Prediction (NTP)</strong> 作为训练目标，其公式为：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>n</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L = - \\sum_{n=1}^N \\log p(x_n | x_1, x_2, ..., x_{n-1}; \\theta)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.0954em;vertical-align:-1.2671em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.8829em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2671em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">...</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n<li>模型根据上下文预测下一个最可能的单词，并通过对数似然损失优化。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：NTP方法不仅适用于语言，还能扩展到代码、数学等领域的预测任务。</p>\n<hr>\n<h3 id=\"_2-预训练数据的来源与规模\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-预训练数据的来源与规模\"><span>2. <strong>预训练数据的来源与规模</strong></span></a></h3>\n<ul>\n<li><strong>数据量级</strong>：\n<ul>\n<li>初始预训练：约 10T tokens。</li>\n<li>进一步微调：至少 100B tokens。</li>\n</ul>\n</li>\n<li><strong>数据来源</strong>：\n<ul>\n<li><strong>Common Crawl</strong>：开放网页数据平台。</li>\n<li><strong>GitHub</strong>：代码相关数据。</li>\n<li><strong>电子书与教育资料</strong>：涵盖多领域知识。</li>\n<li><strong>内部数据</strong>：企业自有业务数据。</li>\n</ul>\n</li>\n<li><strong>多语种支持</strong>：\n<ul>\n<li>通用模型需覆盖中英文，小语种根据需求选择性收集。</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>数据来源</th>\n<th>特点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Common Crawl</td>\n<td>网页数据，覆盖广泛</td>\n</tr>\n<tr>\n<td>GitHub</td>\n<td>专注于代码与技术</td>\n</tr>\n<tr>\n<td>教育资料与论文</td>\n<td>提供高质量知识内容</td>\n</tr>\n<tr>\n<td>内部数据</td>\n<td>企业特定领域的专属语料</td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：未来，高质量多模态数据（如图像、视频与文本结合）将成为预训练的重要方向。</p>\n<hr>\n<h3 id=\"_3-数据处理的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-数据处理的挑战\"><span>3. <strong>数据处理的挑战</strong></span></a></h3>\n<ul>\n<li><strong>问题</strong>：\n<ul>\n<li>高质量数据（如论文、书籍）通常以PDF格式存在，解析复杂。</li>\n</ul>\n</li>\n<li><strong>解决方案</strong>：\n✅ 使用专业PDF解析服务，避免依赖低效的Python库。<br>\n✅ 训练OCR模型，前提是有足够高质量的PDF-文本对齐数据。<br>\n⚠ 注意：直接用大模型（如GPT-4）解析PDF可能成本过高。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：高效的数据处理工具是预训练成功的关键。</p>\n<hr>\n<h2 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h2>\n<blockquote>\n<p><strong>⚠ 警告</strong>：</p>\n<ul>\n<li>数据质量直接影响模型性能，低质量或偏向单一领域的数据可能导致模型在实际应用中表现不佳。</li>\n<li>多语种处理需确保语料分布均衡，否则可能影响小语种任务的表现。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何优化多模态数据（如图像与文本）的联合预训练方法？</li>\n<li>在多语种模型中，如何权衡不同语种的数据比例以提升性能？</li>\n<li>对于特定领域的垂直应用，是否需要重新设计预训练目标？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：本文内容基于大模型预训练技术文档整理与总结。</p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>✅ 调研现有的开源数据集（如FineWeb、Pile、RedPajama），并尝试整合到自己的项目中。</li>\n<li>✅ 学习如何使用专业PDF解析服务或OCR技术提升数据处理效率。</li>\n<li>❗ 探索多模态预训练方法，关注图像、文本、音频等多种形式的数据整合。</li>\n</ol>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>跟踪最新发布的大规模开源模型及其预训练技术进展。</li>\n<li>深入研究自监督学习在非语言任务（如代码生成、逻辑推理）中的应用潜力。</li>\n</ul>\n<h2 id=\"数据资源概览\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据资源概览\"><span>数据资源概览</span></a></h2>\n<p>以下是一些常用的开源数据集及其获取地址：</p>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Skywork/SkyPile-150B</td>\n<td><a href=\"https://huggingface.co/datasets/Skywork/SkyPile-150B\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>Wikipedia中文20230720</td>\n<td><a href=\"https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>C4</td>\n<td><a href=\"https://github.com/allenai/allennlp/discussions/5056\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>RedPajama</td>\n<td><a href=\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>EleutherAI/the_pile_deduplicated</td>\n<td><a href=\"https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>WuDaoCorporaText</td>\n<td><a href=\"https://data.baai.ac.cn/details/WuDaoCorporaText\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>PRM800K</td>\n<td><a href=\"https://github.com/openai/prm800k?tab=readme-ov-file\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n<tr>\n<td>YeungNLP/firefly-pretrain-dataset</td>\n<td><a href=\"https://huggingface.co/datasets/YeungNLP/firefly-pretrain-dataset\" target=\"_blank\" rel=\"noopener noreferrer\">点击访问</a></td>\n</tr>\n</tbody>\n</table>\n<p>💡 <strong>启发点</strong>：这些数据集涵盖了多语言、多领域的语料，适合用于构建通用型预训练模型。</p>\n<hr>\n<h2 id=\"数据采样与分布策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据采样与分布策略\"><span>数据采样与分布策略</span></a></h2>\n<p>在继续预训练中，通用数据的采样策略对模型性能影响显著。以下是一个具体分配案例：</p>\n<h3 id=\"数据分布比例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据分布比例\"><span>数据分布比例</span></a></h3>\n<ul>\n<li><strong>总Tokens量</strong>：100B</li>\n<li><strong>语言分布</strong>：中文 : 英文 : 代码 = 20% : 70% : 10%</li>\n</ul>\n<h3 id=\"中文语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#中文语料采样\"><span>中文语料采样</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>Tokens数量 (单位：B)</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>cc</td>\n<td>4.024</td>\n<td></td>\n</tr>\n<tr>\n<td>baidu_baike_v3</td>\n<td>0.804</td>\n<td></td>\n</tr>\n<tr>\n<td>wiki_zw</td>\n<td>0.1602</td>\n<td></td>\n</tr>\n<tr>\n<td>qikan</td>\n<td>0.1602</td>\n<td></td>\n</tr>\n<tr>\n<td>recipe</td>\n<td>0.0182</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"英文语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#英文语料采样\"><span>英文语料采样</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>数据集名称</th>\n<th>Tokens数量 (单位：B)</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c4</td>\n<td>32.675</td>\n<td></td>\n</tr>\n<tr>\n<td>arxiv_v2</td>\n<td>3.2652</td>\n<td></td>\n</tr>\n<tr>\n<td>wiki_en</td>\n<td>3.6792</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"code语料采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#code语料采样\"><span>Code语料采样</span></a></h3>\n<p>| 数据集名称          | Tokens数量 (单位：B) | 备注                      |\n|---------------------|---------------------------|\n| code               | 4.9042              |                          |\n| github70v1         | 1.2168              |                          |</p>\n<hr>\n<blockquote>\n<p>原始内容来源：<a href=\"https://huggingface.co/datasets/Skywork/SkyPile-150B\" target=\"_blank\" rel=\"noopener noreferrer\">Skywork/SkyPile-150B</a>、<a href=\"https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia中文20230720</a>、<a href=\"https://github.com/allenai/allennlp/discussions/5056\" target=\"_blank\" rel=\"noopener noreferrer\">C4</a> 等。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"**分类**：人工智能  \n**标签**：预训练、大模型、自监督学习、数据处理、Next Token Prediction  \n**日期**：2023-10-10  \n\n---\n\n\n\n## 核心观点总结\n大模型的预训练旨在通过在大规模数据集上进行自监督学习，捕捉通用特征和模式，从而提升模型的适应性和泛化能力。预训练依赖于高质量、多领域的数据，使用Next Token Prediction（NTP）作为主要训练目标，帮助模型掌握语言和其他复杂技能。\n\n---\n\n\n\n## 重点内容解析\n\n### 1. **预训练的定义与目标**\n- **目标**：\n  - 通过在大规模数据集上学习，捕捉通用特征和模式。\n  - 减少对标注数据的依赖，加速适应新任务。\n- **训练方法**：\n  - 自监督学习（区别于无监督学习）。\n  - 使用 **Next Token Prediction (NTP)** 作为训练目标，其公式为：\n    $$\n    L = - \\sum_{n=1}^N \\log p(x_n | x_1, x_2, ..., x_{n-1}; \\theta)\n    $$\n  - 模型根据上下文预测下一个最可能的单词，并通过对数似然损失优化。\n\n💡 **启发点**：NTP方法不仅适用于语言，还能扩展到代码、数学等领域的预测任务。\n\n---\n\n\n### 2. **预训练数据的来源与规模**\n- **数据量级**：\n  - 初始预训练：约 10T tokens。\n  - 进一步微调：至少 100B tokens。\n- **数据来源**：\n  - **Common Crawl**：开放网页数据平台。\n  - **GitHub**：代码相关数据。\n  - **电子书与教育资料**：涵盖多领域知识。\n  - **内部数据**：企业自有业务数据。\n- **多语种支持**：\n  - 通用模型需覆盖中英文，小语种根据需求选择性收集。\n\n| 数据来源         | 特点                           |\n|------------------|--------------------------------|\n| Common Crawl     | 网页数据，覆盖广泛             |\n| GitHub           | 专注于代码与技术               |\n| 教育资料与论文   | 提供高质量知识内容             |\n| 内部数据         | 企业特定领域的专属语料         |\n\n📈 **趋势预测**：未来，高质量多模态数据（如图像、视频与文本结合）将成为预训练的重要方向。\n\n---\n\n\n### 3. **数据处理的挑战**\n- **问题**：\n  - 高质量数据（如论文、书籍）通常以PDF格式存在，解析复杂。\n- **解决方案**：\n  ✅ 使用专业PDF解析服务，避免依赖低效的Python库。  \n  ✅ 训练OCR模型，前提是有足够高质量的PDF-文本对齐数据。  \n  ⚠ 注意：直接用大模型（如GPT-4）解析PDF可能成本过高。  \n\n💡 **启发点**：高效的数据处理工具是预训练成功的关键。\n\n---\n\n\n\n## 常见错误与注意事项\n> **⚠ 警告**：  \n> - 数据质量直接影响模型性能，低质量或偏向单一领域的数据可能导致模型在实际应用中表现不佳。  \n> - 多语种处理需确保语料分布均衡，否则可能影响小语种任务的表现。\n\n---\n\n\n\n## [思考] 延伸问题\n1. 如何优化多模态数据（如图像与文本）的联合预训练方法？  \n2. 在多语种模型中，如何权衡不同语种的数据比例以提升性能？  \n3. 对于特定领域的垂直应用，是否需要重新设计预训练目标？\n\n---\n\n> **来源**：本文内容基于大模型预训练技术文档整理与总结。\n\n---\n\n\n\n## 行动清单\n1. ✅ 调研现有的开源数据集（如FineWeb、Pile、RedPajama），并尝试整合到自己的项目中。  \n2. ✅ 学习如何使用专业PDF解析服务或OCR技术提升数据处理效率。  \n3. ❗ 探索多模态预训练方法，关注图像、文本、音频等多种形式的数据整合。\n\n---\n\n\n\n## 后续追踪\n- 跟踪最新发布的大规模开源模型及其预训练技术进展。  \n- 深入研究自监督学习在非语言任务（如代码生成、逻辑推理）中的应用潜力。\n\n\n\n## 数据资源概览\n以下是一些常用的开源数据集及其获取地址：\n\n| 数据集名称                     | 地址                                                                 |\n|--------------------------------|----------------------------------------------------------------------|\n| Skywork/SkyPile-150B           | [点击访问](https://huggingface.co/datasets/Skywork/SkyPile-150B)     |\n| Wikipedia中文20230720          | [点击访问](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered) |\n| C4                             | [点击访问](https://github.com/allenai/allennlp/discussions/5056)     |\n| RedPajama                      | [点击访问](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2) |\n| EleutherAI/the_pile_deduplicated | [点击访问](https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated) |\n| WuDaoCorporaText               | [点击访问](https://data.baai.ac.cn/details/WuDaoCorporaText)         |\n| PRM800K                        | [点击访问](https://github.com/openai/prm800k?tab=readme-ov-file)     |\n| YeungNLP/firefly-pretrain-dataset | [点击访问](https://huggingface.co/datasets/YeungNLP/firefly-pretrain-dataset) |\n\n💡 **启发点**：这些数据集涵盖了多语言、多领域的语料，适合用于构建通用型预训练模型。\n\n---\n\n\n\n## 数据采样与分布策略\n在继续预训练中，通用数据的采样策略对模型性能影响显著。以下是一个具体分配案例：\n\n### 数据分布比例\n- **总Tokens量**：100B  \n- **语言分布**：中文 : 英文 : 代码 = 20% : 70% : 10%\n\n\n### 中文语料采样\n| 数据集名称          | Tokens数量 (单位：B) | 备注                      |\n|---------------------|---------------------|---------------------------|\n| cc                 | 4.024               |                          |\n| baidu_baike_v3     | 0.804               |                          |\n| wiki_zw            | 0.1602              |                          |\n| qikan              | 0.1602              |                          |\n| recipe             | 0.0182              |                          |\n\n\n### 英文语料采样\n| 数据集名称          | Tokens数量 (单位：B) | 备注                      |\n|---------------------|---------------------|---------------------------|\n| c4                 | 32.675              |                          |\n| arxiv_v2           | 3.2652              |                          |\n| wiki_en            | 3.6792              |                          |\n\n\n### Code语料采样\n| 数据集名称          | Tokens数量 (单位：B) | 备注                      |\n|---------------------|---------------------------|\n| code               | 4.9042              |                          |\n| github70v1         | 1.2168              |                          |\n\n---\n\n> 原始内容来源：[Skywork/SkyPile-150B](https://huggingface.co/datasets/Skywork/SkyPile-150B)、[Wikipedia中文20230720](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)、[C4](https://github.com/allenai/allennlp/discussions/5056) 等。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点内容解析","slug":"重点内容解析","link":"#重点内容解析","children":[{"level":3,"title":"1. 预训练的定义与目标","slug":"_1-预训练的定义与目标","link":"#_1-预训练的定义与目标","children":[]},{"level":3,"title":"2. 预训练数据的来源与规模","slug":"_2-预训练数据的来源与规模","link":"#_2-预训练数据的来源与规模","children":[]},{"level":3,"title":"3. 数据处理的挑战","slug":"_3-数据处理的挑战","link":"#_3-数据处理的挑战","children":[]}]},{"level":2,"title":"常见错误与注意事项","slug":"常见错误与注意事项","link":"#常见错误与注意事项","children":[]},{"level":2,"title":"[思考] 延伸问题","slug":"思考-延伸问题","link":"#思考-延伸问题","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]},{"level":2,"title":"数据资源概览","slug":"数据资源概览","link":"#数据资源概览","children":[]},{"level":2,"title":"数据采样与分布策略","slug":"数据采样与分布策略","link":"#数据采样与分布策略","children":[{"level":3,"title":"数据分布比例","slug":"数据分布比例","link":"#数据分布比例","children":[]},{"level":3,"title":"中文语料采样","slug":"中文语料采样","link":"#中文语料采样","children":[]},{"level":3,"title":"英文语料采样","slug":"英文语料采样","link":"#英文语料采样","children":[]},{"level":3,"title":"Code语料采样","slug":"code语料采样","link":"#code语料采样","children":[]}]}]}}
