{"content":"<hr>\n<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>：数据采集与处理<br>\n<strong>标签</strong>：数据爬取、爬虫技术、PDF解析、关键词爬取<br>\n<strong>日期</strong>：2023-10-11</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>数据爬取是现代信息处理的重要环节，尤其在高质量数据的获取上，针对不同格式的内容（如PDF文件和网页数据）需要采用不同的技术手段。本文探讨了以下几个方面：</p>\n<ol>\n<li>PDF格式数据的解析难点及解决方案。</li>\n<li>爬虫的分类及具体实现方式。</li>\n<li>爬虫配置的基本建议与代码示例。</li>\n</ol>\n<p>此外，还提供了针对特定平台（如小红书、抖音、快手、B站）的爬虫ID列表配置，以便更高效地获取目标数据。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"✅-高质量pdf数据解析方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-高质量pdf数据解析方法\"><span>✅ 高质量PDF数据解析方法</span></a></h3>\n<ul>\n<li><strong>问题</strong>：许多高质量数据（如论文或书籍）以PDF格式存储，但传统Python库解析效果有限，尤其当PDF中包含公式或表格时。</li>\n<li><strong>解决方案</strong>：\n<ol>\n<li>使用专业的PDF解析服务，提升解析准确性。</li>\n<li>利用GPT-4等大模型进行解析，但成本可能较高。</li>\n<li>自行训练OCR模型（前提是有足够高质量的PDF与文本对齐数据）。</li>\n</ol>\n</li>\n</ul>\n<p>💡启发点：根据需求选择合适的工具，权衡成本与效果。</p>\n<hr>\n<h3 id=\"✅-爬虫分类与实现方式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-爬虫分类与实现方式\"><span>✅ 爬虫分类与实现方式</span></a></h3>\n<p>爬虫主要分为以下三类：</p>\n<ol>\n<li><strong>定向网站爬取</strong>：针对特定网站的数据抓取。</li>\n<li><strong>基于关键词爬取指定网站</strong>：通过关键词过滤目标内容。</li>\n<li><strong>基于搜索引擎爬取</strong>：借助搜索引擎广泛获取相关信息。</li>\n</ol>\n<p>⚠ 常见错误：定向爬取时需注意目标网站的反爬机制，合理设置并发数量及请求间隔。</p>\n<hr>\n<h3 id=\"⚙-基本配置建议-代码片段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚙-基本配置建议-代码片段\"><span>⚙ 基本配置建议（代码片段）</span></a></h3>\n<p>以下是爬虫配置的关键参数：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 爬取视频/帖子的数量控制</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">CRAWLER_MAX_NOTES_COUNT</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 200</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 并发爬虫数量控制</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">MAX_CONCURRENCY_NUM</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬图片模式, 默认不开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_IMAGES</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬评论模式, 默认开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_COMMENTS</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 爬取一级评论的数量控制(单视频/帖子)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 10</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬二级评论模式, 默认不开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_SUB_COMMENTS</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> False</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>💡启发点：合理设置并发数和内容抓取量，可以避免被目标网站封禁，同时提高数据采集效率。</p>\n<hr>\n<h3 id=\"📋-指定平台爬虫id列表示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📋-指定平台爬虫id列表示例\"><span>📋 指定平台爬虫ID列表示例</span></a></h3>\n<p>以下是针对多个平台的数据爬虫ID列表配置：</p>\n<table>\n<thead>\n<tr>\n<th>平台</th>\n<th>ID类型</th>\n<th>示例ID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>小红书</td>\n<td>笔记URL</td>\n<td>https://www.xiaohongshu.com/explore/...</td>\n</tr>\n<tr>\n<td>抖音</td>\n<td>视频ID</td>\n<td>7280854932641664319, 7202432992642387233</td>\n</tr>\n<tr>\n<td>快手</td>\n<td>视频ID</td>\n<td>3xf8enb8dbj6uig, 3x6zz972bchmvqe</td>\n</tr>\n<tr>\n<td>B站</td>\n<td>视频bvid</td>\n<td>BV1d54y1g7db, BV1Sz4y1U77N, BV14Q4y1n7jz</td>\n</tr>\n</tbody>\n</table>\n<p>💡启发点：通过指定ID列表，可以精准定位目标内容，减少不必要的数据抓取。</p>\n<hr>\n<h2 id=\"常见错误警告-⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告-⚠️\"><span>常见错误警告 ⚠️</span></a></h2>\n<ul>\n<li><strong>错误一</strong>：未携带必要参数（如<code v-pre>xsec_token</code>），导致小红书笔记爬取失败。</li>\n<li><strong>错误二</strong>：并发数设置过高，引发目标网站封禁。</li>\n<li><strong>错误三</strong>：未充分考虑目标网站的反爬机制，忽略请求间隔设置。</li>\n</ul>\n<hr>\n<h2 id=\"思考板块-思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考板块-思考\"><span>思考板块 [思考]</span></a></h2>\n<ol>\n<li>如何在保证成本可控的前提下，提升PDF解析的准确性？</li>\n<li>面对复杂的反爬机制，是否有更智能化的解决方案？</li>\n<li>针对不同平台的数据结构差异，该如何设计通用型爬虫框架？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：<a href=\"https://github.com/NanmiCoder/MediaCrawler\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub项目链接</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>测试现有爬虫配置参数，优化并发数量设置。</li>\n<li>调研市面上最优的PDF解析服务，进行效果对比。</li>\n<li>针对小红书、抖音等平台，尝试抓取不同类型的数据并分析其结构。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着AI技术的发展，未来数据解析将更加智能化。大模型（如GPT系列）可能逐步成为主流工具，但成本控制仍是关键挑战。此外，反爬机制也会愈发复杂，对技术人员提出更高要求。</p>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>深入研究OCR模型训练方法，尝试构建自定义模型。</li>\n<li>探索更高效的反反爬技术，如动态代理池或模拟用户行为。</li>\n<li>对比多种PDF解析工具的性能，撰写评测报告。</li>\n</ul>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Pre-training 预训练/数据爬取 2.md","filePathRelative":"notes_bak/大语言模型学习/Pre-training 预训练/数据爬取 2.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/数据爬取","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/数据爬取/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-08T04:08:34.000Z","updated":"2025-04-13T05:06:02.000Z","title":"数据爬取","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><hr>\n<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>：数据采集与处理<br>\n<strong>标签</strong>：数据爬取、爬虫技术、PDF解析、关键词爬取<br>\n<strong>日期</strong>：2023-10-11</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>数据爬取是现代信息处理的重要环节，尤其在高质量数据的获取上，针对不同格式的内容（如PDF文件和网页数据）需要采用不同的技术手段。本文探讨了以下几个方面：</p>\n<ol>\n<li>PDF格式数据的解析难点及解决方案。</li>\n<li>爬虫的分类及具体实现方式。</li>\n<li>爬虫配置的基本建议与代码示例。</li>\n</ol>\n<p>此外，还提供了针对特定平台（如小红书、抖音、快手、B站）的爬虫ID列表配置，以便更高效地获取目标数据。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"✅-高质量pdf数据解析方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-高质量pdf数据解析方法\"><span>✅ 高质量PDF数据解析方法</span></a></h3>\n<ul>\n<li><strong>问题</strong>：许多高质量数据（如论文或书籍）以PDF格式存储，但传统Python库解析效果有限，尤其当PDF中包含公式或表格时。</li>\n<li><strong>解决方案</strong>：\n<ol>\n<li>使用专业的PDF解析服务，提升解析准确性。</li>\n<li>利用GPT-4等大模型进行解析，但成本可能较高。</li>\n<li>自行训练OCR模型（前提是有足够高质量的PDF与文本对齐数据）。</li>\n</ol>\n</li>\n</ul>\n<p>💡启发点：根据需求选择合适的工具，权衡成本与效果。</p>\n<hr>\n<h3 id=\"✅-爬虫分类与实现方式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-爬虫分类与实现方式\"><span>✅ 爬虫分类与实现方式</span></a></h3>\n<p>爬虫主要分为以下三类：</p>\n<ol>\n<li><strong>定向网站爬取</strong>：针对特定网站的数据抓取。</li>\n<li><strong>基于关键词爬取指定网站</strong>：通过关键词过滤目标内容。</li>\n<li><strong>基于搜索引擎爬取</strong>：借助搜索引擎广泛获取相关信息。</li>\n</ol>\n<p>⚠ 常见错误：定向爬取时需注意目标网站的反爬机制，合理设置并发数量及请求间隔。</p>\n<hr>\n<h3 id=\"⚙-基本配置建议-代码片段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚙-基本配置建议-代码片段\"><span>⚙ 基本配置建议（代码片段）</span></a></h3>\n<p>以下是爬虫配置的关键参数：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 爬取视频/帖子的数量控制</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">CRAWLER_MAX_NOTES_COUNT</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 200</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 并发爬虫数量控制</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">MAX_CONCURRENCY_NUM</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬图片模式, 默认不开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_IMAGES</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬评论模式, 默认开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_COMMENTS</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 爬取一级评论的数量控制(单视频/帖子)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 10</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬二级评论模式, 默认不开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_SUB_COMMENTS</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> False</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>💡启发点：合理设置并发数和内容抓取量，可以避免被目标网站封禁，同时提高数据采集效率。</p>\n<hr>\n<h3 id=\"📋-指定平台爬虫id列表示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📋-指定平台爬虫id列表示例\"><span>📋 指定平台爬虫ID列表示例</span></a></h3>\n<p>以下是针对多个平台的数据爬虫ID列表配置：</p>\n<table>\n<thead>\n<tr>\n<th>平台</th>\n<th>ID类型</th>\n<th>示例ID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>小红书</td>\n<td>笔记URL</td>\n<td>https://www.xiaohongshu.com/explore/...</td>\n</tr>\n<tr>\n<td>抖音</td>\n<td>视频ID</td>\n<td>7280854932641664319, 7202432992642387233</td>\n</tr>\n<tr>\n<td>快手</td>\n<td>视频ID</td>\n<td>3xf8enb8dbj6uig, 3x6zz972bchmvqe</td>\n</tr>\n<tr>\n<td>B站</td>\n<td>视频bvid</td>\n<td>BV1d54y1g7db, BV1Sz4y1U77N, BV14Q4y1n7jz</td>\n</tr>\n</tbody>\n</table>\n<p>💡启发点：通过指定ID列表，可以精准定位目标内容，减少不必要的数据抓取。</p>\n<hr>\n<h2 id=\"常见错误警告-⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告-⚠️\"><span>常见错误警告 ⚠️</span></a></h2>\n<ul>\n<li><strong>错误一</strong>：未携带必要参数（如<code v-pre>xsec_token</code>），导致小红书笔记爬取失败。</li>\n<li><strong>错误二</strong>：并发数设置过高，引发目标网站封禁。</li>\n<li><strong>错误三</strong>：未充分考虑目标网站的反爬机制，忽略请求间隔设置。</li>\n</ul>\n<hr>\n<h2 id=\"思考板块-思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考板块-思考\"><span>思考板块 [思考]</span></a></h2>\n<ol>\n<li>如何在保证成本可控的前提下，提升PDF解析的准确性？</li>\n<li>面对复杂的反爬机制，是否有更智能化的解决方案？</li>\n<li>针对不同平台的数据结构差异，该如何设计通用型爬虫框架？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：<a href=\"https://github.com/NanmiCoder/MediaCrawler\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub项目链接</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>测试现有爬虫配置参数，优化并发数量设置。</li>\n<li>调研市面上最优的PDF解析服务，进行效果对比。</li>\n<li>针对小红书、抖音等平台，尝试抓取不同类型的数据并分析其结构。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着AI技术的发展，未来数据解析将更加智能化。大模型（如GPT系列）可能逐步成为主流工具，但成本控制仍是关键挑战。此外，反爬机制也会愈发复杂，对技术人员提出更高要求。</p>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>深入研究OCR模型训练方法，尝试构建自定义模型。</li>\n<li>探索更高效的反反爬技术，如动态代理池或模拟用户行为。</li>\n<li>对比多种PDF解析工具的性能，撰写评测报告。</li>\n</ul>\n</template>","contentStripped":"<hr>\n<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>：数据采集与处理<br>\n<strong>标签</strong>：数据爬取、爬虫技术、PDF解析、关键词爬取<br>\n<strong>日期</strong>：2023-10-11</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>数据爬取是现代信息处理的重要环节，尤其在高质量数据的获取上，针对不同格式的内容（如PDF文件和网页数据）需要采用不同的技术手段。本文探讨了以下几个方面：</p>\n<ol>\n<li>PDF格式数据的解析难点及解决方案。</li>\n<li>爬虫的分类及具体实现方式。</li>\n<li>爬虫配置的基本建议与代码示例。</li>\n</ol>\n<p>此外，还提供了针对特定平台（如小红书、抖音、快手、B站）的爬虫ID列表配置，以便更高效地获取目标数据。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"✅-高质量pdf数据解析方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-高质量pdf数据解析方法\"><span>✅ 高质量PDF数据解析方法</span></a></h3>\n<ul>\n<li><strong>问题</strong>：许多高质量数据（如论文或书籍）以PDF格式存储，但传统Python库解析效果有限，尤其当PDF中包含公式或表格时。</li>\n<li><strong>解决方案</strong>：\n<ol>\n<li>使用专业的PDF解析服务，提升解析准确性。</li>\n<li>利用GPT-4等大模型进行解析，但成本可能较高。</li>\n<li>自行训练OCR模型（前提是有足够高质量的PDF与文本对齐数据）。</li>\n</ol>\n</li>\n</ul>\n<p>💡启发点：根据需求选择合适的工具，权衡成本与效果。</p>\n<hr>\n<h3 id=\"✅-爬虫分类与实现方式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-爬虫分类与实现方式\"><span>✅ 爬虫分类与实现方式</span></a></h3>\n<p>爬虫主要分为以下三类：</p>\n<ol>\n<li><strong>定向网站爬取</strong>：针对特定网站的数据抓取。</li>\n<li><strong>基于关键词爬取指定网站</strong>：通过关键词过滤目标内容。</li>\n<li><strong>基于搜索引擎爬取</strong>：借助搜索引擎广泛获取相关信息。</li>\n</ol>\n<p>⚠ 常见错误：定向爬取时需注意目标网站的反爬机制，合理设置并发数量及请求间隔。</p>\n<hr>\n<h3 id=\"⚙-基本配置建议-代码片段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚙-基本配置建议-代码片段\"><span>⚙ 基本配置建议（代码片段）</span></a></h3>\n<p>以下是爬虫配置的关键参数：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 爬取视频/帖子的数量控制</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">CRAWLER_MAX_NOTES_COUNT</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 200</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 并发爬虫数量控制</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">MAX_CONCURRENCY_NUM</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬图片模式, 默认不开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_IMAGES</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬评论模式, 默认开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_COMMENTS</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 爬取一级评论的数量控制(单视频/帖子)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 10</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 是否开启爬二级评论模式, 默认不开启</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ENABLE_GET_SUB_COMMENTS</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> False</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>💡启发点：合理设置并发数和内容抓取量，可以避免被目标网站封禁，同时提高数据采集效率。</p>\n<hr>\n<h3 id=\"📋-指定平台爬虫id列表示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📋-指定平台爬虫id列表示例\"><span>📋 指定平台爬虫ID列表示例</span></a></h3>\n<p>以下是针对多个平台的数据爬虫ID列表配置：</p>\n<table>\n<thead>\n<tr>\n<th>平台</th>\n<th>ID类型</th>\n<th>示例ID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>小红书</td>\n<td>笔记URL</td>\n<td>https://www.xiaohongshu.com/explore/...</td>\n</tr>\n<tr>\n<td>抖音</td>\n<td>视频ID</td>\n<td>7280854932641664319, 7202432992642387233</td>\n</tr>\n<tr>\n<td>快手</td>\n<td>视频ID</td>\n<td>3xf8enb8dbj6uig, 3x6zz972bchmvqe</td>\n</tr>\n<tr>\n<td>B站</td>\n<td>视频bvid</td>\n<td>BV1d54y1g7db, BV1Sz4y1U77N, BV14Q4y1n7jz</td>\n</tr>\n</tbody>\n</table>\n<p>💡启发点：通过指定ID列表，可以精准定位目标内容，减少不必要的数据抓取。</p>\n<hr>\n<h2 id=\"常见错误警告-⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告-⚠️\"><span>常见错误警告 ⚠️</span></a></h2>\n<ul>\n<li><strong>错误一</strong>：未携带必要参数（如<code v-pre>xsec_token</code>），导致小红书笔记爬取失败。</li>\n<li><strong>错误二</strong>：并发数设置过高，引发目标网站封禁。</li>\n<li><strong>错误三</strong>：未充分考虑目标网站的反爬机制，忽略请求间隔设置。</li>\n</ul>\n<hr>\n<h2 id=\"思考板块-思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考板块-思考\"><span>思考板块 [思考]</span></a></h2>\n<ol>\n<li>如何在保证成本可控的前提下，提升PDF解析的准确性？</li>\n<li>面对复杂的反爬机制，是否有更智能化的解决方案？</li>\n<li>针对不同平台的数据结构差异，该如何设计通用型爬虫框架？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：<a href=\"https://github.com/NanmiCoder/MediaCrawler\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub项目链接</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>测试现有爬虫配置参数，优化并发数量设置。</li>\n<li>调研市面上最优的PDF解析服务，进行效果对比。</li>\n<li>针对小红书、抖音等平台，尝试抓取不同类型的数据并分析其结构。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着AI技术的发展，未来数据解析将更加智能化。大模型（如GPT系列）可能逐步成为主流工具，但成本控制仍是关键挑战。此外，反爬机制也会愈发复杂，对技术人员提出更高要求。</p>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>深入研究OCR模型训练方法，尝试构建自定义模型。</li>\n<li>探索更高效的反反爬技术，如动态代理池或模拟用户行为。</li>\n<li>对比多种PDF解析工具的性能，撰写评测报告。</li>\n</ul>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"---\n\n## 元数据\n**分类**：数据采集与处理  \n**标签**：数据爬取、爬虫技术、PDF解析、关键词爬取  \n**日期**：2023-10-11  \n\n---\n\n\n## 核心观点总结\n数据爬取是现代信息处理的重要环节，尤其在高质量数据的获取上，针对不同格式的内容（如PDF文件和网页数据）需要采用不同的技术手段。本文探讨了以下几个方面：  \n\n1. PDF格式数据的解析难点及解决方案。  \n2. 爬虫的分类及具体实现方式。  \n3. 爬虫配置的基本建议与代码示例。  \n\n此外，还提供了针对特定平台（如小红书、抖音、快手、B站）的爬虫ID列表配置，以便更高效地获取目标数据。\n\n---\n\n\n## 重点内容解析\n\n### ✅ 高质量PDF数据解析方法\n- **问题**：许多高质量数据（如论文或书籍）以PDF格式存储，但传统Python库解析效果有限，尤其当PDF中包含公式或表格时。\n- **解决方案**：\n  1. 使用专业的PDF解析服务，提升解析准确性。\n  2. 利用GPT-4等大模型进行解析，但成本可能较高。\n  3. 自行训练OCR模型（前提是有足够高质量的PDF与文本对齐数据）。\n\n💡启发点：根据需求选择合适的工具，权衡成本与效果。\n\n---\n\n\n### ✅ 爬虫分类与实现方式\n爬虫主要分为以下三类：\n1. **定向网站爬取**：针对特定网站的数据抓取。\n2. **基于关键词爬取指定网站**：通过关键词过滤目标内容。\n3. **基于搜索引擎爬取**：借助搜索引擎广泛获取相关信息。\n\n⚠ 常见错误：定向爬取时需注意目标网站的反爬机制，合理设置并发数量及请求间隔。\n\n---\n\n\n### ⚙ 基本配置建议（代码片段）\n以下是爬虫配置的关键参数：\n\n```python\n# 爬取视频/帖子的数量控制\nCRAWLER_MAX_NOTES_COUNT = 200\n\n# 并发爬虫数量控制\nMAX_CONCURRENCY_NUM = 2\n\n# 是否开启爬图片模式, 默认不开启\nENABLE_GET_IMAGES = False\n\n# 是否开启爬评论模式, 默认开启\nENABLE_GET_COMMENTS = True\n\n# 爬取一级评论的数量控制(单视频/帖子)\nCRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = 10\n\n# 是否开启爬二级评论模式, 默认不开启\nENABLE_GET_SUB_COMMENTS = False\n```\n\n💡启发点：合理设置并发数和内容抓取量，可以避免被目标网站封禁，同时提高数据采集效率。\n\n---\n\n\n### 📋 指定平台爬虫ID列表示例\n以下是针对多个平台的数据爬虫ID列表配置：\n\n| 平台      | ID类型          | 示例ID                                      |\n|-----------|-----------------|---------------------------------------------|\n| 小红书    | 笔记URL         | https://www.xiaohongshu.com/explore/...    |\n| 抖音      | 视频ID          | 7280854932641664319, 7202432992642387233   |\n| 快手      | 视频ID          | 3xf8enb8dbj6uig, 3x6zz972bchmvqe           |\n| B站       | 视频bvid        | BV1d54y1g7db, BV1Sz4y1U77N, BV14Q4y1n7jz   |\n\n💡启发点：通过指定ID列表，可以精准定位目标内容，减少不必要的数据抓取。\n\n---\n\n\n## 常见错误警告 ⚠️\n- **错误一**：未携带必要参数（如`xsec_token`），导致小红书笔记爬取失败。\n- **错误二**：并发数设置过高，引发目标网站封禁。\n- **错误三**：未充分考虑目标网站的反爬机制，忽略请求间隔设置。\n\n---\n\n\n## 思考板块 [思考]\n1. 如何在保证成本可控的前提下，提升PDF解析的准确性？  \n2. 面对复杂的反爬机制，是否有更智能化的解决方案？  \n3. 针对不同平台的数据结构差异，该如何设计通用型爬虫框架？\n\n---\n\n> 原始出处：[GitHub项目链接](https://github.com/NanmiCoder/MediaCrawler)\n\n---\n\n\n## 行动清单\n1. 测试现有爬虫配置参数，优化并发数量设置。  \n2. 调研市面上最优的PDF解析服务，进行效果对比。  \n3. 针对小红书、抖音等平台，尝试抓取不同类型的数据并分析其结构。  \n\n---\n\n\n## 📈趋势预测\n随着AI技术的发展，未来数据解析将更加智能化。大模型（如GPT系列）可能逐步成为主流工具，但成本控制仍是关键挑战。此外，反爬机制也会愈发复杂，对技术人员提出更高要求。\n\n---\n\n\n## 后续追踪\n- 深入研究OCR模型训练方法，尝试构建自定义模型。  \n- 探索更高效的反反爬技术，如动态代理池或模拟用户行为。  \n- 对比多种PDF解析工具的性能，撰写评测报告。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点内容解析","slug":"重点内容解析","link":"#重点内容解析","children":[{"level":3,"title":"✅ 高质量PDF数据解析方法","slug":"✅-高质量pdf数据解析方法","link":"#✅-高质量pdf数据解析方法","children":[]},{"level":3,"title":"✅ 爬虫分类与实现方式","slug":"✅-爬虫分类与实现方式","link":"#✅-爬虫分类与实现方式","children":[]},{"level":3,"title":"⚙ 基本配置建议（代码片段）","slug":"⚙-基本配置建议-代码片段","link":"#⚙-基本配置建议-代码片段","children":[]},{"level":3,"title":"📋 指定平台爬虫ID列表示例","slug":"📋-指定平台爬虫id列表示例","link":"#📋-指定平台爬虫id列表示例","children":[]}]},{"level":2,"title":"常见错误警告 ⚠️","slug":"常见错误警告-⚠️","link":"#常见错误警告-⚠️","children":[]},{"level":2,"title":"思考板块 [思考]","slug":"思考板块-思考","link":"#思考板块-思考","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
