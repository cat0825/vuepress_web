{"content":"<p>分类：机器学习，人工智能</p>\n<p>标签：大模型训练，多任务学习，SFT策略</p>\n<p>日期：2023年10月20日</p>\n<h2 id=\"训练技巧\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练技巧\"><span>训练技巧</span></a></h2>\n<p>在进行大模型训练时，不同的任务类型（task_type）需要使用不同的损失函数（channel_loss）进行观察。特殊标记（special_token）的损失一开始可能会较高，但下降速度也很快。对于创作类任务，其损失通常比其他任务更高，因为这些任务的答案较为固定，搜索结果越单一，损失越低。</p>\n<h3 id=\"关键步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关键步骤\"><span>关键步骤</span></a></h3>\n<ol>\n<li><strong>观察损失函数变化</strong>：✅ 不同任务需要不同的损失函数。</li>\n<li><strong>注意初始损失水平</strong>：⚠ 确保使用通用的数据进行采样，初始损失不会特别高。</li>\n<li><strong>监控损失趋势</strong>：❗ 如果损失持续升高，检查训练代码而非数据难度。</li>\n</ol>\n<h3 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h3>\n<blockquote>\n<p>在训练过程中，如果发现损失持续升高，不要怀疑数据的难度，而是检查训练代码是否有问题。</p>\n</blockquote>\n<h2 id=\"sft阶段的packing策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sft阶段的packing策略\"><span>SFT阶段的Packing策略</span></a></h2>\n<p>在SFT（Supervised Fine-Tuning）阶段，不建议使用packing策略，因为这可能削弱模型对短查询和短答案的拟合能力。无packing情况下，短文本的梯度更集中，有助于提升模型拟合能力。然而，packing策略在大批量数据上对泛化效果无损。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ul>\n<li><strong>无packing时的梯度集中性</strong>：无packing情况下，batch的梯度全是短文本的梯度，这增强了模型对短查询的拟合能力。</li>\n<li><strong>packing对泛化效果的影响</strong>：在小数据量或特定困难的数据上，packing可能损害泛化效果。</li>\n</ul>\n<h2 id=\"训练策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练策略\"><span>训练策略</span></a></h2>\n<h3 id=\"多任务学习\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多任务学习\"><span>多任务学习</span></a></h3>\n<p>直接混合不同的SFT数据源并应用SFT，将每个数据源视为不同任务进行多任务学习。</p>\n<h3 id=\"顺序训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#顺序训练\"><span>顺序训练</span></a></h3>\n<p>依次在每个数据集上应用SFT，如编码、数学推理和综合能力数据集。</p>\n<h3 id=\"混合序列训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#混合序列训练\"><span>混合序列训练</span></a></h3>\n<p>在专业数据集（如代码、数学）上应用多任务学习，然后在通用能力数据集上应用SFT。\n<img src=\"/img/user/附件/Pasted image 20250411111212.png\" alt=\"Pasted image 20250411111212.png\"></p>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>在大规模数据集上，如何优化packing策略以提升泛化能力？</li>\n<li>多任务学习是否适用于所有类型的数据集？</li>\n<li>如何在训练过程中实时监控和调整损失函数？</li>\n</ul>\n<blockquote>\n<p>来源：《Do We Really Need Packing in LLM SFT?》，《Enhancing Training Efficiency Using Packing with Flash Attention》</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 评估当前模型的损失函数设置是否合理。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 实验不同的packing策略对小数据集的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索多任务学习在不同数据集上的效果。</label></li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着大模型训练技术的发展，未来可能会出现更加智能的自动化调参工具，以优化训练效率和效果。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>进一步研究如何在不同的数据集上优化多任务学习策略。</li>\n<li>探索更多关于SFT packing策略对模型性能影响的实验证据。</li>\n</ul>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略.md","filePathRelative":"notes_bak/大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-11T03:07:06.000Z","updated":"2025-04-13T05:06:02.000Z","title":"训练技巧和训练策略","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>分类：机器学习，人工智能</p>\n<p>标签：大模型训练，多任务学习，SFT策略</p>\n<p>日期：2023年10月20日</p>\n<h2 id=\"训练技巧\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练技巧\"><span>训练技巧</span></a></h2>\n<p>在进行大模型训练时，不同的任务类型（task_type）需要使用不同的损失函数（channel_loss）进行观察。特殊标记（special_token）的损失一开始可能会较高，但下降速度也很快。对于创作类任务，其损失通常比其他任务更高，因为这些任务的答案较为固定，搜索结果越单一，损失越低。</p>\n<h3 id=\"关键步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关键步骤\"><span>关键步骤</span></a></h3>\n<ol>\n<li><strong>观察损失函数变化</strong>：✅ 不同任务需要不同的损失函数。</li>\n<li><strong>注意初始损失水平</strong>：⚠ 确保使用通用的数据进行采样，初始损失不会特别高。</li>\n<li><strong>监控损失趋势</strong>：❗ 如果损失持续升高，检查训练代码而非数据难度。</li>\n</ol>\n<h3 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h3>\n<blockquote>\n<p>在训练过程中，如果发现损失持续升高，不要怀疑数据的难度，而是检查训练代码是否有问题。</p>\n</blockquote>\n<h2 id=\"sft阶段的packing策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sft阶段的packing策略\"><span>SFT阶段的Packing策略</span></a></h2>\n<p>在SFT（Supervised Fine-Tuning）阶段，不建议使用packing策略，因为这可能削弱模型对短查询和短答案的拟合能力。无packing情况下，短文本的梯度更集中，有助于提升模型拟合能力。然而，packing策略在大批量数据上对泛化效果无损。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ul>\n<li><strong>无packing时的梯度集中性</strong>：无packing情况下，batch的梯度全是短文本的梯度，这增强了模型对短查询的拟合能力。</li>\n<li><strong>packing对泛化效果的影响</strong>：在小数据量或特定困难的数据上，packing可能损害泛化效果。</li>\n</ul>\n<h2 id=\"训练策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练策略\"><span>训练策略</span></a></h2>\n<h3 id=\"多任务学习\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多任务学习\"><span>多任务学习</span></a></h3>\n<p>直接混合不同的SFT数据源并应用SFT，将每个数据源视为不同任务进行多任务学习。</p>\n<h3 id=\"顺序训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#顺序训练\"><span>顺序训练</span></a></h3>\n<p>依次在每个数据集上应用SFT，如编码、数学推理和综合能力数据集。</p>\n<h3 id=\"混合序列训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#混合序列训练\"><span>混合序列训练</span></a></h3>\n<p>在专业数据集（如代码、数学）上应用多任务学习，然后在通用能力数据集上应用SFT。\n<img src=\"/img/user/附件/Pasted image 20250411111212.png\" alt=\"Pasted image 20250411111212.png\"></p>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>在大规模数据集上，如何优化packing策略以提升泛化能力？</li>\n<li>多任务学习是否适用于所有类型的数据集？</li>\n<li>如何在训练过程中实时监控和调整损失函数？</li>\n</ul>\n<blockquote>\n<p>来源：《Do We Really Need Packing in LLM SFT?》，《Enhancing Training Efficiency Using Packing with Flash Attention》</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 评估当前模型的损失函数设置是否合理。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 实验不同的packing策略对小数据集的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索多任务学习在不同数据集上的效果。</label></li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着大模型训练技术的发展，未来可能会出现更加智能的自动化调参工具，以优化训练效率和效果。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>进一步研究如何在不同的数据集上优化多任务学习策略。</li>\n<li>探索更多关于SFT packing策略对模型性能影响的实验证据。</li>\n</ul>\n</template>","contentStripped":"<p>分类：机器学习，人工智能</p>\n<p>标签：大模型训练，多任务学习，SFT策略</p>\n<p>日期：2023年10月20日</p>\n<h2 id=\"训练技巧\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练技巧\"><span>训练技巧</span></a></h2>\n<p>在进行大模型训练时，不同的任务类型（task_type）需要使用不同的损失函数（channel_loss）进行观察。特殊标记（special_token）的损失一开始可能会较高，但下降速度也很快。对于创作类任务，其损失通常比其他任务更高，因为这些任务的答案较为固定，搜索结果越单一，损失越低。</p>\n<h3 id=\"关键步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关键步骤\"><span>关键步骤</span></a></h3>\n<ol>\n<li><strong>观察损失函数变化</strong>：✅ 不同任务需要不同的损失函数。</li>\n<li><strong>注意初始损失水平</strong>：⚠ 确保使用通用的数据进行采样，初始损失不会特别高。</li>\n<li><strong>监控损失趋势</strong>：❗ 如果损失持续升高，检查训练代码而非数据难度。</li>\n</ol>\n<h3 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h3>\n<blockquote>\n<p>在训练过程中，如果发现损失持续升高，不要怀疑数据的难度，而是检查训练代码是否有问题。</p>\n</blockquote>\n<h2 id=\"sft阶段的packing策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sft阶段的packing策略\"><span>SFT阶段的Packing策略</span></a></h2>\n<p>在SFT（Supervised Fine-Tuning）阶段，不建议使用packing策略，因为这可能削弱模型对短查询和短答案的拟合能力。无packing情况下，短文本的梯度更集中，有助于提升模型拟合能力。然而，packing策略在大批量数据上对泛化效果无损。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ul>\n<li><strong>无packing时的梯度集中性</strong>：无packing情况下，batch的梯度全是短文本的梯度，这增强了模型对短查询的拟合能力。</li>\n<li><strong>packing对泛化效果的影响</strong>：在小数据量或特定困难的数据上，packing可能损害泛化效果。</li>\n</ul>\n<h2 id=\"训练策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练策略\"><span>训练策略</span></a></h2>\n<h3 id=\"多任务学习\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多任务学习\"><span>多任务学习</span></a></h3>\n<p>直接混合不同的SFT数据源并应用SFT，将每个数据源视为不同任务进行多任务学习。</p>\n<h3 id=\"顺序训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#顺序训练\"><span>顺序训练</span></a></h3>\n<p>依次在每个数据集上应用SFT，如编码、数学推理和综合能力数据集。</p>\n<h3 id=\"混合序列训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#混合序列训练\"><span>混合序列训练</span></a></h3>\n<p>在专业数据集（如代码、数学）上应用多任务学习，然后在通用能力数据集上应用SFT。\n<img src=\"/img/user/附件/Pasted image 20250411111212.png\" alt=\"Pasted image 20250411111212.png\"></p>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>在大规模数据集上，如何优化packing策略以提升泛化能力？</li>\n<li>多任务学习是否适用于所有类型的数据集？</li>\n<li>如何在训练过程中实时监控和调整损失函数？</li>\n</ul>\n<blockquote>\n<p>来源：《Do We Really Need Packing in LLM SFT?》，《Enhancing Training Efficiency Using Packing with Flash Attention》</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 评估当前模型的损失函数设置是否合理。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 实验不同的packing策略对小数据集的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 探索多任务学习在不同数据集上的效果。</label></li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着大模型训练技术的发展，未来可能会出现更加智能的自动化调参工具，以优化训练效率和效果。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>进一步研究如何在不同的数据集上优化多任务学习策略。</li>\n<li>探索更多关于SFT packing策略对模型性能影响的实验证据。</li>\n</ul>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"分类：机器学习，人工智能\n\n标签：大模型训练，多任务学习，SFT策略\n\n日期：2023年10月20日\n\n## 训练技巧\n在进行大模型训练时，不同的任务类型（task_type）需要使用不同的损失函数（channel_loss）进行观察。特殊标记（special_token）的损失一开始可能会较高，但下降速度也很快。对于创作类任务，其损失通常比其他任务更高，因为这些任务的答案较为固定，搜索结果越单一，损失越低。\n\n### 关键步骤\n1. **观察损失函数变化**：✅ 不同任务需要不同的损失函数。\n2. **注意初始损失水平**：⚠ 确保使用通用的数据进行采样，初始损失不会特别高。\n3. **监控损失趋势**：❗ 如果损失持续升高，检查训练代码而非数据难度。\n\n\n### 常见错误\n> 在训练过程中，如果发现损失持续升高，不要怀疑数据的难度，而是检查训练代码是否有问题。\n\n\n## SFT阶段的Packing策略\n在SFT（Supervised Fine-Tuning）阶段，不建议使用packing策略，因为这可能削弱模型对短查询和短答案的拟合能力。无packing情况下，短文本的梯度更集中，有助于提升模型拟合能力。然而，packing策略在大批量数据上对泛化效果无损。\n\n### 重点段落\n- **无packing时的梯度集中性**：无packing情况下，batch的梯度全是短文本的梯度，这增强了模型对短查询的拟合能力。\n- **packing对泛化效果的影响**：在小数据量或特定困难的数据上，packing可能损害泛化效果。\n\n\n## 训练策略\n\n### 多任务学习\n直接混合不同的SFT数据源并应用SFT，将每个数据源视为不同任务进行多任务学习。\n\n\n### 顺序训练\n依次在每个数据集上应用SFT，如编码、数学推理和综合能力数据集。\n\n\n### 混合序列训练\n在专业数据集（如代码、数学）上应用多任务学习，然后在通用能力数据集上应用SFT。\n![Pasted image 20250411111212.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250411111212.png)\n\n\n## 思考\n- 在大规模数据集上，如何优化packing策略以提升泛化能力？\n- 多任务学习是否适用于所有类型的数据集？\n- 如何在训练过程中实时监控和调整损失函数？\n\n> 来源：《Do We Really Need Packing in LLM SFT?》，《Enhancing Training Efficiency Using Packing with Flash Attention》\n\n\n## 行动清单\n- [ ] 评估当前模型的损失函数设置是否合理。\n- [ ] 实验不同的packing策略对小数据集的影响。\n- [ ] 探索多任务学习在不同数据集上的效果。\n\n\n## 📈趋势预测\n随着大模型训练技术的发展，未来可能会出现更加智能的自动化调参工具，以优化训练效率和效果。\n\n\n## 后续追踪\n- 进一步研究如何在不同的数据集上优化多任务学习策略。\n- 探索更多关于SFT packing策略对模型性能影响的实验证据。","excerpt":"","includedFiles":[],"tasklistId":3,"title":"","headers":[{"level":2,"title":"训练技巧","slug":"训练技巧","link":"#训练技巧","children":[{"level":3,"title":"关键步骤","slug":"关键步骤","link":"#关键步骤","children":[]},{"level":3,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]}]},{"level":2,"title":"SFT阶段的Packing策略","slug":"sft阶段的packing策略","link":"#sft阶段的packing策略","children":[{"level":3,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[]}]},{"level":2,"title":"训练策略","slug":"训练策略","link":"#训练策略","children":[{"level":3,"title":"多任务学习","slug":"多任务学习","link":"#多任务学习","children":[]},{"level":3,"title":"顺序训练","slug":"顺序训练","link":"#顺序训练","children":[]},{"level":3,"title":"混合序列训练","slug":"混合序列训练","link":"#混合序列训练","children":[]}]},{"level":2,"title":"思考","slug":"思考","link":"#思考","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
