{"content":"<p>在大模型的训练过程中，如何高效地利用计算资源是一个非常重要的问题。本文将介绍一种常用的技巧——Packing，来帮助我们在模型训练中更高效地利用计算资源。</p>\n<h2 id=\"why\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#why\"><span>Why</span></a></h2>\n<p>在传统的模型训练中，通常会使用一个固定长度的输入序列。但实际数据中的文本片段长度各不相同，如果直接将短文本用 [PAD] token 填充到固定长度，会造成大量无效计算。</p>\n<h2 id=\"what\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#what\"><span>What</span></a></h2>\n<p>Packing 是指将多个短的序列拼接成一个长序列，使得每个 batch 的 token 数更接近最大长度限制，最大化 token 利用率。实际上，做 packing 时既可以按照 batch 内最长句子填充，也可以按照模型最长输入长度填充。</p>\n<h3 id=\"_5-7-1-预训练阶段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-7-1-预训练阶段\"><span>5.7.1 预训练阶段</span></a></h3>\n<p>在预训练阶段，packing 大部分就是传统的直接拼接，不同来源的文本使用特殊字符隔开，例如 [SEP] 的特殊 token。</p>\n<p>当然，如果遇到超长文本则会面临阶段的问题。其实我们允许部分过长的文本在预训练前期（非长文本继续预训练）出现这种阶段现象，但是占比不能过高。那么如何解决呢？</p>\n<p>一个很好的思路是，短文本和长文本各自 packing，而且短文本的 pack 大小等于原来的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>max</mi><mo>⁡</mo><mtext> </mtext><mtext>length</mtext></mrow><annotation encoding=\"application/x-tex\">\\max \\, \\text{length}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mop\">max</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\">length</span></span></span></span></span>（比如 4k），长文本的 pack 大小为 128k，然后再将短文本再多进行一次 pack 就可以解决了。</p>\n<h3 id=\"_5-7-2-微调阶段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-7-2-微调阶段\"><span>5.7.2 微调阶段</span></a></h3>\n<p>不做 packing 的高效做法</p>\n<p>这里的高效主要还是体现在多轮对话场景下。</p>\n<p>由于 CLM 采用下三角形式的 attention mask，其结构确保每个 token 仅能关注自身及之前的 token。因此，在预测答案一下 token 时，模型只会接收到该 token 之前的信息，后续的内容无法被访问。基于这一机制，我们可以在一次前向计算中获取整个多轮对话中所有 response 的 logits，随后在计算 loss 时，仅保留 response 部分的 loss，而忽略掉 prompt 部分。</p>\n<h2 id=\"咱们来看看-llama-factory-是如何优化的\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#咱们来看看-llama-factory-是如何优化的\"><span>咱们来看看 Llama Factory 是如何优化的</span></a></h2>\n<p>和之前直接使用贪心搜索算法不同，得益于目前 transformer 支持 4D mask 机制，目前最新的实现方式新增了对 attention mask 进行区分，不同文档使用不同的标识符，然后 padding 部分用 0 标识。</p>\n<p>具体的实现细节如下所示：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">if</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> !=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> data_args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cutoff_len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    raise</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> ValueError</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">The length of packed example should be identical to the cutoff length.</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">input_ids</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">attention_mask</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_attention_masks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">labels</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_labels</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> {</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">input_ids</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">attention_mask</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">labels</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 这里将长度排序，然后贪心检索最大长度加入，直到要超过 cutoff-len</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 得到一个二维数组，里面是每组数据包含的数据长度</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 如： [[2048],[1024,1023],[1000,1000,41],[500,500,500,500,40]]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">knapsacks </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> greedy_knapsack</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">lengths</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> data_args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cutoff_len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> knapsack </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> knapsacks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> packed_attention_masks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> packed_labels </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> …</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>什么是4D mask ？</p>\n<p>在模型内部，二维张量掩码会变成四维张量，形状为</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo separator=\"true\">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>d</mi><mi>s</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[batch\\_size, heads, input\\_ids\\_length, total\\_sequence\\_length]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">ba</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">h</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">ze</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">in</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">ce</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p>这种格式允许更细致的注意力策略，例如因果解码，它使用一个由 1 组成的下三角矩阵，有时如果存在键值 (KV) 缓存，还会辅以一个由 1 组成的矩形矩阵。</p>\n<p>对batch中每一条数据进行上面的操作，然后复制num_heads份，就可以构成4D mask，维度是</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>b</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant=\"normal\">_</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo separator=\"true\">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[bs, num\\_heads, seq\\_len, seq\\_len]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">m</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p><img src=\"/img/user/附件/Pasted image 20250430225058.png\" alt=\"Pasted image 20250430225058.png\"></p>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/训练推理优化/大模型的packing技巧.md","filePathRelative":"notes_bak/大语言模型学习/训练推理优化/大模型的packing技巧.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/训练推理优化/大模型的packing技巧","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/训练推理优化/大模型的packing技巧/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-30T14:44:59.000Z","updated":"2025-05-06T02:29:38.000Z","title":"大模型的packing技巧","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>在大模型的训练过程中，如何高效地利用计算资源是一个非常重要的问题。本文将介绍一种常用的技巧——Packing，来帮助我们在模型训练中更高效地利用计算资源。</p>\n<h2 id=\"why\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#why\"><span>Why</span></a></h2>\n<p>在传统的模型训练中，通常会使用一个固定长度的输入序列。但实际数据中的文本片段长度各不相同，如果直接将短文本用 [PAD] token 填充到固定长度，会造成大量无效计算。</p>\n<h2 id=\"what\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#what\"><span>What</span></a></h2>\n<p>Packing 是指将多个短的序列拼接成一个长序列，使得每个 batch 的 token 数更接近最大长度限制，最大化 token 利用率。实际上，做 packing 时既可以按照 batch 内最长句子填充，也可以按照模型最长输入长度填充。</p>\n<h3 id=\"_5-7-1-预训练阶段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-7-1-预训练阶段\"><span>5.7.1 预训练阶段</span></a></h3>\n<p>在预训练阶段，packing 大部分就是传统的直接拼接，不同来源的文本使用特殊字符隔开，例如 [SEP] 的特殊 token。</p>\n<p>当然，如果遇到超长文本则会面临阶段的问题。其实我们允许部分过长的文本在预训练前期（非长文本继续预训练）出现这种阶段现象，但是占比不能过高。那么如何解决呢？</p>\n<p>一个很好的思路是，短文本和长文本各自 packing，而且短文本的 pack 大小等于原来的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>max</mi><mo>⁡</mo><mtext> </mtext><mtext>length</mtext></mrow><annotation encoding=\"application/x-tex\">\\max \\, \\text{length}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mop\">max</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\">length</span></span></span></span></span>（比如 4k），长文本的 pack 大小为 128k，然后再将短文本再多进行一次 pack 就可以解决了。</p>\n<h3 id=\"_5-7-2-微调阶段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-7-2-微调阶段\"><span>5.7.2 微调阶段</span></a></h3>\n<p>不做 packing 的高效做法</p>\n<p>这里的高效主要还是体现在多轮对话场景下。</p>\n<p>由于 CLM 采用下三角形式的 attention mask，其结构确保每个 token 仅能关注自身及之前的 token。因此，在预测答案一下 token 时，模型只会接收到该 token 之前的信息，后续的内容无法被访问。基于这一机制，我们可以在一次前向计算中获取整个多轮对话中所有 response 的 logits，随后在计算 loss 时，仅保留 response 部分的 loss，而忽略掉 prompt 部分。</p>\n<h2 id=\"咱们来看看-llama-factory-是如何优化的\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#咱们来看看-llama-factory-是如何优化的\"><span>咱们来看看 Llama Factory 是如何优化的</span></a></h2>\n<p>和之前直接使用贪心搜索算法不同，得益于目前 transformer 支持 4D mask 机制，目前最新的实现方式新增了对 attention mask 进行区分，不同文档使用不同的标识符，然后 padding 部分用 0 标识。</p>\n<p>具体的实现细节如下所示：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">if</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> !=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> data_args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cutoff_len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    raise</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> ValueError</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">The length of packed example should be identical to the cutoff length.</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">input_ids</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">attention_mask</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_attention_masks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">labels</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_labels</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> {</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">input_ids</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">attention_mask</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">labels</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 这里将长度排序，然后贪心检索最大长度加入，直到要超过 cutoff-len</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 得到一个二维数组，里面是每组数据包含的数据长度</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 如： [[2048],[1024,1023],[1000,1000,41],[500,500,500,500,40]]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">knapsacks </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> greedy_knapsack</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">lengths</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> data_args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cutoff_len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> knapsack </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> knapsacks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> packed_attention_masks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> packed_labels </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> …</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>什么是4D mask ？</p>\n<p>在模型内部，二维张量掩码会变成四维张量，形状为</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo separator=\"true\">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>d</mi><mi>s</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[batch\\_size, heads, input\\_ids\\_length, total\\_sequence\\_length]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">ba</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">h</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">ze</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">in</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">ce</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p>这种格式允许更细致的注意力策略，例如因果解码，它使用一个由 1 组成的下三角矩阵，有时如果存在键值 (KV) 缓存，还会辅以一个由 1 组成的矩形矩阵。</p>\n<p>对batch中每一条数据进行上面的操作，然后复制num_heads份，就可以构成4D mask，维度是</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>b</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant=\"normal\">_</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo separator=\"true\">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[bs, num\\_heads, seq\\_len, seq\\_len]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">m</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p><img src=\"/img/user/附件/Pasted image 20250430225058.png\" alt=\"Pasted image 20250430225058.png\"></p>\n</template>","contentStripped":"<p>在大模型的训练过程中，如何高效地利用计算资源是一个非常重要的问题。本文将介绍一种常用的技巧——Packing，来帮助我们在模型训练中更高效地利用计算资源。</p>\n<h2 id=\"why\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#why\"><span>Why</span></a></h2>\n<p>在传统的模型训练中，通常会使用一个固定长度的输入序列。但实际数据中的文本片段长度各不相同，如果直接将短文本用 [PAD] token 填充到固定长度，会造成大量无效计算。</p>\n<h2 id=\"what\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#what\"><span>What</span></a></h2>\n<p>Packing 是指将多个短的序列拼接成一个长序列，使得每个 batch 的 token 数更接近最大长度限制，最大化 token 利用率。实际上，做 packing 时既可以按照 batch 内最长句子填充，也可以按照模型最长输入长度填充。</p>\n<h3 id=\"_5-7-1-预训练阶段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-7-1-预训练阶段\"><span>5.7.1 预训练阶段</span></a></h3>\n<p>在预训练阶段，packing 大部分就是传统的直接拼接，不同来源的文本使用特殊字符隔开，例如 [SEP] 的特殊 token。</p>\n<p>当然，如果遇到超长文本则会面临阶段的问题。其实我们允许部分过长的文本在预训练前期（非长文本继续预训练）出现这种阶段现象，但是占比不能过高。那么如何解决呢？</p>\n<p>一个很好的思路是，短文本和长文本各自 packing，而且短文本的 pack 大小等于原来的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>max</mi><mo>⁡</mo><mtext> </mtext><mtext>length</mtext></mrow><annotation encoding=\"application/x-tex\">\\max \\, \\text{length}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mop\">max</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\">length</span></span></span></span></span>（比如 4k），长文本的 pack 大小为 128k，然后再将短文本再多进行一次 pack 就可以解决了。</p>\n<h3 id=\"_5-7-2-微调阶段\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-7-2-微调阶段\"><span>5.7.2 微调阶段</span></a></h3>\n<p>不做 packing 的高效做法</p>\n<p>这里的高效主要还是体现在多轮对话场景下。</p>\n<p>由于 CLM 采用下三角形式的 attention mask，其结构确保每个 token 仅能关注自身及之前的 token。因此，在预测答案一下 token 时，模型只会接收到该 token 之前的信息，后续的内容无法被访问。基于这一机制，我们可以在一次前向计算中获取整个多轮对话中所有 response 的 logits，随后在计算 loss 时，仅保留 response 部分的 loss，而忽略掉 prompt 部分。</p>\n<h2 id=\"咱们来看看-llama-factory-是如何优化的\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#咱们来看看-llama-factory-是如何优化的\"><span>咱们来看看 Llama Factory 是如何优化的</span></a></h2>\n<p>和之前直接使用贪心搜索算法不同，得益于目前 transformer 支持 4D mask 机制，目前最新的实现方式新增了对 attention mask 进行区分，不同文档使用不同的标识符，然后 padding 部分用 0 标识。</p>\n<p>具体的实现细节如下所示：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">if</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> !=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> data_args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cutoff_len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    raise</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> ValueError</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">The length of packed example should be identical to the cutoff length.</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">input_ids</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">attention_mask</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_attention_masks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">labels</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">].</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">append</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">packed_labels</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">model_inputs </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> {</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">input_ids</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">attention_mask</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">labels</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 这里将长度排序，然后贪心检索最大长度加入，直到要超过 cutoff-len</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 得到一个二维数组，里面是每组数据包含的数据长度</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 如： [[2048],[1024,1023],[1000,1000,41],[500,500,500,500,40]]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">knapsacks </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> greedy_knapsack</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">lengths</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> data_args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cutoff_len</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> knapsack </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">in</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> knapsacks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    packed_input_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> packed_attention_masks</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> packed_labels </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> [],</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> []</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    for</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> …</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>什么是4D mask ？</p>\n<p>在模型内部，二维张量掩码会变成四维张量，形状为</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo separator=\"true\">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>d</mi><mi>s</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[batch\\_size, heads, input\\_ids\\_length, total\\_sequence\\_length]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">ba</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">h</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">ze</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">in</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">ce</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p>这种格式允许更细致的注意力策略，例如因果解码，它使用一个由 1 组成的下三角矩阵，有时如果存在键值 (KV) 缓存，还会辅以一个由 1 组成的矩形矩阵。</p>\n<p>对batch中每一条数据进行上面的操作，然后复制num_heads份，就可以构成4D mask，维度是</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>b</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant=\"normal\">_</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo separator=\"true\">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi mathvariant=\"normal\">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[bs, num\\_heads, seq\\_len, seq\\_len]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">m</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">se</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p><img src=\"/img/user/附件/Pasted image 20250430225058.png\" alt=\"Pasted image 20250430225058.png\"></p>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"在大模型的训练过程中，如何高效地利用计算资源是一个非常重要的问题。本文将介绍一种常用的技巧——Packing，来帮助我们在模型训练中更高效地利用计算资源。\n\n## Why\n在传统的模型训练中，通常会使用一个固定长度的输入序列。但实际数据中的文本片段长度各不相同，如果直接将短文本用 [PAD] token 填充到固定长度，会造成大量无效计算。\n\n\n## What\nPacking 是指将多个短的序列拼接成一个长序列，使得每个 batch 的 token 数更接近最大长度限制，最大化 token 利用率。实际上，做 packing 时既可以按照 batch 内最长句子填充，也可以按照模型最长输入长度填充。\n\n### 5.7.1 预训练阶段\n在预训练阶段，packing 大部分就是传统的直接拼接，不同来源的文本使用特殊字符隔开，例如 [SEP] 的特殊 token。\n\n当然，如果遇到超长文本则会面临阶段的问题。其实我们允许部分过长的文本在预训练前期（非长文本继续预训练）出现这种阶段现象，但是占比不能过高。那么如何解决呢？\n\n一个很好的思路是，短文本和长文本各自 packing，而且短文本的 pack 大小等于原来的 $\\max \\, \\text{length}$（比如 4k），长文本的 pack 大小为 128k，然后再将短文本再多进行一次 pack 就可以解决了。\n\n\n### 5.7.2 微调阶段\n不做 packing 的高效做法\n\n这里的高效主要还是体现在多轮对话场景下。\n\n由于 CLM 采用下三角形式的 attention mask，其结构确保每个 token 仅能关注自身及之前的 token。因此，在预测答案一下 token 时，模型只会接收到该 token 之前的信息，后续的内容无法被访问。基于这一机制，我们可以在一次前向计算中获取整个多轮对话中所有 response 的 logits，随后在计算 loss 时，仅保留 response 部分的 loss，而忽略掉 prompt 部分。\n\n\n## 咱们来看看 Llama Factory 是如何优化的\n和之前直接使用贪心搜索算法不同，得益于目前 transformer 支持 4D mask 机制，目前最新的实现方式新增了对 attention mask 进行区分，不同文档使用不同的标识符，然后 padding 部分用 0 标识。\n\n具体的实现细节如下所示：\n\n```python\nif len(packed_input_ids) != data_args.cutoff_len:\n    raise ValueError(\"The length of packed example should be identical to the cutoff length.\")\n\nmodel_inputs[\"input_ids\"].append(packed_input_ids)\nmodel_inputs[\"attention_mask\"].append(packed_attention_masks)\nmodel_inputs[\"labels\"].append(packed_labels)\nmodel_inputs = {\n    \"input_ids\": [],\n    \"attention_mask\": [],\n    \"labels\": []\n}\n\n# 这里将长度排序，然后贪心检索最大长度加入，直到要超过 cutoff-len\n# 得到一个二维数组，里面是每组数据包含的数据长度\n# 如： [[2048],[1024,1023],[1000,1000,41],[500,500,500,500,40]]\n\nknapsacks = greedy_knapsack(lengths, data_args.cutoff_len)\n\nfor knapsack in knapsacks:\n    packed_input_ids, packed_attention_masks, packed_labels = [], [], []\n    for …\n```\n\n什么是4D mask ？\n\n在模型内部，二维张量掩码会变成四维张量，形状为 \n\n$$\n[batch\\_size, heads, input\\_ids\\_length, total\\_sequence\\_length]\n$$\n\n这种格式允许更细致的注意力策略，例如因果解码，它使用一个由 1 组成的下三角矩阵，有时如果存在键值 (KV) 缓存，还会辅以一个由 1 组成的矩形矩阵。\n\n对batch中每一条数据进行上面的操作，然后复制num_heads份，就可以构成4D mask，维度是\n\n$$\n[bs, num\\_heads, seq\\_len, seq\\_len]\n$$\n![Pasted image 20250430225058.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250430225058.png)","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"Why","slug":"why","link":"#why","children":[]},{"level":2,"title":"What","slug":"what","link":"#what","children":[{"level":3,"title":"5.7.1 预训练阶段","slug":"_5-7-1-预训练阶段","link":"#_5-7-1-预训练阶段","children":[]},{"level":3,"title":"5.7.2 微调阶段","slug":"_5-7-2-微调阶段","link":"#_5-7-2-微调阶段","children":[]}]},{"level":2,"title":"咱们来看看 Llama Factory 是如何优化的","slug":"咱们来看看-llama-factory-是如何优化的","link":"#咱们来看看-llama-factory-是如何优化的","children":[]}]}}
