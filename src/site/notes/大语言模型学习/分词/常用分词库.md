---
{"dg-publish":true,"dg-permalink":"/å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ /åˆ†è¯/å¸¸ç”¨åˆ†è¯åº“","dg-home":false,"dg-description":"åœ¨æ­¤è¾“å…¥ç¬”è®°çš„æè¿°","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"åœ¨æ­¤è¾“å…¥è®¿é—®å¯†ç ","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":10,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ /åˆ†è¯/å¸¸ç”¨åˆ†è¯åº“/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":10,"created":"2025-03-27T12:51:56.518+08:00","updated":"2025-04-12T12:53:56.473+08:00"}
---



# **SentencePieceä¸Tokenizersåº“ï¼šæ–‡æœ¬åˆ†è¯çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ**

## å…ƒæ•°æ®
- **åˆ†ç±»**ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰
- **æ ‡ç­¾**ï¼šåˆ†è¯ç®—æ³•ã€SentencePieceã€Tokenizersåº“ã€NLPå·¥å…·ã€å¤šè¯­è¨€æ”¯æŒ
- **æ—¥æœŸ**ï¼š2025å¹´4æœˆ2æ—¥  

---


## **æ ¸å¿ƒå†…å®¹æ€»ç»“**
æ–‡æœ¬åˆ†è¯æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„å…³é”®æ­¥éª¤ï¼Œè€Œ`SentencePiece`å’Œ`Tokenizersåº“`æä¾›äº†é«˜æ•ˆã€çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚ä»¥ä¸‹æ˜¯ä¸¤è€…çš„æ ¸å¿ƒç‰¹ç‚¹å’ŒåŠŸèƒ½æ€»ç»“ï¼š

- **SentencePiece**ï¼šæ”¯æŒå¤šç§åˆ†è¯ç®—æ³•ï¼ˆå¦‚BPEã€ULMï¼‰ï¼Œå…·å¤‡å¤šè¯­è¨€æ”¯æŒã€ç¼–è§£ç å¯é€†æ€§ï¼Œæ— éœ€é¢„åˆ†è¯ï¼Œå¿«é€Ÿè½»é‡ã€‚
- **Tokenizersåº“**ï¼šæä¾›å®Œæ•´çš„ç¼–ç æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®æ ‡å‡†åŒ–ã€é¢„åˆ†è¯ã€åˆ†è¯æ¨¡å‹å’Œåå¤„ç†ï¼Œæ”¯æŒçµæ´»å®šåˆ¶ã€‚

---


## **é‡ç‚¹å†…å®¹**

### **1. SentencePiece çš„æ ¸å¿ƒç‰¹ç‚¹**
- **å¤šåˆ†è¯ç²’åº¦æ”¯æŒ**ï¼š  
  æ”¯æŒBPEï¼ˆByte Pair Encodingï¼‰ã€ULMå­è¯ç®—æ³•ï¼Œä»¥åŠå­—ç¬¦çº§ï¼ˆcharï¼‰å’Œå•è¯çº§ï¼ˆwordï¼‰åˆ†è¯ã€‚
  
- **å¤šè¯­è¨€å…¼å®¹æ€§**ï¼š  
  ä½¿ç”¨Unicodeç¼–ç å­—ç¬¦ï¼Œç»Ÿä¸€å¤„ç†ä¸åŒè¯­è¨€çš„è¾“å…¥ï¼ˆå¦‚è‹±æ–‡ä¸ä¸­æ–‡ï¼‰ï¼Œé¿å…äº†å¤šè¯­è¨€ç¼–ç å·®å¼‚é—®é¢˜ã€‚

- **ç¼–è§£ç çš„å¯é€†æ€§**ï¼š  
  é€šè¿‡æ˜¾å¼å¤„ç†ç©ºæ ¼å¹¶ç”¨ç‰¹æ®Šç¬¦å·â€œâ–â€ï¼ˆU+2581ï¼‰è½¬ä¹‰ï¼Œå®ç°äº†ç®€å•ä¸”å¯é€†çš„ç¼–è§£ç æµç¨‹ã€‚  
  ç¤ºä¾‹å…¬å¼ï¼š  
  ```
  Decode(Encode(Normalized(text))) = Normalized(text)
  ```

- **æ— éœ€é¢„åˆ†è¯ï¼ˆPre-tokenizationï¼‰**ï¼š  
  ä¸éœ€è¦å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œç›´æ¥ä»åŸå§‹æ•°æ®ä¸­è¿›è¡Œè®­ç»ƒã€‚

---


### **2. Tokenizersåº“çš„ç¼–ç æµç¨‹**
Tokenizersåº“åœ¨æ–‡æœ¬åˆ†è¯æ—¶ï¼Œé‡‡ç”¨ä»¥ä¸‹ç®¡é“åŒ–æµç¨‹ï¼š

#### âœ… **Normalizationï¼ˆæ ‡å‡†åŒ–ï¼‰**
å¯¹æ–‡æœ¬è¿›è¡Œæ¸…ç†å’Œæ ¼å¼åŒ–ï¼ŒåŒ…æ‹¬ï¼š
- åˆ é™¤å¤šä½™ç©ºæ ¼
- å»é™¤å˜éŸ³ç¬¦å·
- è½¬æ¢ä¸ºå°å†™
- Unicodeæ­£è§„åŒ–

ä»£ç ç¤ºä¾‹ï¼š

```python
from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents, Lowercase

# å®šä¹‰ä¸€ä¸ªnormalizer
normalizer = normalizers.Sequence([
    NFD(),           # Unicodeæ­£è§„åŒ–
    StripAccents(),  # å»é™¤å˜éŸ³ç¬¦å·
    Lowercase()      # è½¬å°å†™
])

# å¯¹å­—ç¬¦ä¸²è¿›è¡Œæ ‡å‡†åŒ–
output = normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?")
print(output)  # Output: 'hello how are u?'
```


#### âš ï¸ **Pre-tokenizationï¼ˆé¢„åˆ†è¯ï¼‰**
å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ›´å°çš„å•ä½ï¼Œä¾‹å¦‚å•è¯æˆ–å­è¯ã€‚  
ç¤ºä¾‹ï¼š

```python
from tokenizers.pre_tokenizers import Whitespace

pre_tokenizer = Whitespace()
output = pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")
print(output)
# Output: [('Hello', (0, 5)), ('!', (5, 6)), ('How', (7, 10)), ...]
```


#### â—ï¸ **æ¨¡å‹åˆ†è¯ä¸åå¤„ç†**
- ä½¿ç”¨å…·ä½“åˆ†è¯ç®—æ³•ï¼ˆå¦‚BPEï¼‰ã€‚
- æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚[CLS]ã€[SEP]ï¼‰ä»¥æ»¡è¶³ç‰¹å®šä»»åŠ¡éœ€æ±‚ã€‚

---


### **3. å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹**
> âš ï¸ **å¸¸è§é”™è¯¯**ï¼š
> - å¿½ç•¥æ–‡æœ¬æ ‡å‡†åŒ–æ­¥éª¤å¯èƒ½å¯¼è‡´ç¼–ç ä¸ä¸€è‡´ã€‚
> - æœªæ­£ç¡®è®¾ç½®é¢„åˆ†è¯å™¨æˆ–æ¨¡å‹å‚æ•°ä¼šå½±å“æœ€ç»ˆåˆ†è¯æ•ˆæœã€‚
> - å¯¹å¤šè¯­è¨€æ–‡æœ¬æœªä½¿ç”¨Unicodeå¤„ç†ï¼Œå¯èƒ½å¯¼è‡´å­—ç¬¦ç¼–ç é—®é¢˜ã€‚

---


## **ä½œè€…è§‚ç‚¹ vs ä¸ªäººè§‚ç‚¹**
| **ä½œè€…è§‚ç‚¹** | **ä¸ªäººè§‚ç‚¹** |
|--------------|--------------|
| SentencePieceæ— éœ€é¢„åˆ†è¯ï¼Œé€‚åˆä»é›¶å¼€å§‹è®­ç»ƒ | SentencePieceå¯¹å°å‹æ•°æ®é›†å¯èƒ½ä¸å¤Ÿé«˜æ•ˆ |
| Tokenizersåº“çµæ´»å¯æ‰©å±•ï¼Œé€‚åˆå¤šä»»åŠ¡åº”ç”¨ | Tokenizersåº“çš„å­¦ä¹ æ›²çº¿ç¨é™¡ï¼Œéœ€è¦è‰¯å¥½ç¼–ç¨‹åŸºç¡€ |
| å¤šè¯­è¨€æ”¯æŒè®©å·¥å…·æ›´é€šç”¨ | å¯¹ä½èµ„æºè¯­è¨€å¯èƒ½éœ€è¦é¢å¤–ä¼˜åŒ– |

ğŸ’¡ å¯å‘ç‚¹ï¼š  
- åˆ†è¯å·¥å…·çš„é€‰æ‹©åº”æ ¹æ®å…·ä½“ä»»åŠ¡éœ€æ±‚å’Œæ•°æ®è§„æ¨¡è°ƒæ•´ã€‚  
- ç¼–è§£ç çš„å¯é€†æ€§æ˜¯æå‡æ•°æ®ä¸€è‡´æ€§çš„é‡è¦ç‰¹æ€§ã€‚

---


## **è¡ŒåŠ¨æ¸…å•**
1. âœ… å­¦ä¹ å¹¶å®ç°SentencePieceçš„åŸºæœ¬ç”¨æ³•ï¼Œå°è¯•ä¸åŒåˆ†è¯ç®—æ³•ï¼ˆå¦‚BPEï¼‰ã€‚
2. âš ï¸ ä½¿ç”¨Tokenizersåº“å¯¹å¤šè¯­è¨€æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–å’Œé¢„åˆ†è¯ã€‚
3. â—ï¸ å¯¹æ¯”ä¸åŒåˆ†è¯å·¥å…·åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè®°å½•å®éªŒç»“æœã€‚

---


## ğŸ“ˆ **è¶‹åŠ¿é¢„æµ‹**
éšç€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ™®åŠï¼Œå¤šè¯­è¨€å’Œä½èµ„æºè¯­è¨€çš„æ”¯æŒå°†æˆä¸ºåˆ†è¯å·¥å…·çš„é‡è¦å‘å±•æ–¹å‘ã€‚æœªæ¥å¯èƒ½ä¼šå‡ºç°æ›´å¤šç»“åˆæ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•çš„æ–°å‹åˆ†è¯å·¥å…·ã€‚

---


## [æ€è€ƒ] å»¶ä¼¸é—®é¢˜
1. SentencePieceåœ¨å¤„ç†æ··åˆè¯­è¨€ï¼ˆå¦‚ä¸­è‹±æ··åˆï¼‰æ—¶è¡¨ç°å¦‚ä½•ï¼Ÿæ˜¯å¦éœ€è¦é¢å¤–ä¼˜åŒ–ï¼Ÿ
2. Tokenizersåº“å¦‚ä½•ä¸ä¸»æµæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚TensorFlowæˆ–PyTorchï¼‰æ— ç¼é›†æˆï¼Ÿ
3. åœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦æœ‰æ›´é€‚åˆçš„å°å‹åˆ†è¯æ¨¡å‹ï¼Ÿ

---

> å¼•ç”¨æ¥æºï¼š[SentencePieceå®˜æ–¹æ–‡æ¡£](https://github.com/google/sentencepiece)ã€[Tokenizersåº“æ–‡æ¡£](https://huggingface.co/docs/tokenizers/)
