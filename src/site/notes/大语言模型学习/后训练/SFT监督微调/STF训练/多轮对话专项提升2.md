---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-11T13:05:15.000+08:00","updated":"2025-04-13T13:06:02.000+08:00"}
---



# 多轮对话加速计算与损失函数优化

## 分类：机器学习


## 标签：多轮对话、损失函数优化、加速计算


## 日期：2023年10月30日


## 核心观点总结
本文讨论了如何通过合并多轮对话样本来加速计算，以及如何调整损失函数以避免训练不充分的问题。尤其是在不同轮次输出长度不一致的情况下，传统的损失计算方式可能导致短输出的数据训练不充分。通过调整损失计算的方法，可以有效解决这一问题。


## 重点段落
- **多轮合并加速计算**：通过合并多轮对话样本，可以利用causal attention mask实现等效计算，然而需要注意损失函数的计算问题。

- **损失函数的优化**：传统的CrossEntropyLoss在不同轮次输出长度不同时可能导致训练不充分，通过禁用默认的平均机制并调整分母为全局批次的对话轮数，可以实现更合理的损失计算。

- **最终方案**：在新版Megatron-LM框架中，提供了禁用DP和梯度累加平均的选项，并通过修改loss_func来实现精确的损失计算。


## 通俗解读
- **多轮合并加速**：简单来说，就是把多个对话合并成一个进行处理，这样可以节省时间，但要小心处理损失计算。

- **损失函数的问题**：如果不调整，短对话可能会被低估，长对话则被高估，这会影响模型的训练效果。

- **解决方案**：通过调整损失函数的计算方式，确保每个对话都能被公平地训练。


## 操作步骤
1. ✅ 合并多轮对话样本。
2. ⚠ 确保每个token只能看到前面的token。
3. ❗ 调整损失函数以避免训练不充分。


## 常见错误
> **警告**：在合并样本时，如果不调整损失函数，可能导致短对话数据训练不充分。


## 代码示例
```python
def loss_func(output_tensor, loss_mask, loss_token_num, turn_num):
    losses = output_tensor.view(-1).float()
    loss_mask = loss_mask.view(-1).float()
    loss_token_num = loss_token_num.view(-1).float()
    # label: [-100, -100, a, a, a, -100, b, b, -100, -100, c, c, c, -100, -100]
```


## 💡启发点
- 调整损失函数可以显著提高模型在多轮对话上的表现。
- 使用causal attention mask可以在不增加计算复杂度的情况下合并样本。


## 行动清单
- [ ] 研究其他加速计算的方法。
- [ ] 探索更多关于损失函数优化的技术。
- [ ] 测试不同对话长度下的模型表现。


## 数据转换
| 参数          | 说明                       |
|---------------|----------------------------|
| output_tensor | 模型输出张量               |
| loss_mask     | 损失掩码                   |
| loss_token_num| 损失token数量              |
| turn_num      | 对话轮数                   |


## 📈趋势预测
未来，随着对话系统复杂度的增加，优化多轮对话的处理效率将成为研究热点之一。


## 后续追踪
- 探索其他框架中类似功能的实现。
- 研究更多关于causal attention mask的应用场景。
