{"content":"<p><strong>分类</strong>：机器学习<br>\n<strong>标签</strong>：继续预训练、长文本、RoPE、模型优化<br>\n<strong>日期</strong>：2023年10月24日</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>长文本继续预训练是对基础大语言模型进行进一步优化的一种方法，旨在通过扩展上下文长度和增强远程注意力能力，提升模型处理长序列文本的表现。本文以CodeLlama为参考，探讨了如何通过调整模型参数和工程优化手段，实现更高效的长文本预训练。</p>\n<hr>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"_1-什么是继续预训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-什么是继续预训练\"><span>1. 什么是继续预训练？</span></a></h3>\n<p>继续预训练（Continue Pre-train）是基于现有基础模型，注入特定领域知识或针对长文本进行优化的过程。其核心在于：</p>\n<ul>\n<li><strong>领域继续预训练</strong>：通过加入特定领域的数据（如代码、法律等），调整数据比例（如7:2:1的通用语料、领域语料和指令数据），实现知识注入。</li>\n<li><strong>长文本继续预训练</strong>：扩展模型的上下文长度（如从4096扩展至16384），优化模型在长距离依赖上的表现。</li>\n</ul>\n<hr>\n<h3 id=\"_2-长文本预训练的技术细节\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-长文本预训练的技术细节\"><span>2. 长文本预训练的技术细节</span></a></h3>\n<ul>\n<li>\n<p><strong>参数调整</strong>：</p>\n<ul>\n<li>使用RoPE（旋转位置编码）技术，将公式中的参数 $$\\theta$$ 从10000调整到1000000，减弱远距离token的衰减效应。</li>\n<li>优化注意力机制，使模型更关注长程依赖。</li>\n</ul>\n</li>\n<li>\n<p><strong>工程优化</strong>：</p>\n<ul>\n<li>引入 <strong>context parallel</strong> 并行训练机制，对输入序列长度进行切分，类似于 ring attention 和 flash attention 的原理。</li>\n<li>显著提升32k/128k长度训练的吞吐量，效率增加50%以上。</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"_3-数据与采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-数据与采样\"><span>3. 数据与采样</span></a></h3>\n<ul>\n<li>数据量：20B规模数据。</li>\n<li>数据组成：中文语料与长文本语料相结合。</li>\n<li>方法：采用CodeLlama中的NTK-Aware外推方法，提升长文本处理能力。</li>\n</ul>\n<hr>\n<h3 id=\"_4-主要步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-主要步骤\"><span>4. 主要步骤</span></a></h3>\n<ol>\n<li>✅ <strong>数据准备</strong>：\n<ul>\n<li>收集并清洗通用语料、领域语料及指令数据。</li>\n<li>确保数据比例合理（如7:2:1）。</li>\n</ul>\n</li>\n<li>⚠ <strong>参数调整</strong>：\n<ul>\n<li>调整RoPE参数以减少远距离token衰减。</li>\n<li>扩展上下文长度至目标值（如16384）。</li>\n</ul>\n</li>\n<li>❗ <strong>工程优化</strong>：\n<ul>\n<li>实现context parallel机制，提升训练效率。</li>\n<li>针对长文本输入切分序列，减少计算压力。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：</p>\n<ul>\n<li>数据配比不合理可能导致领域知识注入不足或模型泛化能力下降。</li>\n<li>参数调整过于激进可能引发模型不稳定性。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>调整RoPE参数对远程注意力的影响为模型优化提供了新思路。</li>\n<li>使用 context parallel 提升训练效率，为大规模长文本预训练提供了工程上的可行性。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>收集更多领域数据，尝试不同领域的继续预训练。</li>\n<li>实验不同RoPE参数值对长文本处理效果的影响。</li>\n<li>探索其他并行训练机制以进一步提升效率。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<ul>\n<li>长文本预训练将成为大语言模型优化的重要方向，尤其在需要处理复杂上下文的应用场景中（如法律分析、代码生成）。</li>\n<li>类似context parallel的并行优化技术有望在未来进一步推广到其他领域。</li>\n</ul>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ol>\n<li>调研其他增强长文本处理能力的方法，如混合注意力机制。</li>\n<li>探索RoPE与其他位置编码方法的融合效果。</li>\n<li>跟进CodeLlama的最新成果及其在实际应用中的表现。</li>\n</ol>\n<hr>\n<p>[思考]</p>\n<ol>\n<li>如何平衡长文本处理能力与计算资源消耗？</li>\n<li>在不同领域中，数据比例如何影响继续预训练的效果？</li>\n<li>除了RoPE，还有哪些位置编码方法适合长文本优化？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：CodeLlama, Effective Long-Context Scaling of Foundation Models, YaRN: Efficient Context Window Extension of Large Language Models</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Pre-training 预训练/继续预训练 2.md","filePathRelative":"notes_bak/大语言模型学习/Pre-training 预训练/继续预训练 2.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/继续预训练","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/继续预训练/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-10T09:54:40.000Z","updated":"2025-04-13T05:06:02.000Z","title":"继续预训练","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p><strong>分类</strong>：机器学习<br>\n<strong>标签</strong>：继续预训练、长文本、RoPE、模型优化<br>\n<strong>日期</strong>：2023年10月24日</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>长文本继续预训练是对基础大语言模型进行进一步优化的一种方法，旨在通过扩展上下文长度和增强远程注意力能力，提升模型处理长序列文本的表现。本文以CodeLlama为参考，探讨了如何通过调整模型参数和工程优化手段，实现更高效的长文本预训练。</p>\n<hr>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"_1-什么是继续预训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-什么是继续预训练\"><span>1. 什么是继续预训练？</span></a></h3>\n<p>继续预训练（Continue Pre-train）是基于现有基础模型，注入特定领域知识或针对长文本进行优化的过程。其核心在于：</p>\n<ul>\n<li><strong>领域继续预训练</strong>：通过加入特定领域的数据（如代码、法律等），调整数据比例（如7:2:1的通用语料、领域语料和指令数据），实现知识注入。</li>\n<li><strong>长文本继续预训练</strong>：扩展模型的上下文长度（如从4096扩展至16384），优化模型在长距离依赖上的表现。</li>\n</ul>\n<hr>\n<h3 id=\"_2-长文本预训练的技术细节\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-长文本预训练的技术细节\"><span>2. 长文本预训练的技术细节</span></a></h3>\n<ul>\n<li>\n<p><strong>参数调整</strong>：</p>\n<ul>\n<li>使用RoPE（旋转位置编码）技术，将公式中的参数 $$\\theta$$ 从10000调整到1000000，减弱远距离token的衰减效应。</li>\n<li>优化注意力机制，使模型更关注长程依赖。</li>\n</ul>\n</li>\n<li>\n<p><strong>工程优化</strong>：</p>\n<ul>\n<li>引入 <strong>context parallel</strong> 并行训练机制，对输入序列长度进行切分，类似于 ring attention 和 flash attention 的原理。</li>\n<li>显著提升32k/128k长度训练的吞吐量，效率增加50%以上。</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"_3-数据与采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-数据与采样\"><span>3. 数据与采样</span></a></h3>\n<ul>\n<li>数据量：20B规模数据。</li>\n<li>数据组成：中文语料与长文本语料相结合。</li>\n<li>方法：采用CodeLlama中的NTK-Aware外推方法，提升长文本处理能力。</li>\n</ul>\n<hr>\n<h3 id=\"_4-主要步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-主要步骤\"><span>4. 主要步骤</span></a></h3>\n<ol>\n<li>✅ <strong>数据准备</strong>：\n<ul>\n<li>收集并清洗通用语料、领域语料及指令数据。</li>\n<li>确保数据比例合理（如7:2:1）。</li>\n</ul>\n</li>\n<li>⚠ <strong>参数调整</strong>：\n<ul>\n<li>调整RoPE参数以减少远距离token衰减。</li>\n<li>扩展上下文长度至目标值（如16384）。</li>\n</ul>\n</li>\n<li>❗ <strong>工程优化</strong>：\n<ul>\n<li>实现context parallel机制，提升训练效率。</li>\n<li>针对长文本输入切分序列，减少计算压力。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：</p>\n<ul>\n<li>数据配比不合理可能导致领域知识注入不足或模型泛化能力下降。</li>\n<li>参数调整过于激进可能引发模型不稳定性。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>调整RoPE参数对远程注意力的影响为模型优化提供了新思路。</li>\n<li>使用 context parallel 提升训练效率，为大规模长文本预训练提供了工程上的可行性。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>收集更多领域数据，尝试不同领域的继续预训练。</li>\n<li>实验不同RoPE参数值对长文本处理效果的影响。</li>\n<li>探索其他并行训练机制以进一步提升效率。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<ul>\n<li>长文本预训练将成为大语言模型优化的重要方向，尤其在需要处理复杂上下文的应用场景中（如法律分析、代码生成）。</li>\n<li>类似context parallel的并行优化技术有望在未来进一步推广到其他领域。</li>\n</ul>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ol>\n<li>调研其他增强长文本处理能力的方法，如混合注意力机制。</li>\n<li>探索RoPE与其他位置编码方法的融合效果。</li>\n<li>跟进CodeLlama的最新成果及其在实际应用中的表现。</li>\n</ol>\n<hr>\n<p>[思考]</p>\n<ol>\n<li>如何平衡长文本处理能力与计算资源消耗？</li>\n<li>在不同领域中，数据比例如何影响继续预训练的效果？</li>\n<li>除了RoPE，还有哪些位置编码方法适合长文本优化？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：CodeLlama, Effective Long-Context Scaling of Foundation Models, YaRN: Efficient Context Window Extension of Large Language Models</p>\n</blockquote>\n</template>","contentStripped":"<p><strong>分类</strong>：机器学习<br>\n<strong>标签</strong>：继续预训练、长文本、RoPE、模型优化<br>\n<strong>日期</strong>：2023年10月24日</p>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>长文本继续预训练是对基础大语言模型进行进一步优化的一种方法，旨在通过扩展上下文长度和增强远程注意力能力，提升模型处理长序列文本的表现。本文以CodeLlama为参考，探讨了如何通过调整模型参数和工程优化手段，实现更高效的长文本预训练。</p>\n<hr>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"_1-什么是继续预训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-什么是继续预训练\"><span>1. 什么是继续预训练？</span></a></h3>\n<p>继续预训练（Continue Pre-train）是基于现有基础模型，注入特定领域知识或针对长文本进行优化的过程。其核心在于：</p>\n<ul>\n<li><strong>领域继续预训练</strong>：通过加入特定领域的数据（如代码、法律等），调整数据比例（如7:2:1的通用语料、领域语料和指令数据），实现知识注入。</li>\n<li><strong>长文本继续预训练</strong>：扩展模型的上下文长度（如从4096扩展至16384），优化模型在长距离依赖上的表现。</li>\n</ul>\n<hr>\n<h3 id=\"_2-长文本预训练的技术细节\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-长文本预训练的技术细节\"><span>2. 长文本预训练的技术细节</span></a></h3>\n<ul>\n<li>\n<p><strong>参数调整</strong>：</p>\n<ul>\n<li>使用RoPE（旋转位置编码）技术，将公式中的参数 $$\\theta$$ 从10000调整到1000000，减弱远距离token的衰减效应。</li>\n<li>优化注意力机制，使模型更关注长程依赖。</li>\n</ul>\n</li>\n<li>\n<p><strong>工程优化</strong>：</p>\n<ul>\n<li>引入 <strong>context parallel</strong> 并行训练机制，对输入序列长度进行切分，类似于 ring attention 和 flash attention 的原理。</li>\n<li>显著提升32k/128k长度训练的吞吐量，效率增加50%以上。</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"_3-数据与采样\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-数据与采样\"><span>3. 数据与采样</span></a></h3>\n<ul>\n<li>数据量：20B规模数据。</li>\n<li>数据组成：中文语料与长文本语料相结合。</li>\n<li>方法：采用CodeLlama中的NTK-Aware外推方法，提升长文本处理能力。</li>\n</ul>\n<hr>\n<h3 id=\"_4-主要步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-主要步骤\"><span>4. 主要步骤</span></a></h3>\n<ol>\n<li>✅ <strong>数据准备</strong>：\n<ul>\n<li>收集并清洗通用语料、领域语料及指令数据。</li>\n<li>确保数据比例合理（如7:2:1）。</li>\n</ul>\n</li>\n<li>⚠ <strong>参数调整</strong>：\n<ul>\n<li>调整RoPE参数以减少远距离token衰减。</li>\n<li>扩展上下文长度至目标值（如16384）。</li>\n</ul>\n</li>\n<li>❗ <strong>工程优化</strong>：\n<ul>\n<li>实现context parallel机制，提升训练效率。</li>\n<li>针对长文本输入切分序列，减少计算压力。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：</p>\n<ul>\n<li>数据配比不合理可能导致领域知识注入不足或模型泛化能力下降。</li>\n<li>参数调整过于激进可能引发模型不稳定性。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>调整RoPE参数对远程注意力的影响为模型优化提供了新思路。</li>\n<li>使用 context parallel 提升训练效率，为大规模长文本预训练提供了工程上的可行性。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>收集更多领域数据，尝试不同领域的继续预训练。</li>\n<li>实验不同RoPE参数值对长文本处理效果的影响。</li>\n<li>探索其他并行训练机制以进一步提升效率。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<ul>\n<li>长文本预训练将成为大语言模型优化的重要方向，尤其在需要处理复杂上下文的应用场景中（如法律分析、代码生成）。</li>\n<li>类似context parallel的并行优化技术有望在未来进一步推广到其他领域。</li>\n</ul>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ol>\n<li>调研其他增强长文本处理能力的方法，如混合注意力机制。</li>\n<li>探索RoPE与其他位置编码方法的融合效果。</li>\n<li>跟进CodeLlama的最新成果及其在实际应用中的表现。</li>\n</ol>\n<hr>\n<p>[思考]</p>\n<ol>\n<li>如何平衡长文本处理能力与计算资源消耗？</li>\n<li>在不同领域中，数据比例如何影响继续预训练的效果？</li>\n<li>除了RoPE，还有哪些位置编码方法适合长文本优化？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：CodeLlama, Effective Long-Context Scaling of Foundation Models, YaRN: Efficient Context Window Extension of Large Language Models</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"**分类**：机器学习  \n**标签**：继续预训练、长文本、RoPE、模型优化  \n**日期**：2023年10月24日  \n\n---\n\n## 核心观点总结\n长文本继续预训练是对基础大语言模型进行进一步优化的一种方法，旨在通过扩展上下文长度和增强远程注意力能力，提升模型处理长序列文本的表现。本文以CodeLlama为参考，探讨了如何通过调整模型参数和工程优化手段，实现更高效的长文本预训练。\n\n---\n\n\n## 重点内容\n\n### 1. 什么是继续预训练？\n继续预训练（Continue Pre-train）是基于现有基础模型，注入特定领域知识或针对长文本进行优化的过程。其核心在于：\n- **领域继续预训练**：通过加入特定领域的数据（如代码、法律等），调整数据比例（如7:2:1的通用语料、领域语料和指令数据），实现知识注入。\n- **长文本继续预训练**：扩展模型的上下文长度（如从4096扩展至16384），优化模型在长距离依赖上的表现。\n\n---\n\n\n### 2. 长文本预训练的技术细节\n- **参数调整**：\n  - 使用RoPE（旋转位置编码）技术，将公式中的参数 $$\\theta$$ 从10000调整到1000000，减弱远距离token的衰减效应。\n  - 优化注意力机制，使模型更关注长程依赖。\n  \n- **工程优化**：\n  - 引入 **context parallel** 并行训练机制，对输入序列长度进行切分，类似于 ring attention 和 flash attention 的原理。\n  - 显著提升32k/128k长度训练的吞吐量，效率增加50%以上。\n\n---\n\n\n### 3. 数据与采样\n- 数据量：20B规模数据。\n- 数据组成：中文语料与长文本语料相结合。\n- 方法：采用CodeLlama中的NTK-Aware外推方法，提升长文本处理能力。\n\n---\n\n\n### 4. 主要步骤\n1. ✅ **数据准备**：\n   - 收集并清洗通用语料、领域语料及指令数据。\n   - 确保数据比例合理（如7:2:1）。\n2. ⚠ **参数调整**：\n   - 调整RoPE参数以减少远距离token衰减。\n   - 扩展上下文长度至目标值（如16384）。\n3. ❗ **工程优化**：\n   - 实现context parallel机制，提升训练效率。\n   - 针对长文本输入切分序列，减少计算压力。\n\n---\n\n\n## 常见错误\n> **警告**：  \n> - 数据配比不合理可能导致领域知识注入不足或模型泛化能力下降。  \n> - 参数调整过于激进可能引发模型不稳定性。\n\n---\n\n\n## 💡启发点\n- 调整RoPE参数对远程注意力的影响为模型优化提供了新思路。\n- 使用 context parallel 提升训练效率，为大规模长文本预训练提供了工程上的可行性。\n\n---\n\n\n## 行动清单\n1. 收集更多领域数据，尝试不同领域的继续预训练。\n2. 实验不同RoPE参数值对长文本处理效果的影响。\n3. 探索其他并行训练机制以进一步提升效率。\n\n---\n\n\n## 📈趋势预测\n- 长文本预训练将成为大语言模型优化的重要方向，尤其在需要处理复杂上下文的应用场景中（如法律分析、代码生成）。\n- 类似context parallel的并行优化技术有望在未来进一步推广到其他领域。\n\n---\n\n\n## 后续追踪\n1. 调研其他增强长文本处理能力的方法，如混合注意力机制。\n2. 探索RoPE与其他位置编码方法的融合效果。\n3. 跟进CodeLlama的最新成果及其在实际应用中的表现。\n\n---\n\n[思考]  \n1. 如何平衡长文本处理能力与计算资源消耗？  \n2. 在不同领域中，数据比例如何影响继续预训练的效果？  \n3. 除了RoPE，还有哪些位置编码方法适合长文本优化？\n\n---\n\n> 原始出处：CodeLlama, Effective Long-Context Scaling of Foundation Models, YaRN: Efficient Context Window Extension of Large Language Models","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点内容","slug":"重点内容","link":"#重点内容","children":[{"level":3,"title":"1. 什么是继续预训练？","slug":"_1-什么是继续预训练","link":"#_1-什么是继续预训练","children":[]},{"level":3,"title":"2. 长文本预训练的技术细节","slug":"_2-长文本预训练的技术细节","link":"#_2-长文本预训练的技术细节","children":[]},{"level":3,"title":"3. 数据与采样","slug":"_3-数据与采样","link":"#_3-数据与采样","children":[]},{"level":3,"title":"4. 主要步骤","slug":"_4-主要步骤","link":"#_4-主要步骤","children":[]}]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
