{"content":"<p>分类：人工智能，深度学习</p>\n<p>标签：推理机制，矩阵计算，时延优化</p>\n<p>日期：2023年10月25日</p>\n<h2 id=\"推理机制概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#推理机制概述\"><span>推理机制概述</span></a></h2>\n<p>传统的推理方式采用逐个token生成的方法，无法并行化处理。推理过程主要包括：</p>\n<ul>\n<li><strong>矩阵-向量乘法</strong>：一个大矩阵（如8192x8192）与一个向量相乘，得到另一个向量。</li>\n<li><strong>Attention计算</strong>：通过KV-cache进行推理。</li>\n</ul>\n<h3 id=\"关键瓶颈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关键瓶颈\"><span>关键瓶颈</span></a></h3>\n<ul>\n<li>矩阵-向量乘法的浮点运算：每个矩阵元素执行一次乘加运算（2 FLOPs）。</li>\n<li>Attention计算需要对每个key和value执行一次乘加。</li>\n</ul>\n<h2 id=\"时延计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#时延计算\"><span>时延计算</span></a></h2>\n<h3 id=\"数据量分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据量分析\"><span>数据量分析</span></a></h3>\n<p>在模型使用FP16格式时，每生成一个token需要加载的数据总量为14.2 GB。虽然下一个token生成时可以复用每个矩阵，但由于硬件缓存大小有限，速度受限于显存带宽。</p>\n<h3 id=\"kv-cache读取\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#kv-cache读取\"><span>KV-cache读取</span></a></h3>\n<p>假设是7B大小的LLM，每个token对应的KV-cache数据量为130KB。例如，第1000个token需要读取130MB的数据，与总数据量14.2GB相比，这部分影响可以忽略不计。</p>\n<h3 id=\"计算时延\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算时延\"><span>计算时延</span></a></h3>\n<p>在NVIDIA RTX 4090上，读取14.2GB (FP16)数据需要约14.1ms，因此每个位置靠前的token大约需要14.1ms。使用8bit权重则需7.0ms。这是生成每个token的理论最小时间。</p>\n<h2 id=\"公式总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#公式总结\"><span>公式总结</span></a></h2>\n<p>模型的预测时间可以近似为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>k</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = kx + b\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span></p>\n<p>其中，$$b$$是首个token的耗时，$$k$$是后续每个token的耗时，$$x$$是生成token的总数量。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠️ 在进行推理机制优化时，容易忽视KV-cache对整体速度的微小影响，导致错误评估性能瓶颈。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>矩阵-向量乘法和attention计算是推理过程中的核心操作，优化这些步骤可以显著提升模型性能。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更高效的矩阵计算方法。</li>\n<li>优化KV-cache使用策略以减少不必要的数据读取。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，大模型的推理速度将继续提升，通过硬件和算法的协同优化实现更快的响应时间。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>研究新型硬件架构对推理速度的影响。</li>\n<li>探索更高效的attention机制以进一步减少推理时延。</li>\n</ul>\n<blockquote>\n<p>来源：《LLM inference speed of light》</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Pre-training 预训练/推理耗时.md","filePathRelative":"notes_bak/大语言模型学习/Pre-training 预训练/推理耗时.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/推理耗时","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/推理耗时/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-10T13:47:51.000Z","updated":"2025-04-13T05:06:02.000Z","title":"推理耗时","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>分类：人工智能，深度学习</p>\n<p>标签：推理机制，矩阵计算，时延优化</p>\n<p>日期：2023年10月25日</p>\n<h2 id=\"推理机制概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#推理机制概述\"><span>推理机制概述</span></a></h2>\n<p>传统的推理方式采用逐个token生成的方法，无法并行化处理。推理过程主要包括：</p>\n<ul>\n<li><strong>矩阵-向量乘法</strong>：一个大矩阵（如8192x8192）与一个向量相乘，得到另一个向量。</li>\n<li><strong>Attention计算</strong>：通过KV-cache进行推理。</li>\n</ul>\n<h3 id=\"关键瓶颈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关键瓶颈\"><span>关键瓶颈</span></a></h3>\n<ul>\n<li>矩阵-向量乘法的浮点运算：每个矩阵元素执行一次乘加运算（2 FLOPs）。</li>\n<li>Attention计算需要对每个key和value执行一次乘加。</li>\n</ul>\n<h2 id=\"时延计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#时延计算\"><span>时延计算</span></a></h2>\n<h3 id=\"数据量分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据量分析\"><span>数据量分析</span></a></h3>\n<p>在模型使用FP16格式时，每生成一个token需要加载的数据总量为14.2 GB。虽然下一个token生成时可以复用每个矩阵，但由于硬件缓存大小有限，速度受限于显存带宽。</p>\n<h3 id=\"kv-cache读取\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#kv-cache读取\"><span>KV-cache读取</span></a></h3>\n<p>假设是7B大小的LLM，每个token对应的KV-cache数据量为130KB。例如，第1000个token需要读取130MB的数据，与总数据量14.2GB相比，这部分影响可以忽略不计。</p>\n<h3 id=\"计算时延\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算时延\"><span>计算时延</span></a></h3>\n<p>在NVIDIA RTX 4090上，读取14.2GB (FP16)数据需要约14.1ms，因此每个位置靠前的token大约需要14.1ms。使用8bit权重则需7.0ms。这是生成每个token的理论最小时间。</p>\n<h2 id=\"公式总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#公式总结\"><span>公式总结</span></a></h2>\n<p>模型的预测时间可以近似为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>k</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = kx + b\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span></p>\n<p>其中，$$b$$是首个token的耗时，$$k$$是后续每个token的耗时，$$x$$是生成token的总数量。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠️ 在进行推理机制优化时，容易忽视KV-cache对整体速度的微小影响，导致错误评估性能瓶颈。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>矩阵-向量乘法和attention计算是推理过程中的核心操作，优化这些步骤可以显著提升模型性能。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更高效的矩阵计算方法。</li>\n<li>优化KV-cache使用策略以减少不必要的数据读取。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，大模型的推理速度将继续提升，通过硬件和算法的协同优化实现更快的响应时间。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>研究新型硬件架构对推理速度的影响。</li>\n<li>探索更高效的attention机制以进一步减少推理时延。</li>\n</ul>\n<blockquote>\n<p>来源：《LLM inference speed of light》</p>\n</blockquote>\n</template>","contentStripped":"<p>分类：人工智能，深度学习</p>\n<p>标签：推理机制，矩阵计算，时延优化</p>\n<p>日期：2023年10月25日</p>\n<h2 id=\"推理机制概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#推理机制概述\"><span>推理机制概述</span></a></h2>\n<p>传统的推理方式采用逐个token生成的方法，无法并行化处理。推理过程主要包括：</p>\n<ul>\n<li><strong>矩阵-向量乘法</strong>：一个大矩阵（如8192x8192）与一个向量相乘，得到另一个向量。</li>\n<li><strong>Attention计算</strong>：通过KV-cache进行推理。</li>\n</ul>\n<h3 id=\"关键瓶颈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#关键瓶颈\"><span>关键瓶颈</span></a></h3>\n<ul>\n<li>矩阵-向量乘法的浮点运算：每个矩阵元素执行一次乘加运算（2 FLOPs）。</li>\n<li>Attention计算需要对每个key和value执行一次乘加。</li>\n</ul>\n<h2 id=\"时延计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#时延计算\"><span>时延计算</span></a></h2>\n<h3 id=\"数据量分析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据量分析\"><span>数据量分析</span></a></h3>\n<p>在模型使用FP16格式时，每生成一个token需要加载的数据总量为14.2 GB。虽然下一个token生成时可以复用每个矩阵，但由于硬件缓存大小有限，速度受限于显存带宽。</p>\n<h3 id=\"kv-cache读取\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#kv-cache读取\"><span>KV-cache读取</span></a></h3>\n<p>假设是7B大小的LLM，每个token对应的KV-cache数据量为130KB。例如，第1000个token需要读取130MB的数据，与总数据量14.2GB相比，这部分影响可以忽略不计。</p>\n<h3 id=\"计算时延\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#计算时延\"><span>计算时延</span></a></h3>\n<p>在NVIDIA RTX 4090上，读取14.2GB (FP16)数据需要约14.1ms，因此每个位置靠前的token大约需要14.1ms。使用8bit权重则需7.0ms。这是生成每个token的理论最小时间。</p>\n<h2 id=\"公式总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#公式总结\"><span>公式总结</span></a></h2>\n<p>模型的预测时间可以近似为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>k</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = kx + b\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span></p>\n<p>其中，$$b$$是首个token的耗时，$$k$$是后续每个token的耗时，$$x$$是生成token的总数量。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠️ 在进行推理机制优化时，容易忽视KV-cache对整体速度的微小影响，导致错误评估性能瓶颈。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>矩阵-向量乘法和attention计算是推理过程中的核心操作，优化这些步骤可以显著提升模型性能。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更高效的矩阵计算方法。</li>\n<li>优化KV-cache使用策略以减少不必要的数据读取。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，大模型的推理速度将继续提升，通过硬件和算法的协同优化实现更快的响应时间。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>研究新型硬件架构对推理速度的影响。</li>\n<li>探索更高效的attention机制以进一步减少推理时延。</li>\n</ul>\n<blockquote>\n<p>来源：《LLM inference speed of light》</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"分类：人工智能，深度学习\n\n标签：推理机制，矩阵计算，时延优化\n\n日期：2023年10月25日\n\n## 推理机制概述\n传统的推理方式采用逐个token生成的方法，无法并行化处理。推理过程主要包括：\n\n- **矩阵-向量乘法**：一个大矩阵（如8192x8192）与一个向量相乘，得到另一个向量。\n- **Attention计算**：通过KV-cache进行推理。\n\n### 关键瓶颈\n- 矩阵-向量乘法的浮点运算：每个矩阵元素执行一次乘加运算（2 FLOPs）。\n- Attention计算需要对每个key和value执行一次乘加。\n\n\n## 时延计算\n\n### 数据量分析\n在模型使用FP16格式时，每生成一个token需要加载的数据总量为14.2 GB。虽然下一个token生成时可以复用每个矩阵，但由于硬件缓存大小有限，速度受限于显存带宽。\n\n\n### KV-cache读取\n假设是7B大小的LLM，每个token对应的KV-cache数据量为130KB。例如，第1000个token需要读取130MB的数据，与总数据量14.2GB相比，这部分影响可以忽略不计。\n\n\n### 计算时延\n在NVIDIA RTX 4090上，读取14.2GB (FP16)数据需要约14.1ms，因此每个位置靠前的token大约需要14.1ms。使用8bit权重则需7.0ms。这是生成每个token的理论最小时间。\n\n\n## 公式总结\n模型的预测时间可以近似为：\n$$\ny = kx + b\n$$\n其中，$$b$$是首个token的耗时，$$k$$是后续每个token的耗时，$$x$$是生成token的总数量。\n\n\n## 常见错误\n> ⚠️ 在进行推理机制优化时，容易忽视KV-cache对整体速度的微小影响，导致错误评估性能瓶颈。\n\n\n## 💡启发点\n- 矩阵-向量乘法和attention计算是推理过程中的核心操作，优化这些步骤可以显著提升模型性能。\n\n\n## 行动清单\n- 探索更高效的矩阵计算方法。\n- 优化KV-cache使用策略以减少不必要的数据读取。\n\n\n## 📈趋势预测\n未来，大模型的推理速度将继续提升，通过硬件和算法的协同优化实现更快的响应时间。\n\n\n## 后续追踪\n- 研究新型硬件架构对推理速度的影响。\n- 探索更高效的attention机制以进一步减少推理时延。\n\n> 来源：《LLM inference speed of light》","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"推理机制概述","slug":"推理机制概述","link":"#推理机制概述","children":[{"level":3,"title":"关键瓶颈","slug":"关键瓶颈","link":"#关键瓶颈","children":[]}]},{"level":2,"title":"时延计算","slug":"时延计算","link":"#时延计算","children":[{"level":3,"title":"数据量分析","slug":"数据量分析","link":"#数据量分析","children":[]},{"level":3,"title":"KV-cache读取","slug":"kv-cache读取","link":"#kv-cache读取","children":[]},{"level":3,"title":"计算时延","slug":"计算时延","link":"#计算时延","children":[]}]},{"level":2,"title":"公式总结","slug":"公式总结","link":"#公式总结","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
