{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：机器学习</li>\n<li><strong>标签</strong>：监督微调, 预训练, 模型优化, 数据处理</li>\n<li><strong>日期</strong>：2023年10月22日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>监督微调（SFT）与预训练（pretrain）在训练方式上无区别，但在数据处理和训练目的上存在显著差异。SFT数据不需要拼接，使用特殊标记符构造知识，并且强调指令遵循能力，而非知识注入。预训练主要是知识的学习，而SFT则是应用这些知识。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ol>\n<li>\n<p><strong>数据组成形式</strong>：</p>\n<ul>\n<li>预训练数据达到模型输入长度上限，需拼接。</li>\n<li>SFT数据保持原始长度，不需拼接，使用特殊标记符来构造语义。</li>\n</ul>\n</li>\n<li>\n<p><strong>训练目标差异</strong>：</p>\n<ul>\n<li>预训练旨在知识学习。</li>\n<li>SFT专注于指令遵循能力。</li>\n</ul>\n</li>\n<li>\n<p><strong>知识注入策略</strong>：</p>\n<ul>\n<li>SFT不适合进行大规模知识注入。</li>\n<li>知识注入应采用继续预训练策略，以维持模型通用能力。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>特殊标记符（special_token）</strong>：在文本中使用的特定符号，用来标识不同角色或语义。</li>\n<li><strong>EOS标记符（eos_token）</strong>：表示文本结束的符号，帮助模型停止生成内容。</li>\n</ul>\n<h2 id=\"重点步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点步骤\"><span>重点步骤</span></a></h2>\n<ol>\n<li>✅ 确保SFT数据保持原始长度，不进行拼接。</li>\n<li>⚠ 使用特殊标记符分割角色和语义。</li>\n<li>❗ 避免在SFT阶段进行大量知识注入，保持模型的通用性。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在SFT阶段进行过多的知识注入，导致模型的通用能力下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用特殊标记符可以有效提升模型理解复杂语境的能力。</li>\n<li>适当控制知识注入比例可保持模型的多样性和灵活性。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何优化特殊标记符的使用以提升模型性能。</li>\n<li>探讨继续预训练策略在不同领域的应用效果。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着自然语言处理技术的发展，SFT将更广泛应用于需要高度精确指令执行的领域，如医疗和法律文本分析。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索SFT在多语言模型中的应用潜力。</li>\n<li>研究继续预训练策略对不同类型数据集的影响。</li>\n</ul>\n<blockquote>\n<p>来源：原文内容整理自关于监督微调与预训练的比较分析。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别.md","filePathRelative":"notes_bak/大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-10T13:55:04.000Z","updated":"2025-04-13T05:06:02.000Z","title":"监督微调与预训练的区别","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：机器学习</li>\n<li><strong>标签</strong>：监督微调, 预训练, 模型优化, 数据处理</li>\n<li><strong>日期</strong>：2023年10月22日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>监督微调（SFT）与预训练（pretrain）在训练方式上无区别，但在数据处理和训练目的上存在显著差异。SFT数据不需要拼接，使用特殊标记符构造知识，并且强调指令遵循能力，而非知识注入。预训练主要是知识的学习，而SFT则是应用这些知识。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ol>\n<li>\n<p><strong>数据组成形式</strong>：</p>\n<ul>\n<li>预训练数据达到模型输入长度上限，需拼接。</li>\n<li>SFT数据保持原始长度，不需拼接，使用特殊标记符来构造语义。</li>\n</ul>\n</li>\n<li>\n<p><strong>训练目标差异</strong>：</p>\n<ul>\n<li>预训练旨在知识学习。</li>\n<li>SFT专注于指令遵循能力。</li>\n</ul>\n</li>\n<li>\n<p><strong>知识注入策略</strong>：</p>\n<ul>\n<li>SFT不适合进行大规模知识注入。</li>\n<li>知识注入应采用继续预训练策略，以维持模型通用能力。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>特殊标记符（special_token）</strong>：在文本中使用的特定符号，用来标识不同角色或语义。</li>\n<li><strong>EOS标记符（eos_token）</strong>：表示文本结束的符号，帮助模型停止生成内容。</li>\n</ul>\n<h2 id=\"重点步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点步骤\"><span>重点步骤</span></a></h2>\n<ol>\n<li>✅ 确保SFT数据保持原始长度，不进行拼接。</li>\n<li>⚠ 使用特殊标记符分割角色和语义。</li>\n<li>❗ 避免在SFT阶段进行大量知识注入，保持模型的通用性。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在SFT阶段进行过多的知识注入，导致模型的通用能力下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用特殊标记符可以有效提升模型理解复杂语境的能力。</li>\n<li>适当控制知识注入比例可保持模型的多样性和灵活性。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何优化特殊标记符的使用以提升模型性能。</li>\n<li>探讨继续预训练策略在不同领域的应用效果。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着自然语言处理技术的发展，SFT将更广泛应用于需要高度精确指令执行的领域，如医疗和法律文本分析。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索SFT在多语言模型中的应用潜力。</li>\n<li>研究继续预训练策略对不同类型数据集的影响。</li>\n</ul>\n<blockquote>\n<p>来源：原文内容整理自关于监督微调与预训练的比较分析。</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：机器学习</li>\n<li><strong>标签</strong>：监督微调, 预训练, 模型优化, 数据处理</li>\n<li><strong>日期</strong>：2023年10月22日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>监督微调（SFT）与预训练（pretrain）在训练方式上无区别，但在数据处理和训练目的上存在显著差异。SFT数据不需要拼接，使用特殊标记符构造知识，并且强调指令遵循能力，而非知识注入。预训练主要是知识的学习，而SFT则是应用这些知识。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ol>\n<li>\n<p><strong>数据组成形式</strong>：</p>\n<ul>\n<li>预训练数据达到模型输入长度上限，需拼接。</li>\n<li>SFT数据保持原始长度，不需拼接，使用特殊标记符来构造语义。</li>\n</ul>\n</li>\n<li>\n<p><strong>训练目标差异</strong>：</p>\n<ul>\n<li>预训练旨在知识学习。</li>\n<li>SFT专注于指令遵循能力。</li>\n</ul>\n</li>\n<li>\n<p><strong>知识注入策略</strong>：</p>\n<ul>\n<li>SFT不适合进行大规模知识注入。</li>\n<li>知识注入应采用继续预训练策略，以维持模型通用能力。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>特殊标记符（special_token）</strong>：在文本中使用的特定符号，用来标识不同角色或语义。</li>\n<li><strong>EOS标记符（eos_token）</strong>：表示文本结束的符号，帮助模型停止生成内容。</li>\n</ul>\n<h2 id=\"重点步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点步骤\"><span>重点步骤</span></a></h2>\n<ol>\n<li>✅ 确保SFT数据保持原始长度，不进行拼接。</li>\n<li>⚠ 使用特殊标记符分割角色和语义。</li>\n<li>❗ 避免在SFT阶段进行大量知识注入，保持模型的通用性。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在SFT阶段进行过多的知识注入，导致模型的通用能力下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用特殊标记符可以有效提升模型理解复杂语境的能力。</li>\n<li>适当控制知识注入比例可保持模型的多样性和灵活性。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究如何优化特殊标记符的使用以提升模型性能。</li>\n<li>探讨继续预训练策略在不同领域的应用效果。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着自然语言处理技术的发展，SFT将更广泛应用于需要高度精确指令执行的领域，如医疗和法律文本分析。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索SFT在多语言模型中的应用潜力。</li>\n<li>研究继续预训练策略对不同类型数据集的影响。</li>\n</ul>\n<blockquote>\n<p>来源：原文内容整理自关于监督微调与预训练的比较分析。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 元数据\n- **分类**：机器学习\n- **标签**：监督微调, 预训练, 模型优化, 数据处理\n- **日期**：2023年10月22日\n\n\n\n## 核心观点总结\n监督微调（SFT）与预训练（pretrain）在训练方式上无区别，但在数据处理和训练目的上存在显著差异。SFT数据不需要拼接，使用特殊标记符构造知识，并且强调指令遵循能力，而非知识注入。预训练主要是知识的学习，而SFT则是应用这些知识。\n\n### 重点段落\n1. **数据组成形式**：\n   - 预训练数据达到模型输入长度上限，需拼接。\n   - SFT数据保持原始长度，不需拼接，使用特殊标记符来构造语义。\n\n2. **训练目标差异**：\n   - 预训练旨在知识学习。\n   - SFT专注于指令遵循能力。\n\n3. **知识注入策略**：\n   - SFT不适合进行大规模知识注入。\n   - 知识注入应采用继续预训练策略，以维持模型通用能力。\n\n\n\n## 技术术语通俗解释\n- **特殊标记符（special_token）**：在文本中使用的特定符号，用来标识不同角色或语义。\n- **EOS标记符（eos_token）**：表示文本结束的符号，帮助模型停止生成内容。\n\n\n\n## 重点步骤\n1. ✅ 确保SFT数据保持原始长度，不进行拼接。\n2. ⚠ 使用特殊标记符分割角色和语义。\n3. ❗ 避免在SFT阶段进行大量知识注入，保持模型的通用性。\n\n\n\n## 常见错误\n> 在SFT阶段进行过多的知识注入，导致模型的通用能力下降。\n\n\n\n## 💡启发点\n- 使用特殊标记符可以有效提升模型理解复杂语境的能力。\n- 适当控制知识注入比例可保持模型的多样性和灵活性。\n\n\n\n## 行动清单\n- 研究如何优化特殊标记符的使用以提升模型性能。\n- 探讨继续预训练策略在不同领域的应用效果。\n\n\n\n## 📈趋势预测\n随着自然语言处理技术的发展，SFT将更广泛应用于需要高度精确指令执行的领域，如医疗和法律文本分析。\n\n\n\n## 后续追踪\n- 探索SFT在多语言模型中的应用潜力。\n- 研究继续预训练策略对不同类型数据集的影响。\n\n> 来源：原文内容整理自关于监督微调与预训练的比较分析。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[{"level":3,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[]}]},{"level":2,"title":"技术术语通俗解释","slug":"技术术语通俗解释","link":"#技术术语通俗解释","children":[]},{"level":2,"title":"重点步骤","slug":"重点步骤","link":"#重点步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
