{"content":"<h2 id=\"分类-机器学习\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-机器学习\"><span>分类：机器学习</span></a></h2>\n<h2 id=\"标签-多轮对话、损失函数优化、加速计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-多轮对话、损失函数优化、加速计算\"><span>标签：多轮对话、损失函数优化、加速计算</span></a></h2>\n<h2 id=\"日期-2023年10月30日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2023年10月30日\"><span>日期：2023年10月30日</span></a></h2>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>本文讨论了如何通过合并多轮对话样本来加速计算，以及如何调整损失函数以避免训练不充分的问题。尤其是在不同轮次输出长度不一致的情况下，传统的损失计算方式可能导致短输出的数据训练不充分。通过调整损失计算的方法，可以有效解决这一问题。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ul>\n<li>\n<p><strong>多轮合并加速计算</strong>：通过合并多轮对话样本，可以利用causal attention mask实现等效计算，然而需要注意损失函数的计算问题。</p>\n</li>\n<li>\n<p><strong>损失函数的优化</strong>：传统的CrossEntropyLoss在不同轮次输出长度不同时可能导致训练不充分，通过禁用默认的平均机制并调整分母为全局批次的对话轮数，可以实现更合理的损失计算。</p>\n</li>\n<li>\n<p><strong>最终方案</strong>：在新版Megatron-LM框架中，提供了禁用DP和梯度累加平均的选项，并通过修改loss_func来实现精确的损失计算。</p>\n</li>\n</ul>\n<h2 id=\"通俗解读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通俗解读\"><span>通俗解读</span></a></h2>\n<ul>\n<li>\n<p><strong>多轮合并加速</strong>：简单来说，就是把多个对话合并成一个进行处理，这样可以节省时间，但要小心处理损失计算。</p>\n</li>\n<li>\n<p><strong>损失函数的问题</strong>：如果不调整，短对话可能会被低估，长对话则被高估，这会影响模型的训练效果。</p>\n</li>\n<li>\n<p><strong>解决方案</strong>：通过调整损失函数的计算方式，确保每个对话都能被公平地训练。</p>\n</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 合并多轮对话样本。</li>\n<li>⚠ 确保每个token只能看到前面的token。</li>\n<li>❗ 调整损失函数以避免训练不充分。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在合并样本时，如果不调整损失函数，可能导致短对话数据训练不充分。</p>\n</blockquote>\n<h2 id=\"代码示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码示例\"><span>代码示例</span></a></h2>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> loss_func</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output_tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_token_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> turn_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    losses </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> output_tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    loss_mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    loss_token_num </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_token_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">    # label: [-100, -100, a, a, a, -100, b, b, -100, -100, c, c, c, -100, -100]</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>调整损失函数可以显著提高模型在多轮对话上的表现。</li>\n<li>使用causal attention mask可以在不增加计算复杂度的情况下合并样本。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 研究其他加速计算的方法。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探索更多关于损失函数优化的技术。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同对话长度下的模型表现。</label></li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>output_tensor</td>\n<td>模型输出张量</td>\n</tr>\n<tr>\n<td>loss_mask</td>\n<td>损失掩码</td>\n</tr>\n<tr>\n<td>loss_token_num</td>\n<td>损失token数量</td>\n</tr>\n<tr>\n<td>turn_num</td>\n<td>对话轮数</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，随着对话系统复杂度的增加，优化多轮对话的处理效率将成为研究热点之一。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索其他框架中类似功能的实现。</li>\n<li>研究更多关于causal attention mask的应用场景。</li>\n</ul>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2.md","filePathRelative":"notes_bak/大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-11T05:05:15.000Z","updated":"2025-04-13T05:06:02.000Z","title":"多轮对话专项提升2","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"分类-机器学习\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-机器学习\"><span>分类：机器学习</span></a></h2>\n<h2 id=\"标签-多轮对话、损失函数优化、加速计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-多轮对话、损失函数优化、加速计算\"><span>标签：多轮对话、损失函数优化、加速计算</span></a></h2>\n<h2 id=\"日期-2023年10月30日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2023年10月30日\"><span>日期：2023年10月30日</span></a></h2>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>本文讨论了如何通过合并多轮对话样本来加速计算，以及如何调整损失函数以避免训练不充分的问题。尤其是在不同轮次输出长度不一致的情况下，传统的损失计算方式可能导致短输出的数据训练不充分。通过调整损失计算的方法，可以有效解决这一问题。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ul>\n<li>\n<p><strong>多轮合并加速计算</strong>：通过合并多轮对话样本，可以利用causal attention mask实现等效计算，然而需要注意损失函数的计算问题。</p>\n</li>\n<li>\n<p><strong>损失函数的优化</strong>：传统的CrossEntropyLoss在不同轮次输出长度不同时可能导致训练不充分，通过禁用默认的平均机制并调整分母为全局批次的对话轮数，可以实现更合理的损失计算。</p>\n</li>\n<li>\n<p><strong>最终方案</strong>：在新版Megatron-LM框架中，提供了禁用DP和梯度累加平均的选项，并通过修改loss_func来实现精确的损失计算。</p>\n</li>\n</ul>\n<h2 id=\"通俗解读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通俗解读\"><span>通俗解读</span></a></h2>\n<ul>\n<li>\n<p><strong>多轮合并加速</strong>：简单来说，就是把多个对话合并成一个进行处理，这样可以节省时间，但要小心处理损失计算。</p>\n</li>\n<li>\n<p><strong>损失函数的问题</strong>：如果不调整，短对话可能会被低估，长对话则被高估，这会影响模型的训练效果。</p>\n</li>\n<li>\n<p><strong>解决方案</strong>：通过调整损失函数的计算方式，确保每个对话都能被公平地训练。</p>\n</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 合并多轮对话样本。</li>\n<li>⚠ 确保每个token只能看到前面的token。</li>\n<li>❗ 调整损失函数以避免训练不充分。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在合并样本时，如果不调整损失函数，可能导致短对话数据训练不充分。</p>\n</blockquote>\n<h2 id=\"代码示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码示例\"><span>代码示例</span></a></h2>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> loss_func</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output_tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_token_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> turn_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    losses </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> output_tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    loss_mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    loss_token_num </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_token_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">    # label: [-100, -100, a, a, a, -100, b, b, -100, -100, c, c, c, -100, -100]</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>调整损失函数可以显著提高模型在多轮对话上的表现。</li>\n<li>使用causal attention mask可以在不增加计算复杂度的情况下合并样本。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 研究其他加速计算的方法。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探索更多关于损失函数优化的技术。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同对话长度下的模型表现。</label></li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>output_tensor</td>\n<td>模型输出张量</td>\n</tr>\n<tr>\n<td>loss_mask</td>\n<td>损失掩码</td>\n</tr>\n<tr>\n<td>loss_token_num</td>\n<td>损失token数量</td>\n</tr>\n<tr>\n<td>turn_num</td>\n<td>对话轮数</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，随着对话系统复杂度的增加，优化多轮对话的处理效率将成为研究热点之一。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索其他框架中类似功能的实现。</li>\n<li>研究更多关于causal attention mask的应用场景。</li>\n</ul>\n</template>","contentStripped":"<h2 id=\"分类-机器学习\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-机器学习\"><span>分类：机器学习</span></a></h2>\n<h2 id=\"标签-多轮对话、损失函数优化、加速计算\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-多轮对话、损失函数优化、加速计算\"><span>标签：多轮对话、损失函数优化、加速计算</span></a></h2>\n<h2 id=\"日期-2023年10月30日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2023年10月30日\"><span>日期：2023年10月30日</span></a></h2>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>本文讨论了如何通过合并多轮对话样本来加速计算，以及如何调整损失函数以避免训练不充分的问题。尤其是在不同轮次输出长度不一致的情况下，传统的损失计算方式可能导致短输出的数据训练不充分。通过调整损失计算的方法，可以有效解决这一问题。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ul>\n<li>\n<p><strong>多轮合并加速计算</strong>：通过合并多轮对话样本，可以利用causal attention mask实现等效计算，然而需要注意损失函数的计算问题。</p>\n</li>\n<li>\n<p><strong>损失函数的优化</strong>：传统的CrossEntropyLoss在不同轮次输出长度不同时可能导致训练不充分，通过禁用默认的平均机制并调整分母为全局批次的对话轮数，可以实现更合理的损失计算。</p>\n</li>\n<li>\n<p><strong>最终方案</strong>：在新版Megatron-LM框架中，提供了禁用DP和梯度累加平均的选项，并通过修改loss_func来实现精确的损失计算。</p>\n</li>\n</ul>\n<h2 id=\"通俗解读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#通俗解读\"><span>通俗解读</span></a></h2>\n<ul>\n<li>\n<p><strong>多轮合并加速</strong>：简单来说，就是把多个对话合并成一个进行处理，这样可以节省时间，但要小心处理损失计算。</p>\n</li>\n<li>\n<p><strong>损失函数的问题</strong>：如果不调整，短对话可能会被低估，长对话则被高估，这会影响模型的训练效果。</p>\n</li>\n<li>\n<p><strong>解决方案</strong>：通过调整损失函数的计算方式，确保每个对话都能被公平地训练。</p>\n</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 合并多轮对话样本。</li>\n<li>⚠ 确保每个token只能看到前面的token。</li>\n<li>❗ 调整损失函数以避免训练不充分。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在合并样本时，如果不调整损失函数，可能导致短对话数据训练不充分。</p>\n</blockquote>\n<h2 id=\"代码示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码示例\"><span>代码示例</span></a></h2>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> loss_func</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output_tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_token_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> turn_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    losses </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> output_tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    loss_mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    loss_token_num </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> loss_token_num</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">view</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">).</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">float</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">    # label: [-100, -100, a, a, a, -100, b, b, -100, -100, c, c, c, -100, -100]</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>调整损失函数可以显著提高模型在多轮对话上的表现。</li>\n<li>使用causal attention mask可以在不增加计算复杂度的情况下合并样本。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 研究其他加速计算的方法。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探索更多关于损失函数优化的技术。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同对话长度下的模型表现。</label></li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>output_tensor</td>\n<td>模型输出张量</td>\n</tr>\n<tr>\n<td>loss_mask</td>\n<td>损失掩码</td>\n</tr>\n<tr>\n<td>loss_token_num</td>\n<td>损失token数量</td>\n</tr>\n<tr>\n<td>turn_num</td>\n<td>对话轮数</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，随着对话系统复杂度的增加，优化多轮对话的处理效率将成为研究热点之一。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索其他框架中类似功能的实现。</li>\n<li>研究更多关于causal attention mask的应用场景。</li>\n</ul>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 分类：机器学习\n\n\n## 标签：多轮对话、损失函数优化、加速计算\n\n\n## 日期：2023年10月30日\n\n\n## 核心观点总结\n本文讨论了如何通过合并多轮对话样本来加速计算，以及如何调整损失函数以避免训练不充分的问题。尤其是在不同轮次输出长度不一致的情况下，传统的损失计算方式可能导致短输出的数据训练不充分。通过调整损失计算的方法，可以有效解决这一问题。\n\n\n## 重点段落\n- **多轮合并加速计算**：通过合并多轮对话样本，可以利用causal attention mask实现等效计算，然而需要注意损失函数的计算问题。\n\n- **损失函数的优化**：传统的CrossEntropyLoss在不同轮次输出长度不同时可能导致训练不充分，通过禁用默认的平均机制并调整分母为全局批次的对话轮数，可以实现更合理的损失计算。\n\n- **最终方案**：在新版Megatron-LM框架中，提供了禁用DP和梯度累加平均的选项，并通过修改loss_func来实现精确的损失计算。\n\n\n## 通俗解读\n- **多轮合并加速**：简单来说，就是把多个对话合并成一个进行处理，这样可以节省时间，但要小心处理损失计算。\n\n- **损失函数的问题**：如果不调整，短对话可能会被低估，长对话则被高估，这会影响模型的训练效果。\n\n- **解决方案**：通过调整损失函数的计算方式，确保每个对话都能被公平地训练。\n\n\n## 操作步骤\n1. ✅ 合并多轮对话样本。\n2. ⚠ 确保每个token只能看到前面的token。\n3. ❗ 调整损失函数以避免训练不充分。\n\n\n## 常见错误\n> **警告**：在合并样本时，如果不调整损失函数，可能导致短对话数据训练不充分。\n\n\n## 代码示例\n```python\ndef loss_func(output_tensor, loss_mask, loss_token_num, turn_num):\n    losses = output_tensor.view(-1).float()\n    loss_mask = loss_mask.view(-1).float()\n    loss_token_num = loss_token_num.view(-1).float()\n    # label: [-100, -100, a, a, a, -100, b, b, -100, -100, c, c, c, -100, -100]\n```\n\n\n## 💡启发点\n- 调整损失函数可以显著提高模型在多轮对话上的表现。\n- 使用causal attention mask可以在不增加计算复杂度的情况下合并样本。\n\n\n## 行动清单\n- [ ] 研究其他加速计算的方法。\n- [ ] 探索更多关于损失函数优化的技术。\n- [ ] 测试不同对话长度下的模型表现。\n\n\n## 数据转换\n| 参数          | 说明                       |\n|---------------|----------------------------|\n| output_tensor | 模型输出张量               |\n| loss_mask     | 损失掩码                   |\n| loss_token_num| 损失token数量              |\n| turn_num      | 对话轮数                   |\n\n\n## 📈趋势预测\n未来，随着对话系统复杂度的增加，优化多轮对话的处理效率将成为研究热点之一。\n\n\n## 后续追踪\n- 探索其他框架中类似功能的实现。\n- 研究更多关于causal attention mask的应用场景。","excerpt":"","includedFiles":[],"tasklistId":3,"title":"","headers":[{"level":2,"title":"分类：机器学习","slug":"分类-机器学习","link":"#分类-机器学习","children":[]},{"level":2,"title":"标签：多轮对话、损失函数优化、加速计算","slug":"标签-多轮对话、损失函数优化、加速计算","link":"#标签-多轮对话、损失函数优化、加速计算","children":[]},{"level":2,"title":"日期：2023年10月30日","slug":"日期-2023年10月30日","link":"#日期-2023年10月30日","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[]},{"level":2,"title":"通俗解读","slug":"通俗解读","link":"#通俗解读","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"代码示例","slug":"代码示例","link":"#代码示例","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"数据转换","slug":"数据转换","link":"#数据转换","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
