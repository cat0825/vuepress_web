---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/策略梯度算法","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/策略梯度算法/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-12T23:29:36.000+08:00","updated":"2025-04-13T13:06:02.000+08:00"}
---



# 策略梯度算法与REINFORCE算法详解

## 元数据
- 分类：机器学习
- 标签：策略梯度，REINFORCE，强化学习，神经网络，蒙特卡洛
- 日期：2025年4月12日


## 核心观点
策略梯度算法是一种基于策略的方法，通过对策略参数化并使用神经网络建模，输入状态输出动作的概率分布。其目标是最大化当前策略在初始状态价值函数的期望。REINFORCE是一种策略梯度算法，利用蒙特卡洛方法估计Q值。


## 重点段落

### 策略梯度算法
策略梯度算法通过对策略参数化，计算目标函数对参数的导数，并利用梯度上升方法最大化目标函数。公式如下：
$$
\nabla_\theta J(\theta) \propto \sum_{s \in S} \nu_{\pi_\theta}(s) \sum_{a \in A} Q_{\pi_\theta}(s, a) \nabla_\theta \log \pi_\theta(a|s)
$$


### REINFORCE算法
REINFORCE算法采用蒙特卡洛方法来估计Q值，其核心公式为：
$$
\nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \sum_{t=0}^{T} \left( \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'} \right) \nabla_\theta \log \pi_\theta(a_t|s_t) \right]
$$


### 算法流程
1. ✅ 初始化策略参数 $\theta$。
2. ⚠️ 对于每个序列 $e = 1$ 到 $E$：
   - 采样并更新策略。
3. ❗ 结束循环。


## 代码示例
```python
class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)

class REINFORCE:
    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device):
        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate) # 使用Adam优化器
        self.gamma = gamma # 折扣因子
        self.device = device

    def take_action(self, state): # 根据动作概率分布随机采样
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.policy_net(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        reward_list = transition_dict['rewards']
        state_list = transition_dict['states']
        action_list = transition_dict['actions']

        G = 0
        self.optimizer.zero_grad()
        for i in reversed(range(len(reward_list))): # 从最后一步算起
            reward = reward_list[i]
            state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device)
            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)
            log_prob = torch.log(self.policy_net(state).gather(1, action))
            G = self.gamma * G + reward
            loss = -log_prob * G # 每一步的损失函数
            loss.backward() # 反向传播计算梯度
            self.optimizer.step() # 梯度下降

```


## 常见错误
> 注意：在实现REINFORCE算法时，确保正确计算折扣因子 $\gamma$ 对回报的影响，以避免策略更新不准确。


## 💡启发点
策略梯度算法通过直接优化策略分布，使得在复杂环境中更容易找到最优策略。


## 📈趋势预测
随着强化学习的深入研究，基于策略的方法可能会在解决高维度问题上表现出更强的能力，尤其是在动态和不确定环境中。


## [思考]
1. 如何有效地结合策略梯度和基于值的方法以提高算法性能？
2. 在不同的应用场景中，如何选择合适的折扣因子 $\gamma$？
3. REINFORCE算法在处理连续动作空间时有哪些挑战？

> 来源：原文内容来源于相关技术文档和学习资料。
