{"content":"<h2 id=\"首token时延\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#首token时延\"><span>首Token时延</span></a></h2>\n<p>在LLM（大型语言模型）推理过程中，生成首token是一个计算密集型任务。生成首token阶段也被称为预填充阶段（prefill phase）或上下文阶段（context phase）。生成首token的时间与处理输入给大模型的Prompt的计算量有关，与Prompt长度直接相关。例如，在Prompt长度相对较长的情况下，再考虑到技术优化，使用FlashAttention2生成首token的时间与输入Prompt的长度近似成线性关系。</p>\n<h3 id=\"首个token的推理延迟\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#首个token的推理延迟\"><span>首个token的推理延迟</span></a></h3>\n<p>首个token的推理延迟可以表示为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>首个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU半精度浮点算力</mtext><mtext>模型浮点计算量 (FLOPs)</mtext></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{首个token的推理延迟} \\geq \\frac{\\text{GPU半精度浮点算力}}{\\text{模型浮点计算量 (FLOPs)}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8304em;vertical-align:-0.136em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">首个</span><span class=\"mord\">token</span><span class=\"mord cjk_fallback\">的推理延迟</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2963em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">模型浮点计算量</span><span class=\"mord\"> (FLOPs)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">GPU</span><span class=\"mord cjk_fallback\">半精度浮点算力</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h3 id=\"后续每个token的推理延迟\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续每个token的推理延迟\"><span>后续每个token的推理延迟</span></a></h3>\n<p>对于后续每个token的推理延迟，可以表示为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>后续每个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU HBM带宽</mtext><mtext>模型参数量 (字节数)</mtext></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{后续每个token的推理延迟} \\geq \\frac{\\text{GPU HBM带宽}}{\\text{模型参数量 (字节数)}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8304em;vertical-align:-0.136em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">后续每个</span><span class=\"mord\">token</span><span class=\"mord cjk_fallback\">的推理延迟</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2963em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">模型参数量</span><span class=\"mord\"> (</span><span class=\"mord cjk_fallback\">字节数</span><span class=\"mord\">)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">GPU HBM</span><span class=\"mord cjk_fallback\">带宽</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h2 id=\"优化system-prompt\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优化system-prompt\"><span>优化System Prompt</span></a></h2>\n<p>System Prompt Caching的基本思想是对System Prompt部分进行一次计算，并缓存其对应的Key和Value值（例如，存放在GPU显存中）。当LLM推理再次遇到相同的（甚至部分相同的）System Prompt时，可以直接利用已经缓存的System Prompt对应的Key和Value值，这样就避免了对于System Prompt的重复计算。</p>\n<h3 id=\"第一种形式-prefix-sharing\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#第一种形式-prefix-sharing\"><span>第一种形式：Prefix Sharing</span></a></h3>\n<p>Prefix Sharing适用于“Prompt = System Prompt + User Prompt”这样的场景，其中System Prompt就是前缀（Prefix）。\n<img src=\"/img/user/附件/Pasted image 20250430224328.png\" alt=\"Pasted image 20250430224328.png\"></p>\n<h3 id=\"第二种形式-prompt-cache\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#第二种形式-prompt-cache\"><span>第二种形式：Prompt Cache</span></a></h3>\n<p><img src=\"/img/user/附件/Pasted image 20250430224338.png\" alt=\"Pasted image 20250430224338.png\">\nPrompt Cache属于相对高级的用法，是对整个输入Prompt对应的Key和Value值进行Caching操作，不局限于共享前缀。</p>\n<p>特别地，对于多轮对话场景，以及基于LLM的AI Agent应用场景，上述第二种方式，即Prompt Cache，可以支持Session Prompt Cache。在一个多轮对话session里，输入到LLM的Prompt会携带多轮对话历史，涉及到很多重复计算。通过Session Prompt Cache可以显著减少不必要的重复计算，节省GPU资源，提高对话响应速度和用户体验。</p>\n<p>通过对首Token时延和System Prompt Caching的优化，可以有效提高LLM推理的效率，降低计算资源消耗，从而提升用户体验。这些技术在实际应用中具有重要价值，特别是在涉及大量文本处理和生成任务的场景中。</p>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/训练推理优化/推理耗时及优化/首Token时延优化.md","filePathRelative":"notes_bak/大语言模型学习/训练推理优化/推理耗时及优化/首Token时延优化.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/训练推理优化/推理耗时及优化/首Token时延优化","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/训练推理优化/推理耗时及优化/首Token时延优化/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-30T14:42:44.000Z","updated":"2025-05-06T02:29:38.000Z","title":"首Token时延优化","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"首token时延\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#首token时延\"><span>首Token时延</span></a></h2>\n<p>在LLM（大型语言模型）推理过程中，生成首token是一个计算密集型任务。生成首token阶段也被称为预填充阶段（prefill phase）或上下文阶段（context phase）。生成首token的时间与处理输入给大模型的Prompt的计算量有关，与Prompt长度直接相关。例如，在Prompt长度相对较长的情况下，再考虑到技术优化，使用FlashAttention2生成首token的时间与输入Prompt的长度近似成线性关系。</p>\n<h3 id=\"首个token的推理延迟\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#首个token的推理延迟\"><span>首个token的推理延迟</span></a></h3>\n<p>首个token的推理延迟可以表示为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>首个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU半精度浮点算力</mtext><mtext>模型浮点计算量 (FLOPs)</mtext></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{首个token的推理延迟} \\geq \\frac{\\text{GPU半精度浮点算力}}{\\text{模型浮点计算量 (FLOPs)}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8304em;vertical-align:-0.136em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">首个</span><span class=\"mord\">token</span><span class=\"mord cjk_fallback\">的推理延迟</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2963em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">模型浮点计算量</span><span class=\"mord\"> (FLOPs)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">GPU</span><span class=\"mord cjk_fallback\">半精度浮点算力</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h3 id=\"后续每个token的推理延迟\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续每个token的推理延迟\"><span>后续每个token的推理延迟</span></a></h3>\n<p>对于后续每个token的推理延迟，可以表示为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>后续每个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU HBM带宽</mtext><mtext>模型参数量 (字节数)</mtext></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{后续每个token的推理延迟} \\geq \\frac{\\text{GPU HBM带宽}}{\\text{模型参数量 (字节数)}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8304em;vertical-align:-0.136em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">后续每个</span><span class=\"mord\">token</span><span class=\"mord cjk_fallback\">的推理延迟</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2963em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">模型参数量</span><span class=\"mord\"> (</span><span class=\"mord cjk_fallback\">字节数</span><span class=\"mord\">)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">GPU HBM</span><span class=\"mord cjk_fallback\">带宽</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h2 id=\"优化system-prompt\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优化system-prompt\"><span>优化System Prompt</span></a></h2>\n<p>System Prompt Caching的基本思想是对System Prompt部分进行一次计算，并缓存其对应的Key和Value值（例如，存放在GPU显存中）。当LLM推理再次遇到相同的（甚至部分相同的）System Prompt时，可以直接利用已经缓存的System Prompt对应的Key和Value值，这样就避免了对于System Prompt的重复计算。</p>\n<h3 id=\"第一种形式-prefix-sharing\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#第一种形式-prefix-sharing\"><span>第一种形式：Prefix Sharing</span></a></h3>\n<p>Prefix Sharing适用于“Prompt = System Prompt + User Prompt”这样的场景，其中System Prompt就是前缀（Prefix）。\n<img src=\"/img/user/附件/Pasted image 20250430224328.png\" alt=\"Pasted image 20250430224328.png\"></p>\n<h3 id=\"第二种形式-prompt-cache\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#第二种形式-prompt-cache\"><span>第二种形式：Prompt Cache</span></a></h3>\n<p><img src=\"/img/user/附件/Pasted image 20250430224338.png\" alt=\"Pasted image 20250430224338.png\">\nPrompt Cache属于相对高级的用法，是对整个输入Prompt对应的Key和Value值进行Caching操作，不局限于共享前缀。</p>\n<p>特别地，对于多轮对话场景，以及基于LLM的AI Agent应用场景，上述第二种方式，即Prompt Cache，可以支持Session Prompt Cache。在一个多轮对话session里，输入到LLM的Prompt会携带多轮对话历史，涉及到很多重复计算。通过Session Prompt Cache可以显著减少不必要的重复计算，节省GPU资源，提高对话响应速度和用户体验。</p>\n<p>通过对首Token时延和System Prompt Caching的优化，可以有效提高LLM推理的效率，降低计算资源消耗，从而提升用户体验。这些技术在实际应用中具有重要价值，特别是在涉及大量文本处理和生成任务的场景中。</p>\n</template>","contentStripped":"<h2 id=\"首token时延\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#首token时延\"><span>首Token时延</span></a></h2>\n<p>在LLM（大型语言模型）推理过程中，生成首token是一个计算密集型任务。生成首token阶段也被称为预填充阶段（prefill phase）或上下文阶段（context phase）。生成首token的时间与处理输入给大模型的Prompt的计算量有关，与Prompt长度直接相关。例如，在Prompt长度相对较长的情况下，再考虑到技术优化，使用FlashAttention2生成首token的时间与输入Prompt的长度近似成线性关系。</p>\n<h3 id=\"首个token的推理延迟\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#首个token的推理延迟\"><span>首个token的推理延迟</span></a></h3>\n<p>首个token的推理延迟可以表示为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>首个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU半精度浮点算力</mtext><mtext>模型浮点计算量 (FLOPs)</mtext></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{首个token的推理延迟} \\geq \\frac{\\text{GPU半精度浮点算力}}{\\text{模型浮点计算量 (FLOPs)}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8304em;vertical-align:-0.136em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">首个</span><span class=\"mord\">token</span><span class=\"mord cjk_fallback\">的推理延迟</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2963em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">模型浮点计算量</span><span class=\"mord\"> (FLOPs)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">GPU</span><span class=\"mord cjk_fallback\">半精度浮点算力</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h3 id=\"后续每个token的推理延迟\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续每个token的推理延迟\"><span>后续每个token的推理延迟</span></a></h3>\n<p>对于后续每个token的推理延迟，可以表示为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>后续每个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU HBM带宽</mtext><mtext>模型参数量 (字节数)</mtext></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{后续每个token的推理延迟} \\geq \\frac{\\text{GPU HBM带宽}}{\\text{模型参数量 (字节数)}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8304em;vertical-align:-0.136em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">后续每个</span><span class=\"mord\">token</span><span class=\"mord cjk_fallback\">的推理延迟</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2963em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">模型参数量</span><span class=\"mord\"> (</span><span class=\"mord cjk_fallback\">字节数</span><span class=\"mord\">)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">GPU HBM</span><span class=\"mord cjk_fallback\">带宽</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h2 id=\"优化system-prompt\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#优化system-prompt\"><span>优化System Prompt</span></a></h2>\n<p>System Prompt Caching的基本思想是对System Prompt部分进行一次计算，并缓存其对应的Key和Value值（例如，存放在GPU显存中）。当LLM推理再次遇到相同的（甚至部分相同的）System Prompt时，可以直接利用已经缓存的System Prompt对应的Key和Value值，这样就避免了对于System Prompt的重复计算。</p>\n<h3 id=\"第一种形式-prefix-sharing\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#第一种形式-prefix-sharing\"><span>第一种形式：Prefix Sharing</span></a></h3>\n<p>Prefix Sharing适用于“Prompt = System Prompt + User Prompt”这样的场景，其中System Prompt就是前缀（Prefix）。\n<img src=\"/img/user/附件/Pasted image 20250430224328.png\" alt=\"Pasted image 20250430224328.png\"></p>\n<h3 id=\"第二种形式-prompt-cache\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#第二种形式-prompt-cache\"><span>第二种形式：Prompt Cache</span></a></h3>\n<p><img src=\"/img/user/附件/Pasted image 20250430224338.png\" alt=\"Pasted image 20250430224338.png\">\nPrompt Cache属于相对高级的用法，是对整个输入Prompt对应的Key和Value值进行Caching操作，不局限于共享前缀。</p>\n<p>特别地，对于多轮对话场景，以及基于LLM的AI Agent应用场景，上述第二种方式，即Prompt Cache，可以支持Session Prompt Cache。在一个多轮对话session里，输入到LLM的Prompt会携带多轮对话历史，涉及到很多重复计算。通过Session Prompt Cache可以显著减少不必要的重复计算，节省GPU资源，提高对话响应速度和用户体验。</p>\n<p>通过对首Token时延和System Prompt Caching的优化，可以有效提高LLM推理的效率，降低计算资源消耗，从而提升用户体验。这些技术在实际应用中具有重要价值，特别是在涉及大量文本处理和生成任务的场景中。</p>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 首Token时延\n在LLM（大型语言模型）推理过程中，生成首token是一个计算密集型任务。生成首token阶段也被称为预填充阶段（prefill phase）或上下文阶段（context phase）。生成首token的时间与处理输入给大模型的Prompt的计算量有关，与Prompt长度直接相关。例如，在Prompt长度相对较长的情况下，再考虑到技术优化，使用FlashAttention2生成首token的时间与输入Prompt的长度近似成线性关系。\n\n### 首个token的推理延迟\n首个token的推理延迟可以表示为：\n\n$$\n\\text{首个token的推理延迟} \\geq \\frac{\\text{GPU半精度浮点算力}}{\\text{模型浮点计算量 (FLOPs)}}\n$$\n\n\n### 后续每个token的推理延迟\n对于后续每个token的推理延迟，可以表示为：\n\n$$\n\\text{后续每个token的推理延迟} \\geq \\frac{\\text{GPU HBM带宽}}{\\text{模型参数量 (字节数)}}\n$$\n\n\n\n## 优化System Prompt\nSystem Prompt Caching的基本思想是对System Prompt部分进行一次计算，并缓存其对应的Key和Value值（例如，存放在GPU显存中）。当LLM推理再次遇到相同的（甚至部分相同的）System Prompt时，可以直接利用已经缓存的System Prompt对应的Key和Value值，这样就避免了对于System Prompt的重复计算。\n\n### 第一种形式：Prefix Sharing\nPrefix Sharing适用于“Prompt = System Prompt + User Prompt”这样的场景，其中System Prompt就是前缀（Prefix）。\n![Pasted image 20250430224328.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250430224328.png)\n\n\n### 第二种形式：Prompt Cache\n![Pasted image 20250430224338.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250430224338.png)\nPrompt Cache属于相对高级的用法，是对整个输入Prompt对应的Key和Value值进行Caching操作，不局限于共享前缀。\n\n特别地，对于多轮对话场景，以及基于LLM的AI Agent应用场景，上述第二种方式，即Prompt Cache，可以支持Session Prompt Cache。在一个多轮对话session里，输入到LLM的Prompt会携带多轮对话历史，涉及到很多重复计算。通过Session Prompt Cache可以显著减少不必要的重复计算，节省GPU资源，提高对话响应速度和用户体验。\n\n通过对首Token时延和System Prompt Caching的优化，可以有效提高LLM推理的效率，降低计算资源消耗，从而提升用户体验。这些技术在实际应用中具有重要价值，特别是在涉及大量文本处理和生成任务的场景中。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"首Token时延","slug":"首token时延","link":"#首token时延","children":[{"level":3,"title":"首个token的推理延迟","slug":"首个token的推理延迟","link":"#首个token的推理延迟","children":[]},{"level":3,"title":"后续每个token的推理延迟","slug":"后续每个token的推理延迟","link":"#后续每个token的推理延迟","children":[]}]},{"level":2,"title":"优化System Prompt","slug":"优化system-prompt","link":"#优化system-prompt","children":[{"level":3,"title":"第一种形式：Prefix Sharing","slug":"第一种形式-prefix-sharing","link":"#第一种形式-prefix-sharing","children":[]},{"level":3,"title":"第二种形式：Prompt Cache","slug":"第二种形式-prompt-cache","link":"#第二种形式-prompt-cache","children":[]}]}]}}
