{"content":"<h2 id=\"模型量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型量化\"><span>模型量化</span></a></h2>\n<p>模型量化是指在保持推理精度损失较低的情况下，将连续取值如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{float32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">float32</span></span></span></span></span>，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>float16</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{float16}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">float16</span></span></span></span></span> 的浮点型权重近似为有限多个离散值权重如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>int8</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{int8}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6679em;\"></span><span class=\"mord text\"><span class=\"mord\">int8</span></span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>int4</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{int4}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6679em;\"></span><span class=\"mord text\"><span class=\"mord\">int4</span></span></span></span></span> 的过程。通过以更少的位数表示浮点数据，模型量化可以减少模型尺寸，进而减少在推理时的内存消耗，并且在一些低精度运算较快的处理器上可以增加推理速度。</p>\n<h2 id=\"常用的数据类型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常用的数据类型\"><span>常用的数据类型</span></a></h2>\n<ul>\n<li><strong>FP32</strong>： 32位浮点数，是最常用的高精度表示方式。</li>\n<li><strong>FP16</strong>： 16位浮点数，数值范围比 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span> 小，但占用内存较少。</li>\n<li><strong>BF16</strong>： 16位截断的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span>，增加指数位，数值范围更广，常用于深度学习。</li>\n<li><strong>INT8</strong>： 8位整数，位数仅为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span> 的1/4，适用于模型参数的数据范围映射。</li>\n<li><strong>INT4</strong>： 4位整数，进一步减少位数，适用于极端资源受限的场景。</li>\n<li><strong>二值网络（Binary Network）</strong>： 1位二值网络，参数只能取0或1，计算效率极高但精度损失较大。</li>\n</ul>\n<p>工业界目前最常用的量化位数是8比特，低于8比特的量化被称为低比特量化。1比特是模型压缩的极限，可以将模型压缩为1/32，在推理时也可以使用高效的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>XNOR</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{XNOR}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">XNOR</span></span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>BitCount</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{BitCount}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">BitCount</span></span></span></span></span> 位运算来提升推理速度。</p>\n<h2 id=\"量化对象\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化对象\"><span>量化对象</span></a></h2>\n<ul>\n<li><strong>权重（Weight）</strong>：权重的量化是最常规的，可达到减少模型大小内存和占用空间。</li>\n<li><strong>激活值（Activation）</strong>： 实际中激活值往往是占内存使用的大头，量化激活值不仅可以大大减少内存占用，更重要的是，结合权重的量化可以充分利用整数计算获得性能提升。</li>\n<li><strong>KV Cache</strong>： 量化 KV 缓存对于提高长序列生成的吞吐量至关重要。</li>\n<li><strong>梯度（Gradients）</strong>： 相对上面的量化对象略微小众一些，因为主要用于训练。在训练深度学习模型时，梯度通常是浮点数，它主要作用是在分布式计算中减少通信开销，也可以减少反向传播时的开销。</li>\n</ul>\n<h1 id=\"量化形式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化形式\"><span>量化形式</span></a></h1>\n<h2 id=\"根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化\"><span>根据量化数据表示的原始数据范围是否均匀，可以将量化方法分为线性量化和非线性量化</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250501212209.png\" alt=\"Pasted image 20250501212209.png\">\n深度神经网络的权重和激活值通常是不均匀的，因此理论上使用非线性量化导致的精度损失更小，但在实际推理中非线性量化的计算复杂度较高，通常使用线性量化：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>q</mi><mo>=</mo><mtext>clip</mtext><mo stretchy=\"false\">(</mo><mtext>round</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><mo>⋅</mo><mi>r</mi><mo>+</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><msub><mi>q</mi><mi>min</mi><mo>⁡</mo></msub><mo separator=\"true\">,</mo><msub><mi>q</mi><mi>max</mi><mo>⁡</mo></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">q = \\text{clip}(\\text{round}(s \\cdot r + z), q_{\\min}, q_{\\max})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">clip</span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">round</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3175em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">i</span><span class=\"mtight\">n</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>其中 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span></span></span></span> 为量化前的浮点数，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span></span></span></span> 为量化后的整数，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>round</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{round}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">round</span></span></span></span></span> 表示取整，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>clip</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{clip}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord text\"><span class=\"mord\">clip</span></span></span></span></span> 为截断，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 为数据量化的间隔，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 为数据偏移，为0的时候为对称量化，不为0的时候为不对称量化。对称量化可以避免量化算子在推理中计算 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 相关的部分，降低推理时的计算复杂度；非对称量化可以根据实际数据的分布确定最小值和最大值，可以更加充分的利用量化数据信息，使得量化导致的损失更低。\n<img src=\"/img/user/附件/Pasted image 20250501212223.png\" alt=\"Pasted image 20250501212223.png\"></p>\n<h2 id=\"根据-和-的共享范围即量化粒度-量化方法可以进行以下分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#根据-和-的共享范围即量化粒度-量化方法可以进行以下分类\"><span>根据 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 的共享范围即量化粒度，量化方法可以进行以下分类</span></a></h2>\n<h3 id=\"逐层量化-per-tensor\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐层量化-per-tensor\"><span>逐层量化 per-tensor</span></a></h3>\n<p>范围最大，最简单，以一层网络为单位一组量化参数。\n<img src=\"/img/user/附件/Pasted image 20250501212233.png\" alt=\"Pasted image 20250501212233.png\"></p>\n<h3 id=\"逐通道量化-per-token-per-channel\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐通道量化-per-token-per-channel\"><span>逐通道量化 per-token &amp; per-channel</span></a></h3>\n<p>以一层网络的每个量化通道为单位，每个通道单独用一组量化参数。量化粒度更细，更高的量化精度，计算也更复杂。其中，per-token 针对激活 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span> 而言，每行对应一个量化系数；per-channel 针对权重 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> 而言，每列对应一个量化系数。结合使用也叫 vector-wise。\n<img src=\"/img/user/附件/Pasted image 20250501212242.png\" alt=\"Pasted image 20250501212242.png\"></p>\n<h3 id=\"逐组量化-per-group\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐组量化-per-group\"><span>逐组量化 per-group</span></a></h3>\n<p>以组为单位，每个组比如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 行激活值或 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 列权重使用一组 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span>；它的粒度处于 per-tensor 和 vector-wise之间。当 groupsize=1 时，逐组量化与逐层量化等价；当 groupsize= 卷积核的数量时，逐组量化与逐通道量化等价。</p>\n<p>此外激活值和权重可以选择不同的粒度进行量化，对于激活值来说还有动态量化（推理过程中，实时计算激活的量化系数，对激活进行量化）与静态量化（在推理前就计算好激活的量化系数，在推理过程中应用）。</p>\n<h2 id=\"llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图\"><span>Llama3 技术报告中提供的 tensor-wise 和 row-wise FP8 量化示意图</span></a></h2>\n<p>在 Llama3 技术报告中，提供了 tensor-wise 和 row-wise 的 FP8 量化示意图，这些示意图展示了不同粒度下的浮点数到整数的转换过程，帮助理解不同粒度选择对于模型性能和计算复杂度的影响。\n<img src=\"/img/user/附件/Pasted image 20250501212137.png\" alt=\"Pasted image 20250501212137.png\"></p>\n<h1 id=\"量化分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化分类\"><span>量化分类</span></a></h1>\n<p>在深度学习模型的部署中，模型的大小和推理速度是两个非常重要的因素。为了在不显著损失模型准确率的前提下减小模型大小并加快推理速度，量化技术应运而生。量化技术通过将模型参数和计算从浮点数表示转换为低精度表示（如 8 位整数）来实现这一目标。根据量化压缩模型的阶段，量化可以分为以下几类：</p>\n<h2 id=\"量化感知训练-quantization-aware-training-qat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知训练-quantization-aware-training-qat\"><span>量化感知训练 Quantization Aware Training (QAT)</span></a></h2>\n<p>量化感知训练（QAT）是在模型训练过程中引入量化的意识。具体来说，在训练过程中，尽管模型的权重和激活值仍然以浮点数形式存储和更新，但在前向传播时会模拟低精度运算。这种方法使得模型在训练过程中就能够适应量化带来的噪声，从而在部署时能够更好地保持性能。</p>\n<p>QAT 的主要优势在于其能够在保持模型准确率的同时实现显著的压缩效果。然而，由于需要在训练过程中进行额外的模拟计算，QAT 的训练时间可能会有所增加。</p>\n<h2 id=\"量化感知微调-quantization-aware-fine-tuning-qaf\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知微调-quantization-aware-fine-tuning-qaf\"><span>量化感知微调 Quantization-Aware Fine-tuning (QAF)</span></a></h2>\n<p>量化感知微调（QAF）是一种在已有的预训练模型基础上进行微调的量化方法。与 QAT 类似，QAF 在微调过程中也会引入量化的意识。然而，由于其基于预训练模型进行微调，因此训练时间通常较短。</p>\n<p>QAF 的优势在于其能够在较短的时间内获得接近 QAT 的量化效果，是一种兼顾效率与性能的方法。</p>\n<h2 id=\"训练后量化-post-training-quantization-ptq\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练后量化-post-training-quantization-ptq\"><span>训练后量化 Post Training Quantization (PTQ)</span></a></h2>\n<p>训练后量化（PTQ）是指在模型训练完成后，对其进行量化处理的一种方法。PTQ 不需要在训练过程中考虑量化问题，因此可以直接应用于任何已经训练好的模型。</p>\n<p>虽然 PTQ 的实施非常简单，并且不会增加训练时间，但其效果通常不如 QAT 和 QAF，因为模型在训练过程中没有机会适应量化引入的噪声。然而，对于一些特定的应用场景，PTQ 仍然是一个快速而有效的选择。</p>\n<h1 id=\"qat-量化感知训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat-量化感知训练\"><span>QAT 量化感知训练</span></a></h1>\n<p>在深度学习模型的训练和部署过程中，模型的精度和计算效率往往是两个重要的考量因素。为了在这两者之间取得平衡，量化感知训练（Quantization Aware Training, QAT）成为了一种有效的方法。本文将介绍QAT的基本原理及其在大语言模型（LLM）中的具体应用。</p>\n<h2 id=\"量化感知训练的基本原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知训练的基本原理\"><span>量化感知训练的基本原理</span></a></h2>\n<p>量化感知训练首先需要对模型进行正常的预训练。在此基础上，模型中会插入“伪量化节点”，即对权重和激活进行量化和反量化操作。这种方法的目的在于引入量化误差，使得模型在训练过程中能够“感知”到量化操作，从而在优化训练误差的同时兼顾量化误差。这种方法特别适用于对模型精度要求较高的场景，其量化目标无缝地集成到模型的训练过程中，使得大语言模型（LLM）在训练过程中能适应低精度表示，增强其处理由量化引起的精度损失的能力。</p>\n<h2 id=\"qat方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat方法\"><span>QAT方法</span></a></h2>\n<h3 id=\"llm-qat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm-qat\"><span>LLM-QAT</span></a></h3>\n<p>LLM-QAT是一种利用预训练模型生成结果来实现无数据蒸馏的方法。通过这种方法，我们不仅可以量化模型的权重和激活，还可以量化KV缓存。这样的策略旨在增强吞吐量并支持更长的序列。这意味着，LLM-QAT能够将带有权重和KV缓存量化的LLaMA模型蒸馏为仅有4比特的模型。</p>\n<p>这种方法不仅提高了计算效率，还在一定程度上保留了模型精度，使得在资源受限的环境中部署大规模语言模型成为可能。\n<img src=\"/img/user/附件/Pasted image 20250501212325.png\" alt=\"Pasted image 20250501212325.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212339.png\" alt=\"Pasted image 20250501212339.png\"></p>\n<h1 id=\"qaf-量化感知微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qaf-量化感知微调\"><span>QAF 量化感知微调</span></a></h1>\n<p>在微调过程中对大型语言模型（LLM）进行量化的主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，可以在模型压缩和保持性能之间取得平衡。</p>\n<h2 id=\"qaf方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qaf方法\"><span>QAF方法</span></a></h2>\n<h3 id=\"peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization\"><span>PEQA：Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</span></a></h3>\n<p>PEQA是一种新的量化感知微调技术，可以促进模型压缩并加速推理。它采用了双阶段过程运行：</p>\n<ol>\n<li><strong>第一阶段</strong>：每个全连接层的参数矩阵被量化为低比特整数矩阵和标量向量。</li>\n<li><strong>第二阶段</strong>：对每个特定下游任务的标量向量进行微调。</li>\n</ol>\n<p>这种策略大大压缩了模型的大小，从而降低了部署时的推理延迟并减少了所需的总体内存。同时，使快速的微调和高效的任务切换成为可能。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212427.png\" alt=\"Pasted image 20250501212427.png\"></p>\n<h3 id=\"qlora\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qlora\"><span>QLoRA</span></a></h3>\n<p>QLoRA也是一种QAF方法，具体内容可以参考3.5.6章节。</p>\n<h2 id=\"peft-参数高效微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#peft-参数高效微调\"><span>PEFT 参数高效微调</span></a></h2>\n<p>在参数高效微调（PEFT）中，目标是通过最小化参数调整来实现最大化的模型性能改进。这种方法不仅节省计算资源，还能在多任务环境中灵活应用。QLoRA作为QAF方法的一种实现，体现了PEFT的理念。</p>\n<h1 id=\"ptq-训练后量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ptq-训练后量化\"><span>PTQ 训练后量化</span></a></h1>\n<p>在深度学习领域，特别是大规模语言模型（LLM）的应用中，模型的存储和计算成本一直是一个重要的挑战。为了应对这一挑战，量化技术被广泛应用于减少模型的复杂性和提高效率。本文将探讨几种常用的训练后量化（PTQ）方法。</p>\n<h2 id=\"qat-插入-伪量化节点-后微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat-插入-伪量化节点-后微调\"><span>QAT 插入“伪量化节点”后微调</span></a></h2>\n<p>量化感知训练（QAT）通过在训练过程中插入“伪量化节点”来模拟量化效果，以便在推理阶段更好地适应量化后的模型。然而，这种方法大大增加了计算成本，尤其是在面对超大规模的 LLM 时。目前，针对 LLM 的量化研究主要集中在训练后量化（PTQ），例如 LLM.int8()、SmoothQuant 和 GPT-Q。对于权重而言，可以在推理前事先计算好量化系数，完成量化。但是对于激活值（即各层的输入），它们事先是未知的，取决于具体的推理输入。</p>\n<h2 id=\"训练后量化-ptq\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练后量化-ptq\"><span>训练后量化（PTQ）</span></a></h2>\n<p>在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性，但PTQ可能会在量化过程中引入一定程度的精度损失。</p>\n<h3 id=\"ptq方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ptq方法\"><span>PTQ方法</span></a></h3>\n<h4 id=\"llm-int8\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm-int8\"><span>LLM.int8()</span></a></h4>\n<p>在激活值 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 中存在一些离群值，它们的绝对值明显更大；并且这些离群值分布在少量的几个特征中，称为离群特征。观察下图中黄色的离群值，不论是 per-token 还是 per-channel 量化，都会受到这些离群值的很大影响。LLM.int8() 的思路是，既然只有少量的特征包含离群值，那就把这些特征拿出来单独计算，只对剩余特征做量化，即采用混合精度分解的量化方法。先做一个矩阵分解，对绝大部分权重和激活用8比特量化（vector-wise），对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。\n<img src=\"/img/user/附件/Pasted image 20250501212515.png\" alt=\"Pasted image 20250501212515.png\"></p>\n<h4 id=\"smoothquant\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#smoothquant\"><span>SmoothQuant</span></a></h4>\n<p>针对激活中的离群值，SmoothQuant 给出了与 LLM.int8() 不同的解题思路。激活值的量化比权重的量化难得多，可以通过一个平滑系数，把二者的难度中和一下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>X</mi><mo>⋅</mo><mtext>diag</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo><mo>⋅</mo><mo stretchy=\"false\">(</mo><mtext>diag</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>W</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mo>⋅</mo><mover accent=\"true\"><mi>W</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">Y = ( X \\cdot \\text{diag}(s)^{-1} ) \\cdot (\\text{diag}(s) \\cdot W) = \\hat{X} \\cdot \\hat{W}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">diag</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">diag</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9468em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.1667em;\"><span class=\"mord\">^</span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9468em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\"><span class=\"mord\">^</span></span></span></span></span></span></span></span></span></span></span></p>\n<p>其中，平滑系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span> 的计算为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><msub><mi>X</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><msup><mo stretchy=\"false\">)</mo><mi>α</mi></msup></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><msub><mi>W</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><msup><mo stretchy=\"false\">)</mo><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">s_j = \\frac{\\max(|X_j|)^\\alpha}{\\max(|W_j|)^{1-\\alpha}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.3991em;vertical-align:-0.9721em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">max</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7401em;\"><span style=\"top:-2.989em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">max</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9721em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>对平滑后的激活值和权重进行量化即可。权重采用 per-tensor 方式，激活采用不同粒度、不同时机的量化有不同版本。\n<img src=\"/img/user/附件/Pasted image 20250501212527.png\" alt=\"Pasted image 20250501212527.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212534.png\" alt=\"Pasted image 20250501212534.png\"></p>\n<h4 id=\"gpt-q\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpt-q\"><span>GPT-Q</span></a></h4>\n<p>LLM.int8() 和 SmoothQuant 都属于 round-to-nearest (RTN) 量化：舍入到最近的定点数。GPT-Q 则是把量化问题视作优化问题，逐层寻找最优的量化权重。对某个块（block）内的所有参数逐个量化，每个参数量化后，需要适当调整这个块内其他未量化的参数，以弥补量化造成的精度损失。此外，GPTQ 量化需要准备校准数据集。\n<img src=\"/img/user/附件/Pasted image 20250501212543.png\" alt=\"Pasted image 20250501212543.png\"></p>\n<p>AWQ： 对于LLM的性能， 权重并不是同等重要的，通过保留$$1%$$的显著权重可以大大减少量化误差。在此基础上，AWQ采用了激活感知方法，考虑与较大激活幅度对应的权重通道的重要性，这在处理重要特征时起着关键作用。采用逐通道缩放技术来确定最佳缩放因子，从而在量化所有权重的同时最小化量化误差。</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>量化误差</mtext><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant=\"normal\">∣</mi><msub><mtext>原始权重</mtext><mi>i</mi></msub><mo>−</mo><msub><mtext>量化权重</mtext><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">\\text{量化误差} = \\sum_{i=1}^{n} |\\text{原始权重}_i - \\text{量化权重}_i|\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">量化误差</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9291em;vertical-align:-1.2777em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">原始权重</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">量化权重</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span></span></span></span></span></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212610.png\" alt=\"Pasted image 20250501212610.png\"></p>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/模型压缩/模型量化.md","filePathRelative":"notes_bak/大语言模型学习/模型压缩/模型量化.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/模型压缩/模型量化","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/模型压缩/模型量化/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-05-01T13:19:00.000Z","updated":"2025-05-06T02:29:38.000Z","title":"模型量化","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"模型量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型量化\"><span>模型量化</span></a></h2>\n<p>模型量化是指在保持推理精度损失较低的情况下，将连续取值如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{float32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">float32</span></span></span></span></span>，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>float16</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{float16}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">float16</span></span></span></span></span> 的浮点型权重近似为有限多个离散值权重如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>int8</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{int8}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6679em;\"></span><span class=\"mord text\"><span class=\"mord\">int8</span></span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>int4</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{int4}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6679em;\"></span><span class=\"mord text\"><span class=\"mord\">int4</span></span></span></span></span> 的过程。通过以更少的位数表示浮点数据，模型量化可以减少模型尺寸，进而减少在推理时的内存消耗，并且在一些低精度运算较快的处理器上可以增加推理速度。</p>\n<h2 id=\"常用的数据类型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常用的数据类型\"><span>常用的数据类型</span></a></h2>\n<ul>\n<li><strong>FP32</strong>： 32位浮点数，是最常用的高精度表示方式。</li>\n<li><strong>FP16</strong>： 16位浮点数，数值范围比 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span> 小，但占用内存较少。</li>\n<li><strong>BF16</strong>： 16位截断的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span>，增加指数位，数值范围更广，常用于深度学习。</li>\n<li><strong>INT8</strong>： 8位整数，位数仅为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span> 的1/4，适用于模型参数的数据范围映射。</li>\n<li><strong>INT4</strong>： 4位整数，进一步减少位数，适用于极端资源受限的场景。</li>\n<li><strong>二值网络（Binary Network）</strong>： 1位二值网络，参数只能取0或1，计算效率极高但精度损失较大。</li>\n</ul>\n<p>工业界目前最常用的量化位数是8比特，低于8比特的量化被称为低比特量化。1比特是模型压缩的极限，可以将模型压缩为1/32，在推理时也可以使用高效的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>XNOR</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{XNOR}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">XNOR</span></span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>BitCount</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{BitCount}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">BitCount</span></span></span></span></span> 位运算来提升推理速度。</p>\n<h2 id=\"量化对象\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化对象\"><span>量化对象</span></a></h2>\n<ul>\n<li><strong>权重（Weight）</strong>：权重的量化是最常规的，可达到减少模型大小内存和占用空间。</li>\n<li><strong>激活值（Activation）</strong>： 实际中激活值往往是占内存使用的大头，量化激活值不仅可以大大减少内存占用，更重要的是，结合权重的量化可以充分利用整数计算获得性能提升。</li>\n<li><strong>KV Cache</strong>： 量化 KV 缓存对于提高长序列生成的吞吐量至关重要。</li>\n<li><strong>梯度（Gradients）</strong>： 相对上面的量化对象略微小众一些，因为主要用于训练。在训练深度学习模型时，梯度通常是浮点数，它主要作用是在分布式计算中减少通信开销，也可以减少反向传播时的开销。</li>\n</ul>\n<h1 id=\"量化形式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化形式\"><span>量化形式</span></a></h1>\n<h2 id=\"根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化\"><span>根据量化数据表示的原始数据范围是否均匀，可以将量化方法分为线性量化和非线性量化</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250501212209.png\" alt=\"Pasted image 20250501212209.png\">\n深度神经网络的权重和激活值通常是不均匀的，因此理论上使用非线性量化导致的精度损失更小，但在实际推理中非线性量化的计算复杂度较高，通常使用线性量化：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>q</mi><mo>=</mo><mtext>clip</mtext><mo stretchy=\"false\">(</mo><mtext>round</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><mo>⋅</mo><mi>r</mi><mo>+</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><msub><mi>q</mi><mi>min</mi><mo>⁡</mo></msub><mo separator=\"true\">,</mo><msub><mi>q</mi><mi>max</mi><mo>⁡</mo></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">q = \\text{clip}(\\text{round}(s \\cdot r + z), q_{\\min}, q_{\\max})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">clip</span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">round</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3175em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">i</span><span class=\"mtight\">n</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>其中 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span></span></span></span> 为量化前的浮点数，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span></span></span></span> 为量化后的整数，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>round</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{round}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">round</span></span></span></span></span> 表示取整，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>clip</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{clip}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord text\"><span class=\"mord\">clip</span></span></span></span></span> 为截断，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 为数据量化的间隔，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 为数据偏移，为0的时候为对称量化，不为0的时候为不对称量化。对称量化可以避免量化算子在推理中计算 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 相关的部分，降低推理时的计算复杂度；非对称量化可以根据实际数据的分布确定最小值和最大值，可以更加充分的利用量化数据信息，使得量化导致的损失更低。\n<img src=\"/img/user/附件/Pasted image 20250501212223.png\" alt=\"Pasted image 20250501212223.png\"></p>\n<h2 id=\"根据-和-的共享范围即量化粒度-量化方法可以进行以下分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#根据-和-的共享范围即量化粒度-量化方法可以进行以下分类\"><span>根据 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 的共享范围即量化粒度，量化方法可以进行以下分类</span></a></h2>\n<h3 id=\"逐层量化-per-tensor\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐层量化-per-tensor\"><span>逐层量化 per-tensor</span></a></h3>\n<p>范围最大，最简单，以一层网络为单位一组量化参数。\n<img src=\"/img/user/附件/Pasted image 20250501212233.png\" alt=\"Pasted image 20250501212233.png\"></p>\n<h3 id=\"逐通道量化-per-token-per-channel\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐通道量化-per-token-per-channel\"><span>逐通道量化 per-token &amp; per-channel</span></a></h3>\n<p>以一层网络的每个量化通道为单位，每个通道单独用一组量化参数。量化粒度更细，更高的量化精度，计算也更复杂。其中，per-token 针对激活 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span> 而言，每行对应一个量化系数；per-channel 针对权重 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> 而言，每列对应一个量化系数。结合使用也叫 vector-wise。\n<img src=\"/img/user/附件/Pasted image 20250501212242.png\" alt=\"Pasted image 20250501212242.png\"></p>\n<h3 id=\"逐组量化-per-group\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐组量化-per-group\"><span>逐组量化 per-group</span></a></h3>\n<p>以组为单位，每个组比如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 行激活值或 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 列权重使用一组 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span>；它的粒度处于 per-tensor 和 vector-wise之间。当 groupsize=1 时，逐组量化与逐层量化等价；当 groupsize= 卷积核的数量时，逐组量化与逐通道量化等价。</p>\n<p>此外激活值和权重可以选择不同的粒度进行量化，对于激活值来说还有动态量化（推理过程中，实时计算激活的量化系数，对激活进行量化）与静态量化（在推理前就计算好激活的量化系数，在推理过程中应用）。</p>\n<h2 id=\"llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图\"><span>Llama3 技术报告中提供的 tensor-wise 和 row-wise FP8 量化示意图</span></a></h2>\n<p>在 Llama3 技术报告中，提供了 tensor-wise 和 row-wise 的 FP8 量化示意图，这些示意图展示了不同粒度下的浮点数到整数的转换过程，帮助理解不同粒度选择对于模型性能和计算复杂度的影响。\n<img src=\"/img/user/附件/Pasted image 20250501212137.png\" alt=\"Pasted image 20250501212137.png\"></p>\n<h1 id=\"量化分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化分类\"><span>量化分类</span></a></h1>\n<p>在深度学习模型的部署中，模型的大小和推理速度是两个非常重要的因素。为了在不显著损失模型准确率的前提下减小模型大小并加快推理速度，量化技术应运而生。量化技术通过将模型参数和计算从浮点数表示转换为低精度表示（如 8 位整数）来实现这一目标。根据量化压缩模型的阶段，量化可以分为以下几类：</p>\n<h2 id=\"量化感知训练-quantization-aware-training-qat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知训练-quantization-aware-training-qat\"><span>量化感知训练 Quantization Aware Training (QAT)</span></a></h2>\n<p>量化感知训练（QAT）是在模型训练过程中引入量化的意识。具体来说，在训练过程中，尽管模型的权重和激活值仍然以浮点数形式存储和更新，但在前向传播时会模拟低精度运算。这种方法使得模型在训练过程中就能够适应量化带来的噪声，从而在部署时能够更好地保持性能。</p>\n<p>QAT 的主要优势在于其能够在保持模型准确率的同时实现显著的压缩效果。然而，由于需要在训练过程中进行额外的模拟计算，QAT 的训练时间可能会有所增加。</p>\n<h2 id=\"量化感知微调-quantization-aware-fine-tuning-qaf\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知微调-quantization-aware-fine-tuning-qaf\"><span>量化感知微调 Quantization-Aware Fine-tuning (QAF)</span></a></h2>\n<p>量化感知微调（QAF）是一种在已有的预训练模型基础上进行微调的量化方法。与 QAT 类似，QAF 在微调过程中也会引入量化的意识。然而，由于其基于预训练模型进行微调，因此训练时间通常较短。</p>\n<p>QAF 的优势在于其能够在较短的时间内获得接近 QAT 的量化效果，是一种兼顾效率与性能的方法。</p>\n<h2 id=\"训练后量化-post-training-quantization-ptq\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练后量化-post-training-quantization-ptq\"><span>训练后量化 Post Training Quantization (PTQ)</span></a></h2>\n<p>训练后量化（PTQ）是指在模型训练完成后，对其进行量化处理的一种方法。PTQ 不需要在训练过程中考虑量化问题，因此可以直接应用于任何已经训练好的模型。</p>\n<p>虽然 PTQ 的实施非常简单，并且不会增加训练时间，但其效果通常不如 QAT 和 QAF，因为模型在训练过程中没有机会适应量化引入的噪声。然而，对于一些特定的应用场景，PTQ 仍然是一个快速而有效的选择。</p>\n<h1 id=\"qat-量化感知训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat-量化感知训练\"><span>QAT 量化感知训练</span></a></h1>\n<p>在深度学习模型的训练和部署过程中，模型的精度和计算效率往往是两个重要的考量因素。为了在这两者之间取得平衡，量化感知训练（Quantization Aware Training, QAT）成为了一种有效的方法。本文将介绍QAT的基本原理及其在大语言模型（LLM）中的具体应用。</p>\n<h2 id=\"量化感知训练的基本原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知训练的基本原理\"><span>量化感知训练的基本原理</span></a></h2>\n<p>量化感知训练首先需要对模型进行正常的预训练。在此基础上，模型中会插入“伪量化节点”，即对权重和激活进行量化和反量化操作。这种方法的目的在于引入量化误差，使得模型在训练过程中能够“感知”到量化操作，从而在优化训练误差的同时兼顾量化误差。这种方法特别适用于对模型精度要求较高的场景，其量化目标无缝地集成到模型的训练过程中，使得大语言模型（LLM）在训练过程中能适应低精度表示，增强其处理由量化引起的精度损失的能力。</p>\n<h2 id=\"qat方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat方法\"><span>QAT方法</span></a></h2>\n<h3 id=\"llm-qat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm-qat\"><span>LLM-QAT</span></a></h3>\n<p>LLM-QAT是一种利用预训练模型生成结果来实现无数据蒸馏的方法。通过这种方法，我们不仅可以量化模型的权重和激活，还可以量化KV缓存。这样的策略旨在增强吞吐量并支持更长的序列。这意味着，LLM-QAT能够将带有权重和KV缓存量化的LLaMA模型蒸馏为仅有4比特的模型。</p>\n<p>这种方法不仅提高了计算效率，还在一定程度上保留了模型精度，使得在资源受限的环境中部署大规模语言模型成为可能。\n<img src=\"/img/user/附件/Pasted image 20250501212325.png\" alt=\"Pasted image 20250501212325.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212339.png\" alt=\"Pasted image 20250501212339.png\"></p>\n<h1 id=\"qaf-量化感知微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qaf-量化感知微调\"><span>QAF 量化感知微调</span></a></h1>\n<p>在微调过程中对大型语言模型（LLM）进行量化的主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，可以在模型压缩和保持性能之间取得平衡。</p>\n<h2 id=\"qaf方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qaf方法\"><span>QAF方法</span></a></h2>\n<h3 id=\"peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization\"><span>PEQA：Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</span></a></h3>\n<p>PEQA是一种新的量化感知微调技术，可以促进模型压缩并加速推理。它采用了双阶段过程运行：</p>\n<ol>\n<li><strong>第一阶段</strong>：每个全连接层的参数矩阵被量化为低比特整数矩阵和标量向量。</li>\n<li><strong>第二阶段</strong>：对每个特定下游任务的标量向量进行微调。</li>\n</ol>\n<p>这种策略大大压缩了模型的大小，从而降低了部署时的推理延迟并减少了所需的总体内存。同时，使快速的微调和高效的任务切换成为可能。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212427.png\" alt=\"Pasted image 20250501212427.png\"></p>\n<h3 id=\"qlora\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qlora\"><span>QLoRA</span></a></h3>\n<p>QLoRA也是一种QAF方法，具体内容可以参考3.5.6章节。</p>\n<h2 id=\"peft-参数高效微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#peft-参数高效微调\"><span>PEFT 参数高效微调</span></a></h2>\n<p>在参数高效微调（PEFT）中，目标是通过最小化参数调整来实现最大化的模型性能改进。这种方法不仅节省计算资源，还能在多任务环境中灵活应用。QLoRA作为QAF方法的一种实现，体现了PEFT的理念。</p>\n<h1 id=\"ptq-训练后量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ptq-训练后量化\"><span>PTQ 训练后量化</span></a></h1>\n<p>在深度学习领域，特别是大规模语言模型（LLM）的应用中，模型的存储和计算成本一直是一个重要的挑战。为了应对这一挑战，量化技术被广泛应用于减少模型的复杂性和提高效率。本文将探讨几种常用的训练后量化（PTQ）方法。</p>\n<h2 id=\"qat-插入-伪量化节点-后微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat-插入-伪量化节点-后微调\"><span>QAT 插入“伪量化节点”后微调</span></a></h2>\n<p>量化感知训练（QAT）通过在训练过程中插入“伪量化节点”来模拟量化效果，以便在推理阶段更好地适应量化后的模型。然而，这种方法大大增加了计算成本，尤其是在面对超大规模的 LLM 时。目前，针对 LLM 的量化研究主要集中在训练后量化（PTQ），例如 LLM.int8()、SmoothQuant 和 GPT-Q。对于权重而言，可以在推理前事先计算好量化系数，完成量化。但是对于激活值（即各层的输入），它们事先是未知的，取决于具体的推理输入。</p>\n<h2 id=\"训练后量化-ptq\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练后量化-ptq\"><span>训练后量化（PTQ）</span></a></h2>\n<p>在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性，但PTQ可能会在量化过程中引入一定程度的精度损失。</p>\n<h3 id=\"ptq方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ptq方法\"><span>PTQ方法</span></a></h3>\n<h4 id=\"llm-int8\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm-int8\"><span>LLM.int8()</span></a></h4>\n<p>在激活值 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 中存在一些离群值，它们的绝对值明显更大；并且这些离群值分布在少量的几个特征中，称为离群特征。观察下图中黄色的离群值，不论是 per-token 还是 per-channel 量化，都会受到这些离群值的很大影响。LLM.int8() 的思路是，既然只有少量的特征包含离群值，那就把这些特征拿出来单独计算，只对剩余特征做量化，即采用混合精度分解的量化方法。先做一个矩阵分解，对绝大部分权重和激活用8比特量化（vector-wise），对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。\n<img src=\"/img/user/附件/Pasted image 20250501212515.png\" alt=\"Pasted image 20250501212515.png\"></p>\n<h4 id=\"smoothquant\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#smoothquant\"><span>SmoothQuant</span></a></h4>\n<p>针对激活中的离群值，SmoothQuant 给出了与 LLM.int8() 不同的解题思路。激活值的量化比权重的量化难得多，可以通过一个平滑系数，把二者的难度中和一下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>X</mi><mo>⋅</mo><mtext>diag</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo><mo>⋅</mo><mo stretchy=\"false\">(</mo><mtext>diag</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>W</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mo>⋅</mo><mover accent=\"true\"><mi>W</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">Y = ( X \\cdot \\text{diag}(s)^{-1} ) \\cdot (\\text{diag}(s) \\cdot W) = \\hat{X} \\cdot \\hat{W}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">diag</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">diag</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9468em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.1667em;\"><span class=\"mord\">^</span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9468em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\"><span class=\"mord\">^</span></span></span></span></span></span></span></span></span></span></span></p>\n<p>其中，平滑系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span> 的计算为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><msub><mi>X</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><msup><mo stretchy=\"false\">)</mo><mi>α</mi></msup></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><msub><mi>W</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><msup><mo stretchy=\"false\">)</mo><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">s_j = \\frac{\\max(|X_j|)^\\alpha}{\\max(|W_j|)^{1-\\alpha}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.3991em;vertical-align:-0.9721em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">max</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7401em;\"><span style=\"top:-2.989em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">max</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9721em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>对平滑后的激活值和权重进行量化即可。权重采用 per-tensor 方式，激活采用不同粒度、不同时机的量化有不同版本。\n<img src=\"/img/user/附件/Pasted image 20250501212527.png\" alt=\"Pasted image 20250501212527.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212534.png\" alt=\"Pasted image 20250501212534.png\"></p>\n<h4 id=\"gpt-q\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpt-q\"><span>GPT-Q</span></a></h4>\n<p>LLM.int8() 和 SmoothQuant 都属于 round-to-nearest (RTN) 量化：舍入到最近的定点数。GPT-Q 则是把量化问题视作优化问题，逐层寻找最优的量化权重。对某个块（block）内的所有参数逐个量化，每个参数量化后，需要适当调整这个块内其他未量化的参数，以弥补量化造成的精度损失。此外，GPTQ 量化需要准备校准数据集。\n<img src=\"/img/user/附件/Pasted image 20250501212543.png\" alt=\"Pasted image 20250501212543.png\"></p>\n<p>AWQ： 对于LLM的性能， 权重并不是同等重要的，通过保留$$1%$$的显著权重可以大大减少量化误差。在此基础上，AWQ采用了激活感知方法，考虑与较大激活幅度对应的权重通道的重要性，这在处理重要特征时起着关键作用。采用逐通道缩放技术来确定最佳缩放因子，从而在量化所有权重的同时最小化量化误差。</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>量化误差</mtext><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant=\"normal\">∣</mi><msub><mtext>原始权重</mtext><mi>i</mi></msub><mo>−</mo><msub><mtext>量化权重</mtext><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">\\text{量化误差} = \\sum_{i=1}^{n} |\\text{原始权重}_i - \\text{量化权重}_i|\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">量化误差</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9291em;vertical-align:-1.2777em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">原始权重</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">量化权重</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span></span></span></span></span></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212610.png\" alt=\"Pasted image 20250501212610.png\"></p>\n</template>","contentStripped":"<h2 id=\"模型量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型量化\"><span>模型量化</span></a></h2>\n<p>模型量化是指在保持推理精度损失较低的情况下，将连续取值如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{float32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">float32</span></span></span></span></span>，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>float16</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{float16}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">float16</span></span></span></span></span> 的浮点型权重近似为有限多个离散值权重如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>int8</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{int8}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6679em;\"></span><span class=\"mord text\"><span class=\"mord\">int8</span></span></span></span></span>、<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>int4</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{int4}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6679em;\"></span><span class=\"mord text\"><span class=\"mord\">int4</span></span></span></span></span> 的过程。通过以更少的位数表示浮点数据，模型量化可以减少模型尺寸，进而减少在推理时的内存消耗，并且在一些低精度运算较快的处理器上可以增加推理速度。</p>\n<h2 id=\"常用的数据类型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常用的数据类型\"><span>常用的数据类型</span></a></h2>\n<ul>\n<li><strong>FP32</strong>： 32位浮点数，是最常用的高精度表示方式。</li>\n<li><strong>FP16</strong>： 16位浮点数，数值范围比 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span> 小，但占用内存较少。</li>\n<li><strong>BF16</strong>： 16位截断的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span>，增加指数位，数值范围更广，常用于深度学习。</li>\n<li><strong>INT8</strong>： 8位整数，位数仅为 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>FP32</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{FP32}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">FP32</span></span></span></span></span> 的1/4，适用于模型参数的数据范围映射。</li>\n<li><strong>INT4</strong>： 4位整数，进一步减少位数，适用于极端资源受限的场景。</li>\n<li><strong>二值网络（Binary Network）</strong>： 1位二值网络，参数只能取0或1，计算效率极高但精度损失较大。</li>\n</ul>\n<p>工业界目前最常用的量化位数是8比特，低于8比特的量化被称为低比特量化。1比特是模型压缩的极限，可以将模型压缩为1/32，在推理时也可以使用高效的 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>XNOR</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{XNOR}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">XNOR</span></span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>BitCount</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{BitCount}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord\">BitCount</span></span></span></span></span> 位运算来提升推理速度。</p>\n<h2 id=\"量化对象\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化对象\"><span>量化对象</span></a></h2>\n<ul>\n<li><strong>权重（Weight）</strong>：权重的量化是最常规的，可达到减少模型大小内存和占用空间。</li>\n<li><strong>激活值（Activation）</strong>： 实际中激活值往往是占内存使用的大头，量化激活值不仅可以大大减少内存占用，更重要的是，结合权重的量化可以充分利用整数计算获得性能提升。</li>\n<li><strong>KV Cache</strong>： 量化 KV 缓存对于提高长序列生成的吞吐量至关重要。</li>\n<li><strong>梯度（Gradients）</strong>： 相对上面的量化对象略微小众一些，因为主要用于训练。在训练深度学习模型时，梯度通常是浮点数，它主要作用是在分布式计算中减少通信开销，也可以减少反向传播时的开销。</li>\n</ul>\n<h1 id=\"量化形式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化形式\"><span>量化形式</span></a></h1>\n<h2 id=\"根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化\"><span>根据量化数据表示的原始数据范围是否均匀，可以将量化方法分为线性量化和非线性量化</span></a></h2>\n<p><img src=\"/img/user/附件/Pasted image 20250501212209.png\" alt=\"Pasted image 20250501212209.png\">\n深度神经网络的权重和激活值通常是不均匀的，因此理论上使用非线性量化导致的精度损失更小，但在实际推理中非线性量化的计算复杂度较高，通常使用线性量化：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>q</mi><mo>=</mo><mtext>clip</mtext><mo stretchy=\"false\">(</mo><mtext>round</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><mo>⋅</mo><mi>r</mi><mo>+</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><msub><mi>q</mi><mi>min</mi><mo>⁡</mo></msub><mo separator=\"true\">,</mo><msub><mi>q</mi><mi>max</mi><mo>⁡</mo></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">q = \\text{clip}(\\text{round}(s \\cdot r + z), q_{\\min}, q_{\\max})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">clip</span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">round</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3175em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">i</span><span class=\"mtight\">n</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>其中 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span></span></span></span> 为量化前的浮点数，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span></span></span></span> 为量化后的整数，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>round</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{round}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">round</span></span></span></span></span> 表示取整，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>clip</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{clip}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord text\"><span class=\"mord\">clip</span></span></span></span></span> 为截断，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 为数据量化的间隔，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 为数据偏移，为0的时候为对称量化，不为0的时候为不对称量化。对称量化可以避免量化算子在推理中计算 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 相关的部分，降低推理时的计算复杂度；非对称量化可以根据实际数据的分布确定最小值和最大值，可以更加充分的利用量化数据信息，使得量化导致的损失更低。\n<img src=\"/img/user/附件/Pasted image 20250501212223.png\" alt=\"Pasted image 20250501212223.png\"></p>\n<h2 id=\"根据-和-的共享范围即量化粒度-量化方法可以进行以下分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#根据-和-的共享范围即量化粒度-量化方法可以进行以下分类\"><span>根据 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span> 的共享范围即量化粒度，量化方法可以进行以下分类</span></a></h2>\n<h3 id=\"逐层量化-per-tensor\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐层量化-per-tensor\"><span>逐层量化 per-tensor</span></a></h3>\n<p>范围最大，最简单，以一层网络为单位一组量化参数。\n<img src=\"/img/user/附件/Pasted image 20250501212233.png\" alt=\"Pasted image 20250501212233.png\"></p>\n<h3 id=\"逐通道量化-per-token-per-channel\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐通道量化-per-token-per-channel\"><span>逐通道量化 per-token &amp; per-channel</span></a></h3>\n<p>以一层网络的每个量化通道为单位，每个通道单独用一组量化参数。量化粒度更细，更高的量化精度，计算也更复杂。其中，per-token 针对激活 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span> 而言，每行对应一个量化系数；per-channel 针对权重 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> 而言，每列对应一个量化系数。结合使用也叫 vector-wise。\n<img src=\"/img/user/附件/Pasted image 20250501212242.png\" alt=\"Pasted image 20250501212242.png\"></p>\n<h3 id=\"逐组量化-per-group\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#逐组量化-per-group\"><span>逐组量化 per-group</span></a></h3>\n<p>以组为单位，每个组比如 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 行激活值或 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 列权重使用一组 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 和 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span>；它的粒度处于 per-tensor 和 vector-wise之间。当 groupsize=1 时，逐组量化与逐层量化等价；当 groupsize= 卷积核的数量时，逐组量化与逐通道量化等价。</p>\n<p>此外激活值和权重可以选择不同的粒度进行量化，对于激活值来说还有动态量化（推理过程中，实时计算激活的量化系数，对激活进行量化）与静态量化（在推理前就计算好激活的量化系数，在推理过程中应用）。</p>\n<h2 id=\"llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图\"><span>Llama3 技术报告中提供的 tensor-wise 和 row-wise FP8 量化示意图</span></a></h2>\n<p>在 Llama3 技术报告中，提供了 tensor-wise 和 row-wise 的 FP8 量化示意图，这些示意图展示了不同粒度下的浮点数到整数的转换过程，帮助理解不同粒度选择对于模型性能和计算复杂度的影响。\n<img src=\"/img/user/附件/Pasted image 20250501212137.png\" alt=\"Pasted image 20250501212137.png\"></p>\n<h1 id=\"量化分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化分类\"><span>量化分类</span></a></h1>\n<p>在深度学习模型的部署中，模型的大小和推理速度是两个非常重要的因素。为了在不显著损失模型准确率的前提下减小模型大小并加快推理速度，量化技术应运而生。量化技术通过将模型参数和计算从浮点数表示转换为低精度表示（如 8 位整数）来实现这一目标。根据量化压缩模型的阶段，量化可以分为以下几类：</p>\n<h2 id=\"量化感知训练-quantization-aware-training-qat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知训练-quantization-aware-training-qat\"><span>量化感知训练 Quantization Aware Training (QAT)</span></a></h2>\n<p>量化感知训练（QAT）是在模型训练过程中引入量化的意识。具体来说，在训练过程中，尽管模型的权重和激活值仍然以浮点数形式存储和更新，但在前向传播时会模拟低精度运算。这种方法使得模型在训练过程中就能够适应量化带来的噪声，从而在部署时能够更好地保持性能。</p>\n<p>QAT 的主要优势在于其能够在保持模型准确率的同时实现显著的压缩效果。然而，由于需要在训练过程中进行额外的模拟计算，QAT 的训练时间可能会有所增加。</p>\n<h2 id=\"量化感知微调-quantization-aware-fine-tuning-qaf\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知微调-quantization-aware-fine-tuning-qaf\"><span>量化感知微调 Quantization-Aware Fine-tuning (QAF)</span></a></h2>\n<p>量化感知微调（QAF）是一种在已有的预训练模型基础上进行微调的量化方法。与 QAT 类似，QAF 在微调过程中也会引入量化的意识。然而，由于其基于预训练模型进行微调，因此训练时间通常较短。</p>\n<p>QAF 的优势在于其能够在较短的时间内获得接近 QAT 的量化效果，是一种兼顾效率与性能的方法。</p>\n<h2 id=\"训练后量化-post-training-quantization-ptq\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练后量化-post-training-quantization-ptq\"><span>训练后量化 Post Training Quantization (PTQ)</span></a></h2>\n<p>训练后量化（PTQ）是指在模型训练完成后，对其进行量化处理的一种方法。PTQ 不需要在训练过程中考虑量化问题，因此可以直接应用于任何已经训练好的模型。</p>\n<p>虽然 PTQ 的实施非常简单，并且不会增加训练时间，但其效果通常不如 QAT 和 QAF，因为模型在训练过程中没有机会适应量化引入的噪声。然而，对于一些特定的应用场景，PTQ 仍然是一个快速而有效的选择。</p>\n<h1 id=\"qat-量化感知训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat-量化感知训练\"><span>QAT 量化感知训练</span></a></h1>\n<p>在深度学习模型的训练和部署过程中，模型的精度和计算效率往往是两个重要的考量因素。为了在这两者之间取得平衡，量化感知训练（Quantization Aware Training, QAT）成为了一种有效的方法。本文将介绍QAT的基本原理及其在大语言模型（LLM）中的具体应用。</p>\n<h2 id=\"量化感知训练的基本原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#量化感知训练的基本原理\"><span>量化感知训练的基本原理</span></a></h2>\n<p>量化感知训练首先需要对模型进行正常的预训练。在此基础上，模型中会插入“伪量化节点”，即对权重和激活进行量化和反量化操作。这种方法的目的在于引入量化误差，使得模型在训练过程中能够“感知”到量化操作，从而在优化训练误差的同时兼顾量化误差。这种方法特别适用于对模型精度要求较高的场景，其量化目标无缝地集成到模型的训练过程中，使得大语言模型（LLM）在训练过程中能适应低精度表示，增强其处理由量化引起的精度损失的能力。</p>\n<h2 id=\"qat方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat方法\"><span>QAT方法</span></a></h2>\n<h3 id=\"llm-qat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm-qat\"><span>LLM-QAT</span></a></h3>\n<p>LLM-QAT是一种利用预训练模型生成结果来实现无数据蒸馏的方法。通过这种方法，我们不仅可以量化模型的权重和激活，还可以量化KV缓存。这样的策略旨在增强吞吐量并支持更长的序列。这意味着，LLM-QAT能够将带有权重和KV缓存量化的LLaMA模型蒸馏为仅有4比特的模型。</p>\n<p>这种方法不仅提高了计算效率，还在一定程度上保留了模型精度，使得在资源受限的环境中部署大规模语言模型成为可能。\n<img src=\"/img/user/附件/Pasted image 20250501212325.png\" alt=\"Pasted image 20250501212325.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212339.png\" alt=\"Pasted image 20250501212339.png\"></p>\n<h1 id=\"qaf-量化感知微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qaf-量化感知微调\"><span>QAF 量化感知微调</span></a></h1>\n<p>在微调过程中对大型语言模型（LLM）进行量化的主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，可以在模型压缩和保持性能之间取得平衡。</p>\n<h2 id=\"qaf方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qaf方法\"><span>QAF方法</span></a></h2>\n<h3 id=\"peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization\"><span>PEQA：Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</span></a></h3>\n<p>PEQA是一种新的量化感知微调技术，可以促进模型压缩并加速推理。它采用了双阶段过程运行：</p>\n<ol>\n<li><strong>第一阶段</strong>：每个全连接层的参数矩阵被量化为低比特整数矩阵和标量向量。</li>\n<li><strong>第二阶段</strong>：对每个特定下游任务的标量向量进行微调。</li>\n</ol>\n<p>这种策略大大压缩了模型的大小，从而降低了部署时的推理延迟并减少了所需的总体内存。同时，使快速的微调和高效的任务切换成为可能。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212427.png\" alt=\"Pasted image 20250501212427.png\"></p>\n<h3 id=\"qlora\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qlora\"><span>QLoRA</span></a></h3>\n<p>QLoRA也是一种QAF方法，具体内容可以参考3.5.6章节。</p>\n<h2 id=\"peft-参数高效微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#peft-参数高效微调\"><span>PEFT 参数高效微调</span></a></h2>\n<p>在参数高效微调（PEFT）中，目标是通过最小化参数调整来实现最大化的模型性能改进。这种方法不仅节省计算资源，还能在多任务环境中灵活应用。QLoRA作为QAF方法的一种实现，体现了PEFT的理念。</p>\n<h1 id=\"ptq-训练后量化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ptq-训练后量化\"><span>PTQ 训练后量化</span></a></h1>\n<p>在深度学习领域，特别是大规模语言模型（LLM）的应用中，模型的存储和计算成本一直是一个重要的挑战。为了应对这一挑战，量化技术被广泛应用于减少模型的复杂性和提高效率。本文将探讨几种常用的训练后量化（PTQ）方法。</p>\n<h2 id=\"qat-插入-伪量化节点-后微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qat-插入-伪量化节点-后微调\"><span>QAT 插入“伪量化节点”后微调</span></a></h2>\n<p>量化感知训练（QAT）通过在训练过程中插入“伪量化节点”来模拟量化效果，以便在推理阶段更好地适应量化后的模型。然而，这种方法大大增加了计算成本，尤其是在面对超大规模的 LLM 时。目前，针对 LLM 的量化研究主要集中在训练后量化（PTQ），例如 LLM.int8()、SmoothQuant 和 GPT-Q。对于权重而言，可以在推理前事先计算好量化系数，完成量化。但是对于激活值（即各层的输入），它们事先是未知的，取决于具体的推理输入。</p>\n<h2 id=\"训练后量化-ptq\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练后量化-ptq\"><span>训练后量化（PTQ）</span></a></h2>\n<p>在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性，但PTQ可能会在量化过程中引入一定程度的精度损失。</p>\n<h3 id=\"ptq方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ptq方法\"><span>PTQ方法</span></a></h3>\n<h4 id=\"llm-int8\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm-int8\"><span>LLM.int8()</span></a></h4>\n<p>在激活值 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 中存在一些离群值，它们的绝对值明显更大；并且这些离群值分布在少量的几个特征中，称为离群特征。观察下图中黄色的离群值，不论是 per-token 还是 per-channel 量化，都会受到这些离群值的很大影响。LLM.int8() 的思路是，既然只有少量的特征包含离群值，那就把这些特征拿出来单独计算，只对剩余特征做量化，即采用混合精度分解的量化方法。先做一个矩阵分解，对绝大部分权重和激活用8比特量化（vector-wise），对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。\n<img src=\"/img/user/附件/Pasted image 20250501212515.png\" alt=\"Pasted image 20250501212515.png\"></p>\n<h4 id=\"smoothquant\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#smoothquant\"><span>SmoothQuant</span></a></h4>\n<p>针对激活中的离群值，SmoothQuant 给出了与 LLM.int8() 不同的解题思路。激活值的量化比权重的量化难得多，可以通过一个平滑系数，把二者的难度中和一下：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>X</mi><mo>⋅</mo><mtext>diag</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo><mo>⋅</mo><mo stretchy=\"false\">(</mo><mtext>diag</mtext><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>W</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mo>⋅</mo><mover accent=\"true\"><mi>W</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">Y = ( X \\cdot \\text{diag}(s)^{-1} ) \\cdot (\\text{diag}(s) \\cdot W) = \\hat{X} \\cdot \\hat{W}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">diag</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">diag</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9468em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.1667em;\"><span class=\"mord\">^</span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9468em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\"><span class=\"mord\">^</span></span></span></span></span></span></span></span></span></span></span></p>\n<p>其中，平滑系数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span> 的计算为：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><msub><mi>X</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><msup><mo stretchy=\"false\">)</mo><mi>α</mi></msup></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><msub><mi>W</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><msup><mo stretchy=\"false\">)</mo><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">s_j = \\frac{\\max(|X_j|)^\\alpha}{\\max(|W_j|)^{1-\\alpha}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.3991em;vertical-align:-0.9721em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">max</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7401em;\"><span style=\"top:-2.989em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">max</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9721em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>对平滑后的激活值和权重进行量化即可。权重采用 per-tensor 方式，激活采用不同粒度、不同时机的量化有不同版本。\n<img src=\"/img/user/附件/Pasted image 20250501212527.png\" alt=\"Pasted image 20250501212527.png\"></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212534.png\" alt=\"Pasted image 20250501212534.png\"></p>\n<h4 id=\"gpt-q\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpt-q\"><span>GPT-Q</span></a></h4>\n<p>LLM.int8() 和 SmoothQuant 都属于 round-to-nearest (RTN) 量化：舍入到最近的定点数。GPT-Q 则是把量化问题视作优化问题，逐层寻找最优的量化权重。对某个块（block）内的所有参数逐个量化，每个参数量化后，需要适当调整这个块内其他未量化的参数，以弥补量化造成的精度损失。此外，GPTQ 量化需要准备校准数据集。\n<img src=\"/img/user/附件/Pasted image 20250501212543.png\" alt=\"Pasted image 20250501212543.png\"></p>\n<p>AWQ： 对于LLM的性能， 权重并不是同等重要的，通过保留$$1%$$的显著权重可以大大减少量化误差。在此基础上，AWQ采用了激活感知方法，考虑与较大激活幅度对应的权重通道的重要性，这在处理重要特征时起着关键作用。采用逐通道缩放技术来确定最佳缩放因子，从而在量化所有权重的同时最小化量化误差。</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>量化误差</mtext><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant=\"normal\">∣</mi><msub><mtext>原始权重</mtext><mi>i</mi></msub><mo>−</mo><msub><mtext>量化权重</mtext><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">\\text{量化误差} = \\sum_{i=1}^{n} |\\text{原始权重}_i - \\text{量化权重}_i|\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">量化误差</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9291em;vertical-align:-1.2777em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">原始权重</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord cjk_fallback\">量化权重</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span></span></span></span></span></p>\n<p><img src=\"/img/user/附件/Pasted image 20250501212610.png\" alt=\"Pasted image 20250501212610.png\"></p>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 模型量化\n模型量化是指在保持推理精度损失较低的情况下，将连续取值如 $\\text{float32}$，$\\text{float16}$ 的浮点型权重近似为有限多个离散值权重如 $\\text{int8}$、$\\text{int4}$ 的过程。通过以更少的位数表示浮点数据，模型量化可以减少模型尺寸，进而减少在推理时的内存消耗，并且在一些低精度运算较快的处理器上可以增加推理速度。\n\n\n## 常用的数据类型\n- **FP32**： 32位浮点数，是最常用的高精度表示方式。\n- **FP16**： 16位浮点数，数值范围比 $\\text{FP32}$ 小，但占用内存较少。\n- **BF16**： 16位截断的 $\\text{FP32}$，增加指数位，数值范围更广，常用于深度学习。\n- **INT8**： 8位整数，位数仅为 $\\text{FP32}$ 的1/4，适用于模型参数的数据范围映射。\n- **INT4**： 4位整数，进一步减少位数，适用于极端资源受限的场景。\n- **二值网络（Binary Network）**： 1位二值网络，参数只能取0或1，计算效率极高但精度损失较大。\n\n工业界目前最常用的量化位数是8比特，低于8比特的量化被称为低比特量化。1比特是模型压缩的极限，可以将模型压缩为1/32，在推理时也可以使用高效的 $\\text{XNOR}$ 和 $\\text{BitCount}$ 位运算来提升推理速度。\n\n\n## 量化对象\n- **权重（Weight）**：权重的量化是最常规的，可达到减少模型大小内存和占用空间。\n- **激活值（Activation）**： 实际中激活值往往是占内存使用的大头，量化激活值不仅可以大大减少内存占用，更重要的是，结合权重的量化可以充分利用整数计算获得性能提升。\n- **KV Cache**： 量化 KV 缓存对于提高长序列生成的吞吐量至关重要。\n- **梯度（Gradients）**： 相对上面的量化对象略微小众一些，因为主要用于训练。在训练深度学习模型时，梯度通常是浮点数，它主要作用是在分布式计算中减少通信开销，也可以减少反向传播时的开销。\n\n\n\n# 量化形式\n\n## 根据量化数据表示的原始数据范围是否均匀，可以将量化方法分为线性量化和非线性量化\n![Pasted image 20250501212209.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212209.png)\n深度神经网络的权重和激活值通常是不均匀的，因此理论上使用非线性量化导致的精度损失更小，但在实际推理中非线性量化的计算复杂度较高，通常使用线性量化：\n\n$$\nq = \\text{clip}(\\text{round}(s \\cdot r + z), q_{\\min}, q_{\\max})\n$$\n\n其中 $r$ 为量化前的浮点数，$q$ 为量化后的整数，$\\text{round}$ 表示取整，$\\text{clip}$ 为截断，$s$ 为数据量化的间隔，$z$ 为数据偏移，为0的时候为对称量化，不为0的时候为不对称量化。对称量化可以避免量化算子在推理中计算 $z$ 相关的部分，降低推理时的计算复杂度；非对称量化可以根据实际数据的分布确定最小值和最大值，可以更加充分的利用量化数据信息，使得量化导致的损失更低。\n![Pasted image 20250501212223.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212223.png)\n\n\n## 根据 $s$ 和 $z$ 的共享范围即量化粒度，量化方法可以进行以下分类\n\n### 逐层量化 per-tensor\n范围最大，最简单，以一层网络为单位一组量化参数。\n![Pasted image 20250501212233.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212233.png)\n\n\n### 逐通道量化 per-token & per-channel\n以一层网络的每个量化通道为单位，每个通道单独用一组量化参数。量化粒度更细，更高的量化精度，计算也更复杂。其中，per-token 针对激活 $x$ 而言，每行对应一个量化系数；per-channel 针对权重 $w$ 而言，每列对应一个量化系数。结合使用也叫 vector-wise。\n![Pasted image 20250501212242.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212242.png)\n\n\n### 逐组量化 per-group\n以组为单位，每个组比如 $K$ 行激活值或 $K$ 列权重使用一组 $s$ 和 $z$；它的粒度处于 per-tensor 和 vector-wise之间。当 groupsize=1 时，逐组量化与逐层量化等价；当 groupsize= 卷积核的数量时，逐组量化与逐通道量化等价。\n\n此外激活值和权重可以选择不同的粒度进行量化，对于激活值来说还有动态量化（推理过程中，实时计算激活的量化系数，对激活进行量化）与静态量化（在推理前就计算好激活的量化系数，在推理过程中应用）。\n\n\n## Llama3 技术报告中提供的 tensor-wise 和 row-wise FP8 量化示意图\n在 Llama3 技术报告中，提供了 tensor-wise 和 row-wise 的 FP8 量化示意图，这些示意图展示了不同粒度下的浮点数到整数的转换过程，帮助理解不同粒度选择对于模型性能和计算复杂度的影响。\n![Pasted image 20250501212137.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212137.png)\n\n\n\n# 量化分类\n在深度学习模型的部署中，模型的大小和推理速度是两个非常重要的因素。为了在不显著损失模型准确率的前提下减小模型大小并加快推理速度，量化技术应运而生。量化技术通过将模型参数和计算从浮点数表示转换为低精度表示（如 8 位整数）来实现这一目标。根据量化压缩模型的阶段，量化可以分为以下几类：\n\n## 量化感知训练 Quantization Aware Training (QAT)\n量化感知训练（QAT）是在模型训练过程中引入量化的意识。具体来说，在训练过程中，尽管模型的权重和激活值仍然以浮点数形式存储和更新，但在前向传播时会模拟低精度运算。这种方法使得模型在训练过程中就能够适应量化带来的噪声，从而在部署时能够更好地保持性能。\n\nQAT 的主要优势在于其能够在保持模型准确率的同时实现显著的压缩效果。然而，由于需要在训练过程中进行额外的模拟计算，QAT 的训练时间可能会有所增加。\n\n\n## 量化感知微调 Quantization-Aware Fine-tuning (QAF)\n量化感知微调（QAF）是一种在已有的预训练模型基础上进行微调的量化方法。与 QAT 类似，QAF 在微调过程中也会引入量化的意识。然而，由于其基于预训练模型进行微调，因此训练时间通常较短。\n\nQAF 的优势在于其能够在较短的时间内获得接近 QAT 的量化效果，是一种兼顾效率与性能的方法。\n\n\n## 训练后量化 Post Training Quantization (PTQ)\n训练后量化（PTQ）是指在模型训练完成后，对其进行量化处理的一种方法。PTQ 不需要在训练过程中考虑量化问题，因此可以直接应用于任何已经训练好的模型。\n\n虽然 PTQ 的实施非常简单，并且不会增加训练时间，但其效果通常不如 QAT 和 QAF，因为模型在训练过程中没有机会适应量化引入的噪声。然而，对于一些特定的应用场景，PTQ 仍然是一个快速而有效的选择。\n\n\n\n# QAT 量化感知训练\n在深度学习模型的训练和部署过程中，模型的精度和计算效率往往是两个重要的考量因素。为了在这两者之间取得平衡，量化感知训练（Quantization Aware Training, QAT）成为了一种有效的方法。本文将介绍QAT的基本原理及其在大语言模型（LLM）中的具体应用。\n\n## 量化感知训练的基本原理\n量化感知训练首先需要对模型进行正常的预训练。在此基础上，模型中会插入“伪量化节点”，即对权重和激活进行量化和反量化操作。这种方法的目的在于引入量化误差，使得模型在训练过程中能够“感知”到量化操作，从而在优化训练误差的同时兼顾量化误差。这种方法特别适用于对模型精度要求较高的场景，其量化目标无缝地集成到模型的训练过程中，使得大语言模型（LLM）在训练过程中能适应低精度表示，增强其处理由量化引起的精度损失的能力。\n\n\n## QAT方法\n\n### LLM-QAT\nLLM-QAT是一种利用预训练模型生成结果来实现无数据蒸馏的方法。通过这种方法，我们不仅可以量化模型的权重和激活，还可以量化KV缓存。这样的策略旨在增强吞吐量并支持更长的序列。这意味着，LLM-QAT能够将带有权重和KV缓存量化的LLaMA模型蒸馏为仅有4比特的模型。\n\n这种方法不仅提高了计算效率，还在一定程度上保留了模型精度，使得在资源受限的环境中部署大规模语言模型成为可能。\n![Pasted image 20250501212325.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212325.png)\n\n\n![Pasted image 20250501212339.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212339.png)\n\n\n\n# QAF 量化感知微调\n在微调过程中对大型语言模型（LLM）进行量化的主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，可以在模型压缩和保持性能之间取得平衡。\n\n## QAF方法\n\n### PEQA：Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\nPEQA是一种新的量化感知微调技术，可以促进模型压缩并加速推理。它采用了双阶段过程运行：\n\n1. **第一阶段**：每个全连接层的参数矩阵被量化为低比特整数矩阵和标量向量。\n2. **第二阶段**：对每个特定下游任务的标量向量进行微调。\n\n这种策略大大压缩了模型的大小，从而降低了部署时的推理延迟并减少了所需的总体内存。同时，使快速的微调和高效的任务切换成为可能。\n\n![Pasted image 20250501212427.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212427.png)\n\n\n### QLoRA\nQLoRA也是一种QAF方法，具体内容可以参考3.5.6章节。\n\n\n##  PEFT 参数高效微调\n在参数高效微调（PEFT）中，目标是通过最小化参数调整来实现最大化的模型性能改进。这种方法不仅节省计算资源，还能在多任务环境中灵活应用。QLoRA作为QAF方法的一种实现，体现了PEFT的理念。\n\n\n\n# PTQ 训练后量化\n在深度学习领域，特别是大规模语言模型（LLM）的应用中，模型的存储和计算成本一直是一个重要的挑战。为了应对这一挑战，量化技术被广泛应用于减少模型的复杂性和提高效率。本文将探讨几种常用的训练后量化（PTQ）方法。\n\n## QAT 插入“伪量化节点”后微调\n量化感知训练（QAT）通过在训练过程中插入“伪量化节点”来模拟量化效果，以便在推理阶段更好地适应量化后的模型。然而，这种方法大大增加了计算成本，尤其是在面对超大规模的 LLM 时。目前，针对 LLM 的量化研究主要集中在训练后量化（PTQ），例如 LLM.int8()、SmoothQuant 和 GPT-Q。对于权重而言，可以在推理前事先计算好量化系数，完成量化。但是对于激活值（即各层的输入），它们事先是未知的，取决于具体的推理输入。\n\n\n## 训练后量化（PTQ）\n在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性，但PTQ可能会在量化过程中引入一定程度的精度损失。\n\n### PTQ方法\n\n#### LLM.int8()\n在激活值 $X$ 中存在一些离群值，它们的绝对值明显更大；并且这些离群值分布在少量的几个特征中，称为离群特征。观察下图中黄色的离群值，不论是 per-token 还是 per-channel 量化，都会受到这些离群值的很大影响。LLM.int8() 的思路是，既然只有少量的特征包含离群值，那就把这些特征拿出来单独计算，只对剩余特征做量化，即采用混合精度分解的量化方法。先做一个矩阵分解，对绝大部分权重和激活用8比特量化（vector-wise），对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。\n![Pasted image 20250501212515.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212515.png)\n\n\n#### SmoothQuant\n针对激活中的离群值，SmoothQuant 给出了与 LLM.int8() 不同的解题思路。激活值的量化比权重的量化难得多，可以通过一个平滑系数，把二者的难度中和一下：\n\n$$\nY = ( X \\cdot \\text{diag}(s)^{-1} ) \\cdot (\\text{diag}(s) \\cdot W) = \\hat{X} \\cdot \\hat{W}\n$$\n\n其中，平滑系数 $s_j$ 的计算为：\n\n$$\ns_j = \\frac{\\max(|X_j|)^\\alpha}{\\max(|W_j|)^{1-\\alpha}}\n$$\n\n对平滑后的激活值和权重进行量化即可。权重采用 per-tensor 方式，激活采用不同粒度、不同时机的量化有不同版本。\n![Pasted image 20250501212527.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212527.png)\n\n![Pasted image 20250501212534.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212534.png)\n\n\n#### GPT-Q\nLLM.int8() 和 SmoothQuant 都属于 round-to-nearest (RTN) 量化：舍入到最近的定点数。GPT-Q 则是把量化问题视作优化问题，逐层寻找最优的量化权重。对某个块（block）内的所有参数逐个量化，每个参数量化后，需要适当调整这个块内其他未量化的参数，以弥补量化造成的精度损失。此外，GPTQ 量化需要准备校准数据集。\n![Pasted image 20250501212543.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212543.png)\n\n\nAWQ： 对于LLM的性能， 权重并不是同等重要的，通过保留$$1\\%$$的显著权重可以大大减少量化误差。在此基础上，AWQ采用了激活感知方法，考虑与较大激活幅度对应的权重通道的重要性，这在处理重要特征时起着关键作用。采用逐通道缩放技术来确定最佳缩放因子，从而在量化所有权重的同时最小化量化误差。\n\n$$\n\\text{量化误差} = \\sum_{i=1}^{n} |\\text{原始权重}_i - \\text{量化权重}_i|\n$$\n![Pasted image 20250501212610.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250501212610.png)","excerpt":"","includedFiles":[],"tasklistId":0,"title":"量化形式","headers":[{"level":2,"title":"模型量化","slug":"模型量化","link":"#模型量化","children":[]},{"level":2,"title":"常用的数据类型","slug":"常用的数据类型","link":"#常用的数据类型","children":[]},{"level":2,"title":"量化对象","slug":"量化对象","link":"#量化对象","children":[]},{"level":2,"title":"根据量化数据表示的原始数据范围是否均匀，可以将量化方法分为线性量化和非线性量化","slug":"根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化","link":"#根据量化数据表示的原始数据范围是否均匀-可以将量化方法分为线性量化和非线性量化","children":[]},{"level":2,"title":"根据  和  的共享范围即量化粒度，量化方法可以进行以下分类","slug":"根据-和-的共享范围即量化粒度-量化方法可以进行以下分类","link":"#根据-和-的共享范围即量化粒度-量化方法可以进行以下分类","children":[{"level":3,"title":"逐层量化 per-tensor","slug":"逐层量化-per-tensor","link":"#逐层量化-per-tensor","children":[]},{"level":3,"title":"逐通道量化 per-token & per-channel","slug":"逐通道量化-per-token-per-channel","link":"#逐通道量化-per-token-per-channel","children":[]},{"level":3,"title":"逐组量化 per-group","slug":"逐组量化-per-group","link":"#逐组量化-per-group","children":[]}]},{"level":2,"title":"Llama3 技术报告中提供的 tensor-wise 和 row-wise FP8 量化示意图","slug":"llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图","link":"#llama3-技术报告中提供的-tensor-wise-和-row-wise-fp8-量化示意图","children":[]},{"level":2,"title":"量化感知训练 Quantization Aware Training (QAT)","slug":"量化感知训练-quantization-aware-training-qat","link":"#量化感知训练-quantization-aware-training-qat","children":[]},{"level":2,"title":"量化感知微调 Quantization-Aware Fine-tuning (QAF)","slug":"量化感知微调-quantization-aware-fine-tuning-qaf","link":"#量化感知微调-quantization-aware-fine-tuning-qaf","children":[]},{"level":2,"title":"训练后量化 Post Training Quantization (PTQ)","slug":"训练后量化-post-training-quantization-ptq","link":"#训练后量化-post-training-quantization-ptq","children":[]},{"level":2,"title":"量化感知训练的基本原理","slug":"量化感知训练的基本原理","link":"#量化感知训练的基本原理","children":[]},{"level":2,"title":"QAT方法","slug":"qat方法","link":"#qat方法","children":[{"level":3,"title":"LLM-QAT","slug":"llm-qat","link":"#llm-qat","children":[]}]},{"level":2,"title":"QAF方法","slug":"qaf方法","link":"#qaf方法","children":[{"level":3,"title":"PEQA：Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization","slug":"peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization","link":"#peqa-memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization","children":[]},{"level":3,"title":"QLoRA","slug":"qlora","link":"#qlora","children":[]}]},{"level":2,"title":"PEFT 参数高效微调","slug":"peft-参数高效微调","link":"#peft-参数高效微调","children":[]},{"level":2,"title":"QAT 插入“伪量化节点”后微调","slug":"qat-插入-伪量化节点-后微调","link":"#qat-插入-伪量化节点-后微调","children":[]},{"level":2,"title":"训练后量化（PTQ）","slug":"训练后量化-ptq","link":"#训练后量化-ptq","children":[{"level":3,"title":"PTQ方法","slug":"ptq方法","link":"#ptq方法","children":[]}]}]}}
