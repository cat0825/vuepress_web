{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：人工智能/大语言模型</li>\n<li><strong>标签</strong>：大语言模型、混合专家、Transformer、MoE</li>\n<li><strong>日期</strong>：2025年4月8日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>本文解析了大语言模型（LLM）的四种主要结构及其特点，同时介绍了混合专家（MoE）架构的设计理念和技术细节。文章还探讨了不同模型结构在理解和生成任务中的应用场景，以及如何通过 MoE 提升模型性能与计算效率。<img src=\"/img/user/附件/Pasted image 20250407174844.png\" alt=\"Pasted image 20250407174844.png\"></p>\n<hr>\n<h2 id=\"模型结构分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构分类\"><span>模型结构分类</span></a></h2>\n<h3 id=\"decoder-only-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#decoder-only-模型\"><span>Decoder-only 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>使用单向注意力机制（从左到右）。</li>\n<li>模型训练和下游应用一致，适合文本生成任务。</li>\n<li>高效的训练流程，具备强大的零样本（zero-shot）能力。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：GPT、Llama、BLOOM、OPT</li>\n</ul>\n<h3 id=\"encoder-only-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#encoder-only-模型\"><span>Encoder-only 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>以语言表征为目标，主要用于提取文本特征。</li>\n<li>适合理解任务，但生成能力较弱。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：BERT</li>\n</ul>\n<h3 id=\"encoder-decoder-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#encoder-decoder-模型\"><span>Encoder-Decoder 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>输入采用双向注意力，输出为单向注意力。</li>\n<li>在需要深度理解的任务上表现更优，但训练效率低，文本生成效果一般。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：T5、Flan-T5、BART</li>\n</ul>\n<h3 id=\"prefix-lm-前缀语言模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#prefix-lm-前缀语言模型\"><span>Prefix LM（前缀语言模型）</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>可以看作 Encoder-Decoder 的特例，权衡理解与生成能力。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：GLM、U-PaLM</li>\n</ul>\n<hr>\n<h2 id=\"混合专家-moe-架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#混合专家-moe-架构\"><span>混合专家（MoE）架构</span></a></h2>\n<h3 id=\"什么是-moe\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#什么是-moe\"><span>什么是 MoE？</span></a></h3>\n<p>MoE 是一种通过引入多个专家网络（Experts）和门控网络（Gate）来提升计算效率的模型架构。其核心思想是根据输入特征选择性地激活部分专家网络参与计算，而非所有网络。\n<img src=\"/img/user/附件/Pasted image 20250407174905.png\" alt=\"Pasted image 20250407174905.png\"></p>\n<h3 id=\"构成要素\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#构成要素\"><span>构成要素</span></a></h3>\n<ol>\n<li><strong>专家网络</strong>：多个独立的子网络，专注于处理特定类型的输入。</li>\n<li><strong>门控网络</strong>：\n<ul>\n<li>通过 Softmax 激活函数选择合适的专家网络。</li>\n<li>有三种模式：\n<ul>\n<li>稀疏式：仅激活部分专家。</li>\n<li>密集式：激活所有专家。</li>\n<li>Soft 式：可微分的融合方法。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"放置位置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#放置位置\"><span>放置位置</span></a></h3>\n<p>MoE 层通常放置在 Transformer 模块中的自注意力（SA）子层之后，用于优化前向传播网络（FFN）的计算效率。</p>\n<h3 id=\"应用场景\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#应用场景\"><span>应用场景</span></a></h3>\n<p>在参数量极大的模型中，例如 PaLM（5400 亿参数），MoE 能显著降低计算成本。PaLM 的 FFN 层占据了总参数量的 90%。</p>\n<hr>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ <strong>误区提醒</strong>：将 MoE 的稀疏激活机制误解为随机选择专家，而非基于输入特征的动态路由。</p>\n</blockquote>\n<hr>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ <strong>选择模型架构</strong>：根据任务需求选择 Decoder-only、Encoder-only 或 Encoder-Decoder 等架构。</li>\n<li>✅ <strong>设计 MoE 层</strong>：\n<ul>\n<li>确定专家网络数目 $$N$$。</li>\n<li>定义门控网络的类型（稀疏式、密集式或 Soft 式）。</li>\n</ul>\n</li>\n<li>❗ <strong>优化放置位置</strong>：\n<ul>\n<li>将 MoE 层嵌入 Transformer 的自注意力子层之后。</li>\n</ul>\n</li>\n<li>✅ <strong>测试与调优</strong>：\n<ul>\n<li>使用不同任务场景验证模型性能，例如生成与理解任务。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格示例\"><span>数据表格示例</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型类型</th>\n<th>注意力机制</th>\n<th>优势</th>\n<th>典型模型</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Decoder-only</td>\n<td>单向注意力</td>\n<td>文本生成强，效率高</td>\n<td>GPT、Llama</td>\n</tr>\n<tr>\n<td>Encoder-only</td>\n<td>双向注意力</td>\n<td>表征提取优，理解能力强</td>\n<td>BERT</td>\n</tr>\n<tr>\n<td>Encoder-Decoder</td>\n<td>双向（输入）+单向（输出）</td>\n<td>深度理解，适合问答任务</td>\n<td>T5、BART</td>\n</tr>\n<tr>\n<td>Prefix LM</td>\n<td>特殊的 Encoder-Decoder</td>\n<td>平衡理解与生成能力</td>\n<td>GLM、U-PaLM</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"📈-趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-趋势预测\"><span>📈 趋势预测</span></a></h2>\n<ol>\n<li>随着模型参数量的持续增长，MoE 将成为提升计算效率的核心技术之一。</li>\n<li>更高效的稀疏门控机制可能会被开发，用于进一步减少计算成本。</li>\n<li>Prefix LM 或类似架构可能在多模态任务中获得更广泛应用。</li>\n</ol>\n<hr>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ol>\n<li>MoE 架构通过“选择性激活”提升了大模型的效率，这是解决超大规模计算瓶颈的关键思路。</li>\n<li>不同任务场景对模型架构提出了差异化需求，未来可能会出现更多“混合型”架构。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何改进现有的稀疏门控机制，使其更高效且不损失性能？</li>\n<li>在小规模数据集上，是否存在轻量化 MoE 的实现方案？</li>\n<li>Prefix LM 是否可以进一步优化以提升训练效率？</li>\n</ol>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>学习并实现一个简单的 MoE 模型，测试其在小规模数据集上的效果。</li>\n<li>深入研究 Prefix LM 的架构设计，探索其在多模态任务中的潜力。</li>\n<li>跟踪最新的 LLM 和 MoE 技术发展动态。</li>\n</ol>\n<hr>\n<blockquote>\n<p>来源：整理自技术文档《大模型结构与混合专家》</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Structure & Decoding Policy 结构和解码策略/大模型结构与混合专家（LLM & MoE）解析.md","filePathRelative":"notes_bak/大语言模型学习/Structure & Decoding Policy 结构和解码策略/大模型结构与混合专家（LLM & MoE）解析.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Structure-&-Decoding-Policy-结构和解码策略/大模型结构与混合专家（LLM-&-MoE）解析","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Structure-&-Decoding-Policy-结构和解码策略/大模型结构与混合专家（LLM-&-MoE）解析/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-07T09:47:22.000Z","updated":"2025-04-13T05:06:02.000Z","title":"大模型结构与混合专家（LLM & MoE）解析","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：人工智能/大语言模型</li>\n<li><strong>标签</strong>：大语言模型、混合专家、Transformer、MoE</li>\n<li><strong>日期</strong>：2025年4月8日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>本文解析了大语言模型（LLM）的四种主要结构及其特点，同时介绍了混合专家（MoE）架构的设计理念和技术细节。文章还探讨了不同模型结构在理解和生成任务中的应用场景，以及如何通过 MoE 提升模型性能与计算效率。<img src=\"/img/user/附件/Pasted image 20250407174844.png\" alt=\"Pasted image 20250407174844.png\"></p>\n<hr>\n<h2 id=\"模型结构分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构分类\"><span>模型结构分类</span></a></h2>\n<h3 id=\"decoder-only-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#decoder-only-模型\"><span>Decoder-only 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>使用单向注意力机制（从左到右）。</li>\n<li>模型训练和下游应用一致，适合文本生成任务。</li>\n<li>高效的训练流程，具备强大的零样本（zero-shot）能力。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：GPT、Llama、BLOOM、OPT</li>\n</ul>\n<h3 id=\"encoder-only-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#encoder-only-模型\"><span>Encoder-only 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>以语言表征为目标，主要用于提取文本特征。</li>\n<li>适合理解任务，但生成能力较弱。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：BERT</li>\n</ul>\n<h3 id=\"encoder-decoder-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#encoder-decoder-模型\"><span>Encoder-Decoder 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>输入采用双向注意力，输出为单向注意力。</li>\n<li>在需要深度理解的任务上表现更优，但训练效率低，文本生成效果一般。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：T5、Flan-T5、BART</li>\n</ul>\n<h3 id=\"prefix-lm-前缀语言模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#prefix-lm-前缀语言模型\"><span>Prefix LM（前缀语言模型）</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>可以看作 Encoder-Decoder 的特例，权衡理解与生成能力。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：GLM、U-PaLM</li>\n</ul>\n<hr>\n<h2 id=\"混合专家-moe-架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#混合专家-moe-架构\"><span>混合专家（MoE）架构</span></a></h2>\n<h3 id=\"什么是-moe\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#什么是-moe\"><span>什么是 MoE？</span></a></h3>\n<p>MoE 是一种通过引入多个专家网络（Experts）和门控网络（Gate）来提升计算效率的模型架构。其核心思想是根据输入特征选择性地激活部分专家网络参与计算，而非所有网络。\n<img src=\"/img/user/附件/Pasted image 20250407174905.png\" alt=\"Pasted image 20250407174905.png\"></p>\n<h3 id=\"构成要素\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#构成要素\"><span>构成要素</span></a></h3>\n<ol>\n<li><strong>专家网络</strong>：多个独立的子网络，专注于处理特定类型的输入。</li>\n<li><strong>门控网络</strong>：\n<ul>\n<li>通过 Softmax 激活函数选择合适的专家网络。</li>\n<li>有三种模式：\n<ul>\n<li>稀疏式：仅激活部分专家。</li>\n<li>密集式：激活所有专家。</li>\n<li>Soft 式：可微分的融合方法。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"放置位置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#放置位置\"><span>放置位置</span></a></h3>\n<p>MoE 层通常放置在 Transformer 模块中的自注意力（SA）子层之后，用于优化前向传播网络（FFN）的计算效率。</p>\n<h3 id=\"应用场景\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#应用场景\"><span>应用场景</span></a></h3>\n<p>在参数量极大的模型中，例如 PaLM（5400 亿参数），MoE 能显著降低计算成本。PaLM 的 FFN 层占据了总参数量的 90%。</p>\n<hr>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ <strong>误区提醒</strong>：将 MoE 的稀疏激活机制误解为随机选择专家，而非基于输入特征的动态路由。</p>\n</blockquote>\n<hr>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ <strong>选择模型架构</strong>：根据任务需求选择 Decoder-only、Encoder-only 或 Encoder-Decoder 等架构。</li>\n<li>✅ <strong>设计 MoE 层</strong>：\n<ul>\n<li>确定专家网络数目 $$N$$。</li>\n<li>定义门控网络的类型（稀疏式、密集式或 Soft 式）。</li>\n</ul>\n</li>\n<li>❗ <strong>优化放置位置</strong>：\n<ul>\n<li>将 MoE 层嵌入 Transformer 的自注意力子层之后。</li>\n</ul>\n</li>\n<li>✅ <strong>测试与调优</strong>：\n<ul>\n<li>使用不同任务场景验证模型性能，例如生成与理解任务。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格示例\"><span>数据表格示例</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型类型</th>\n<th>注意力机制</th>\n<th>优势</th>\n<th>典型模型</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Decoder-only</td>\n<td>单向注意力</td>\n<td>文本生成强，效率高</td>\n<td>GPT、Llama</td>\n</tr>\n<tr>\n<td>Encoder-only</td>\n<td>双向注意力</td>\n<td>表征提取优，理解能力强</td>\n<td>BERT</td>\n</tr>\n<tr>\n<td>Encoder-Decoder</td>\n<td>双向（输入）+单向（输出）</td>\n<td>深度理解，适合问答任务</td>\n<td>T5、BART</td>\n</tr>\n<tr>\n<td>Prefix LM</td>\n<td>特殊的 Encoder-Decoder</td>\n<td>平衡理解与生成能力</td>\n<td>GLM、U-PaLM</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"📈-趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-趋势预测\"><span>📈 趋势预测</span></a></h2>\n<ol>\n<li>随着模型参数量的持续增长，MoE 将成为提升计算效率的核心技术之一。</li>\n<li>更高效的稀疏门控机制可能会被开发，用于进一步减少计算成本。</li>\n<li>Prefix LM 或类似架构可能在多模态任务中获得更广泛应用。</li>\n</ol>\n<hr>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ol>\n<li>MoE 架构通过“选择性激活”提升了大模型的效率，这是解决超大规模计算瓶颈的关键思路。</li>\n<li>不同任务场景对模型架构提出了差异化需求，未来可能会出现更多“混合型”架构。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何改进现有的稀疏门控机制，使其更高效且不损失性能？</li>\n<li>在小规模数据集上，是否存在轻量化 MoE 的实现方案？</li>\n<li>Prefix LM 是否可以进一步优化以提升训练效率？</li>\n</ol>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>学习并实现一个简单的 MoE 模型，测试其在小规模数据集上的效果。</li>\n<li>深入研究 Prefix LM 的架构设计，探索其在多模态任务中的潜力。</li>\n<li>跟踪最新的 LLM 和 MoE 技术发展动态。</li>\n</ol>\n<hr>\n<blockquote>\n<p>来源：整理自技术文档《大模型结构与混合专家》</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：人工智能/大语言模型</li>\n<li><strong>标签</strong>：大语言模型、混合专家、Transformer、MoE</li>\n<li><strong>日期</strong>：2025年4月8日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>本文解析了大语言模型（LLM）的四种主要结构及其特点，同时介绍了混合专家（MoE）架构的设计理念和技术细节。文章还探讨了不同模型结构在理解和生成任务中的应用场景，以及如何通过 MoE 提升模型性能与计算效率。<img src=\"/img/user/附件/Pasted image 20250407174844.png\" alt=\"Pasted image 20250407174844.png\"></p>\n<hr>\n<h2 id=\"模型结构分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构分类\"><span>模型结构分类</span></a></h2>\n<h3 id=\"decoder-only-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#decoder-only-模型\"><span>Decoder-only 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>使用单向注意力机制（从左到右）。</li>\n<li>模型训练和下游应用一致，适合文本生成任务。</li>\n<li>高效的训练流程，具备强大的零样本（zero-shot）能力。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：GPT、Llama、BLOOM、OPT</li>\n</ul>\n<h3 id=\"encoder-only-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#encoder-only-模型\"><span>Encoder-only 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>以语言表征为目标，主要用于提取文本特征。</li>\n<li>适合理解任务，但生成能力较弱。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：BERT</li>\n</ul>\n<h3 id=\"encoder-decoder-模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#encoder-decoder-模型\"><span>Encoder-Decoder 模型</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>输入采用双向注意力，输出为单向注意力。</li>\n<li>在需要深度理解的任务上表现更优，但训练效率低，文本生成效果一般。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：T5、Flan-T5、BART</li>\n</ul>\n<h3 id=\"prefix-lm-前缀语言模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#prefix-lm-前缀语言模型\"><span>Prefix LM（前缀语言模型）</span></a></h3>\n<ul>\n<li><strong>特点</strong>：\n<ul>\n<li>可以看作 Encoder-Decoder 的特例，权衡理解与生成能力。</li>\n</ul>\n</li>\n<li><strong>典型模型</strong>：GLM、U-PaLM</li>\n</ul>\n<hr>\n<h2 id=\"混合专家-moe-架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#混合专家-moe-架构\"><span>混合专家（MoE）架构</span></a></h2>\n<h3 id=\"什么是-moe\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#什么是-moe\"><span>什么是 MoE？</span></a></h3>\n<p>MoE 是一种通过引入多个专家网络（Experts）和门控网络（Gate）来提升计算效率的模型架构。其核心思想是根据输入特征选择性地激活部分专家网络参与计算，而非所有网络。\n<img src=\"/img/user/附件/Pasted image 20250407174905.png\" alt=\"Pasted image 20250407174905.png\"></p>\n<h3 id=\"构成要素\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#构成要素\"><span>构成要素</span></a></h3>\n<ol>\n<li><strong>专家网络</strong>：多个独立的子网络，专注于处理特定类型的输入。</li>\n<li><strong>门控网络</strong>：\n<ul>\n<li>通过 Softmax 激活函数选择合适的专家网络。</li>\n<li>有三种模式：\n<ul>\n<li>稀疏式：仅激活部分专家。</li>\n<li>密集式：激活所有专家。</li>\n<li>Soft 式：可微分的融合方法。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"放置位置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#放置位置\"><span>放置位置</span></a></h3>\n<p>MoE 层通常放置在 Transformer 模块中的自注意力（SA）子层之后，用于优化前向传播网络（FFN）的计算效率。</p>\n<h3 id=\"应用场景\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#应用场景\"><span>应用场景</span></a></h3>\n<p>在参数量极大的模型中，例如 PaLM（5400 亿参数），MoE 能显著降低计算成本。PaLM 的 FFN 层占据了总参数量的 90%。</p>\n<hr>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ <strong>误区提醒</strong>：将 MoE 的稀疏激活机制误解为随机选择专家，而非基于输入特征的动态路由。</p>\n</blockquote>\n<hr>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ <strong>选择模型架构</strong>：根据任务需求选择 Decoder-only、Encoder-only 或 Encoder-Decoder 等架构。</li>\n<li>✅ <strong>设计 MoE 层</strong>：\n<ul>\n<li>确定专家网络数目 $$N$$。</li>\n<li>定义门控网络的类型（稀疏式、密集式或 Soft 式）。</li>\n</ul>\n</li>\n<li>❗ <strong>优化放置位置</strong>：\n<ul>\n<li>将 MoE 层嵌入 Transformer 的自注意力子层之后。</li>\n</ul>\n</li>\n<li>✅ <strong>测试与调优</strong>：\n<ul>\n<li>使用不同任务场景验证模型性能，例如生成与理解任务。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格示例\"><span>数据表格示例</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型类型</th>\n<th>注意力机制</th>\n<th>优势</th>\n<th>典型模型</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Decoder-only</td>\n<td>单向注意力</td>\n<td>文本生成强，效率高</td>\n<td>GPT、Llama</td>\n</tr>\n<tr>\n<td>Encoder-only</td>\n<td>双向注意力</td>\n<td>表征提取优，理解能力强</td>\n<td>BERT</td>\n</tr>\n<tr>\n<td>Encoder-Decoder</td>\n<td>双向（输入）+单向（输出）</td>\n<td>深度理解，适合问答任务</td>\n<td>T5、BART</td>\n</tr>\n<tr>\n<td>Prefix LM</td>\n<td>特殊的 Encoder-Decoder</td>\n<td>平衡理解与生成能力</td>\n<td>GLM、U-PaLM</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"📈-趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-趋势预测\"><span>📈 趋势预测</span></a></h2>\n<ol>\n<li>随着模型参数量的持续增长，MoE 将成为提升计算效率的核心技术之一。</li>\n<li>更高效的稀疏门控机制可能会被开发，用于进一步减少计算成本。</li>\n<li>Prefix LM 或类似架构可能在多模态任务中获得更广泛应用。</li>\n</ol>\n<hr>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<ol>\n<li>MoE 架构通过“选择性激活”提升了大模型的效率，这是解决超大规模计算瓶颈的关键思路。</li>\n<li>不同任务场景对模型架构提出了差异化需求，未来可能会出现更多“混合型”架构。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何改进现有的稀疏门控机制，使其更高效且不损失性能？</li>\n<li>在小规模数据集上，是否存在轻量化 MoE 的实现方案？</li>\n<li>Prefix LM 是否可以进一步优化以提升训练效率？</li>\n</ol>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>学习并实现一个简单的 MoE 模型，测试其在小规模数据集上的效果。</li>\n<li>深入研究 Prefix LM 的架构设计，探索其在多模态任务中的潜力。</li>\n<li>跟踪最新的 LLM 和 MoE 技术发展动态。</li>\n</ol>\n<hr>\n<blockquote>\n<p>来源：整理自技术文档《大模型结构与混合专家》</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 元数据\n- **分类**：人工智能/大语言模型\n- **标签**：大语言模型、混合专家、Transformer、MoE\n- **日期**：2025年4月8日\n\n---\n\n\n\n## 核心内容总结\n本文解析了大语言模型（LLM）的四种主要结构及其特点，同时介绍了混合专家（MoE）架构的设计理念和技术细节。文章还探讨了不同模型结构在理解和生成任务中的应用场景，以及如何通过 MoE 提升模型性能与计算效率。![Pasted image 20250407174844.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250407174844.png)\n\n---\n\n\n\n## 模型结构分类\n\n### Decoder-only 模型\n- **特点**：\n  - 使用单向注意力机制（从左到右）。\n  - 模型训练和下游应用一致，适合文本生成任务。\n  - 高效的训练流程，具备强大的零样本（zero-shot）能力。\n- **典型模型**：GPT、Llama、BLOOM、OPT\n\n\n### Encoder-only 模型\n- **特点**：\n  - 以语言表征为目标，主要用于提取文本特征。\n  - 适合理解任务，但生成能力较弱。\n- **典型模型**：BERT\n\n\n### Encoder-Decoder 模型\n- **特点**：\n  - 输入采用双向注意力，输出为单向注意力。\n  - 在需要深度理解的任务上表现更优，但训练效率低，文本生成效果一般。\n- **典型模型**：T5、Flan-T5、BART\n\n\n### Prefix LM（前缀语言模型）\n- **特点**：\n  - 可以看作 Encoder-Decoder 的特例，权衡理解与生成能力。\n- **典型模型**：GLM、U-PaLM\n\n---\n\n\n\n## 混合专家（MoE）架构\n\n### 什么是 MoE？\nMoE 是一种通过引入多个专家网络（Experts）和门控网络（Gate）来提升计算效率的模型架构。其核心思想是根据输入特征选择性地激活部分专家网络参与计算，而非所有网络。\n![Pasted image 20250407174905.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250407174905.png)\n\n\n### 构成要素\n1. **专家网络**：多个独立的子网络，专注于处理特定类型的输入。\n2. **门控网络**：\n   - 通过 Softmax 激活函数选择合适的专家网络。\n   - 有三种模式：\n     - 稀疏式：仅激活部分专家。\n     - 密集式：激活所有专家。\n     - Soft 式：可微分的融合方法。\n\n\n### 放置位置\nMoE 层通常放置在 Transformer 模块中的自注意力（SA）子层之后，用于优化前向传播网络（FFN）的计算效率。\n\n\n### 应用场景\n在参数量极大的模型中，例如 PaLM（5400 亿参数），MoE 能显著降低计算成本。PaLM 的 FFN 层占据了总参数量的 90%。\n\n---\n\n\n\n## 常见错误\n> ⚠ **误区提醒**：将 MoE 的稀疏激活机制误解为随机选择专家，而非基于输入特征的动态路由。\n\n---\n\n\n\n## 操作步骤\n1. ✅ **选择模型架构**：根据任务需求选择 Decoder-only、Encoder-only 或 Encoder-Decoder 等架构。\n2. ✅ **设计 MoE 层**：\n   - 确定专家网络数目 $$N$$。\n   - 定义门控网络的类型（稀疏式、密集式或 Soft 式）。\n3. ❗ **优化放置位置**：\n   - 将 MoE 层嵌入 Transformer 的自注意力子层之后。\n4. ✅ **测试与调优**：\n   - 使用不同任务场景验证模型性能，例如生成与理解任务。\n\n---\n\n\n\n## 数据表格示例\n| 模型类型       | 注意力机制         | 优势                     | 典型模型           |\n|----------------|--------------------|--------------------------|--------------------|\n| Decoder-only   | 单向注意力         | 文本生成强，效率高       | GPT、Llama         |\n| Encoder-only   | 双向注意力         | 表征提取优，理解能力强   | BERT               |\n| Encoder-Decoder| 双向（输入）+单向（输出）| 深度理解，适合问答任务 | T5、BART           |\n| Prefix LM      | 特殊的 Encoder-Decoder | 平衡理解与生成能力       | GLM、U-PaLM        |\n\n---\n\n\n\n## 📈 趋势预测\n1. 随着模型参数量的持续增长，MoE 将成为提升计算效率的核心技术之一。\n2. 更高效的稀疏门控机制可能会被开发，用于进一步减少计算成本。\n3. Prefix LM 或类似架构可能在多模态任务中获得更广泛应用。\n\n---\n\n\n\n## 💡 启发点\n1. MoE 架构通过“选择性激活”提升了大模型的效率，这是解决超大规模计算瓶颈的关键思路。\n2. 不同任务场景对模型架构提出了差异化需求，未来可能会出现更多“混合型”架构。\n\n---\n\n\n\n## [思考] 延伸问题\n1. 如何改进现有的稀疏门控机制，使其更高效且不损失性能？\n2. 在小规模数据集上，是否存在轻量化 MoE 的实现方案？\n3. Prefix LM 是否可以进一步优化以提升训练效率？\n\n---\n\n\n\n## 行动清单\n1. 学习并实现一个简单的 MoE 模型，测试其在小规模数据集上的效果。\n2. 深入研究 Prefix LM 的架构设计，探索其在多模态任务中的潜力。\n3. 跟踪最新的 LLM 和 MoE 技术发展动态。\n\n---\n\n> 来源：整理自技术文档《大模型结构与混合专家》","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心内容总结","slug":"核心内容总结","link":"#核心内容总结","children":[]},{"level":2,"title":"模型结构分类","slug":"模型结构分类","link":"#模型结构分类","children":[{"level":3,"title":"Decoder-only 模型","slug":"decoder-only-模型","link":"#decoder-only-模型","children":[]},{"level":3,"title":"Encoder-only 模型","slug":"encoder-only-模型","link":"#encoder-only-模型","children":[]},{"level":3,"title":"Encoder-Decoder 模型","slug":"encoder-decoder-模型","link":"#encoder-decoder-模型","children":[]},{"level":3,"title":"Prefix LM（前缀语言模型）","slug":"prefix-lm-前缀语言模型","link":"#prefix-lm-前缀语言模型","children":[]}]},{"level":2,"title":"混合专家（MoE）架构","slug":"混合专家-moe-架构","link":"#混合专家-moe-架构","children":[{"level":3,"title":"什么是 MoE？","slug":"什么是-moe","link":"#什么是-moe","children":[]},{"level":3,"title":"构成要素","slug":"构成要素","link":"#构成要素","children":[]},{"level":3,"title":"放置位置","slug":"放置位置","link":"#放置位置","children":[]},{"level":3,"title":"应用场景","slug":"应用场景","link":"#应用场景","children":[]}]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"数据表格示例","slug":"数据表格示例","link":"#数据表格示例","children":[]},{"level":2,"title":"📈 趋势预测","slug":"📈-趋势预测","link":"#📈-趋势预测","children":[]},{"level":2,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]},{"level":2,"title":"[思考] 延伸问题","slug":"思考-延伸问题","link":"#思考-延伸问题","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
