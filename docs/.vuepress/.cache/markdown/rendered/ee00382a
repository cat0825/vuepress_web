{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能</li>\n<li>标签：开源模型、LLaMA、深度学习、语言模型</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>Deepseek-V1是基于LLaMA架构的开源语言模型，旨在通过长远发展理念进行扩展。模型采用了多种先进技术以优化性能和推理成本，并通过不同阶段的训练提升其在中英文指令数据上的表现。</p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>Deepseek-V1基于LLaMA架构，采用了以下技术：</p>\n<ul>\n<li><strong>Pre-RMSNorm</strong>：一种用于优化神经网络训练的正则化方法。</li>\n<li><strong>SwiGLU</strong>和<strong>RoPE</strong>：用于提升模型的非线性表达能力。</li>\n<li><strong>GQA</strong>：在67B参数模型中使用以降低推理成本。</li>\n<li><strong>BBPE算法</strong>：用于将文本分词，训练语料库约24GB，词汇表大小为102400。\n<img src=\"/img/user/附件/Pasted image 20250426221705.png\" alt=\"Pasted image 20250426221705.png\"></li>\n</ul>\n<h2 id=\"训练过程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练过程\"><span>训练过程</span></a></h2>\n<h3 id=\"sft训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sft训练\"><span>SFT训练</span></a></h3>\n<ul>\n<li>收集了1.5百万条中英文指令数据。</li>\n<li>微调7B参数模型进行4个epochs，67B参数模型进行2个epochs。</li>\n<li>学习率设置为1e-5和5e-6。</li>\n</ul>\n<h3 id=\"dpo训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#dpo训练\"><span>DPO训练</span></a></h3>\n<ul>\n<li>使用Deepseek Chat Models生成响应，构建偏好对。</li>\n<li>批量大小为512，学习率为5e-6。</li>\n</ul>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型参数</th>\n<th>微调周期</th>\n<th>学习率</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>7B</td>\n<td>4 epochs</td>\n<td>1e-5</td>\n</tr>\n<tr>\n<td>67B</td>\n<td>2 epochs</td>\n<td>5e-6</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#警告区块\"><span>警告区块</span></a></h2>\n<blockquote>\n<p>⚠ 在训练过程中，确保数据集的多样性和质量，以避免模型偏差。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>✅ 研究并实施Pre-RMSNorm、SwiGLU和RoPE在其他模型中的应用。</li>\n<li>✅ 测试GQA在不同规模模型中的推理成本优化效果。</li>\n<li>❗ 收集更多多样化的中英文指令数据以提升模型泛化能力。</li>\n</ul>\n<blockquote>\n<p>来源：DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</p>\n</blockquote>\n<p>💡启发点：通过结合多种优化技术，Deepseek-V1在性能和推理成本上取得了显著平衡，这为未来开源语言模型的发展提供了新思路。</p>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/DeepSeek系列/Deepseek-V1.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/DeepSeek系列/Deepseek-V1.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/DeepSeek系列/Deepseek-V1","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/DeepSeek系列/Deepseek-V1/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-26T14:16:35.378Z","updated":"2025-04-26T14:17:08.121Z","title":"Deepseek-V1","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能</li>\n<li>标签：开源模型、LLaMA、深度学习、语言模型</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>Deepseek-V1是基于LLaMA架构的开源语言模型，旨在通过长远发展理念进行扩展。模型采用了多种先进技术以优化性能和推理成本，并通过不同阶段的训练提升其在中英文指令数据上的表现。</p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>Deepseek-V1基于LLaMA架构，采用了以下技术：</p>\n<ul>\n<li><strong>Pre-RMSNorm</strong>：一种用于优化神经网络训练的正则化方法。</li>\n<li><strong>SwiGLU</strong>和<strong>RoPE</strong>：用于提升模型的非线性表达能力。</li>\n<li><strong>GQA</strong>：在67B参数模型中使用以降低推理成本。</li>\n<li><strong>BBPE算法</strong>：用于将文本分词，训练语料库约24GB，词汇表大小为102400。\n<img src=\"/img/user/附件/Pasted image 20250426221705.png\" alt=\"Pasted image 20250426221705.png\"></li>\n</ul>\n<h2 id=\"训练过程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练过程\"><span>训练过程</span></a></h2>\n<h3 id=\"sft训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sft训练\"><span>SFT训练</span></a></h3>\n<ul>\n<li>收集了1.5百万条中英文指令数据。</li>\n<li>微调7B参数模型进行4个epochs，67B参数模型进行2个epochs。</li>\n<li>学习率设置为1e-5和5e-6。</li>\n</ul>\n<h3 id=\"dpo训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#dpo训练\"><span>DPO训练</span></a></h3>\n<ul>\n<li>使用Deepseek Chat Models生成响应，构建偏好对。</li>\n<li>批量大小为512，学习率为5e-6。</li>\n</ul>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型参数</th>\n<th>微调周期</th>\n<th>学习率</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>7B</td>\n<td>4 epochs</td>\n<td>1e-5</td>\n</tr>\n<tr>\n<td>67B</td>\n<td>2 epochs</td>\n<td>5e-6</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#警告区块\"><span>警告区块</span></a></h2>\n<blockquote>\n<p>⚠ 在训练过程中，确保数据集的多样性和质量，以避免模型偏差。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>✅ 研究并实施Pre-RMSNorm、SwiGLU和RoPE在其他模型中的应用。</li>\n<li>✅ 测试GQA在不同规模模型中的推理成本优化效果。</li>\n<li>❗ 收集更多多样化的中英文指令数据以提升模型泛化能力。</li>\n</ul>\n<blockquote>\n<p>来源：DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</p>\n</blockquote>\n<p>💡启发点：通过结合多种优化技术，Deepseek-V1在性能和推理成本上取得了显著平衡，这为未来开源语言模型的发展提供了新思路。</p>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：人工智能</li>\n<li>标签：开源模型、LLaMA、深度学习、语言模型</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>Deepseek-V1是基于LLaMA架构的开源语言模型，旨在通过长远发展理念进行扩展。模型采用了多种先进技术以优化性能和推理成本，并通过不同阶段的训练提升其在中英文指令数据上的表现。</p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>Deepseek-V1基于LLaMA架构，采用了以下技术：</p>\n<ul>\n<li><strong>Pre-RMSNorm</strong>：一种用于优化神经网络训练的正则化方法。</li>\n<li><strong>SwiGLU</strong>和<strong>RoPE</strong>：用于提升模型的非线性表达能力。</li>\n<li><strong>GQA</strong>：在67B参数模型中使用以降低推理成本。</li>\n<li><strong>BBPE算法</strong>：用于将文本分词，训练语料库约24GB，词汇表大小为102400。\n<img src=\"/img/user/附件/Pasted image 20250426221705.png\" alt=\"Pasted image 20250426221705.png\"></li>\n</ul>\n<h2 id=\"训练过程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练过程\"><span>训练过程</span></a></h2>\n<h3 id=\"sft训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sft训练\"><span>SFT训练</span></a></h3>\n<ul>\n<li>收集了1.5百万条中英文指令数据。</li>\n<li>微调7B参数模型进行4个epochs，67B参数模型进行2个epochs。</li>\n<li>学习率设置为1e-5和5e-6。</li>\n</ul>\n<h3 id=\"dpo训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#dpo训练\"><span>DPO训练</span></a></h3>\n<ul>\n<li>使用Deepseek Chat Models生成响应，构建偏好对。</li>\n<li>批量大小为512，学习率为5e-6。</li>\n</ul>\n<h2 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>模型参数</th>\n<th>微调周期</th>\n<th>学习率</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>7B</td>\n<td>4 epochs</td>\n<td>1e-5</td>\n</tr>\n<tr>\n<td>67B</td>\n<td>2 epochs</td>\n<td>5e-6</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#警告区块\"><span>警告区块</span></a></h2>\n<blockquote>\n<p>⚠ 在训练过程中，确保数据集的多样性和质量，以避免模型偏差。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>✅ 研究并实施Pre-RMSNorm、SwiGLU和RoPE在其他模型中的应用。</li>\n<li>✅ 测试GQA在不同规模模型中的推理成本优化效果。</li>\n<li>❗ 收集更多多样化的中英文指令数据以提升模型泛化能力。</li>\n</ul>\n<blockquote>\n<p>来源：DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</p>\n</blockquote>\n<p>💡启发点：通过结合多种优化技术，Deepseek-V1在性能和推理成本上取得了显著平衡，这为未来开源语言模型的发展提供了新思路。</p>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：人工智能\n- 标签：开源模型、LLaMA、深度学习、语言模型\n- 日期：2025年4月12日\n\n\n## 内容概述\nDeepseek-V1是基于LLaMA架构的开源语言模型，旨在通过长远发展理念进行扩展。模型采用了多种先进技术以优化性能和推理成本，并通过不同阶段的训练提升其在中英文指令数据上的表现。\n\n\n## 模型结构\nDeepseek-V1基于LLaMA架构，采用了以下技术：\n- **Pre-RMSNorm**：一种用于优化神经网络训练的正则化方法。\n- **SwiGLU**和**RoPE**：用于提升模型的非线性表达能力。\n- **GQA**：在67B参数模型中使用以降低推理成本。\n- **BBPE算法**：用于将文本分词，训练语料库约24GB，词汇表大小为102400。\n![Pasted image 20250426221705.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250426221705.png)\n\n\n## 训练过程\n\n### SFT训练\n- 收集了1.5百万条中英文指令数据。\n- 微调7B参数模型进行4个epochs，67B参数模型进行2个epochs。\n- 学习率设置为1e-5和5e-6。\n\n\n### DPO训练\n- 使用Deepseek Chat Models生成响应，构建偏好对。\n- 批量大小为512，学习率为5e-6。\n\n\n## 数据表格\n| 模型参数 | 微调周期 | 学习率 |\n|----------|----------|--------|\n| 7B       | 4 epochs | 1e-5   |\n| 67B      | 2 epochs | 5e-6   |\n\n\n## 警告区块\n> ⚠ 在训练过程中，确保数据集的多样性和质量，以避免模型偏差。\n\n\n## 行动清单\n- ✅ 研究并实施Pre-RMSNorm、SwiGLU和RoPE在其他模型中的应用。\n- ✅ 测试GQA在不同规模模型中的推理成本优化效果。\n- ❗ 收集更多多样化的中英文指令数据以提升模型泛化能力。\n\n> 来源：DeepSeek LLM: Scaling Open-Source Language Models with Longtermism\n\n💡启发点：通过结合多种优化技术，Deepseek-V1在性能和推理成本上取得了显著平衡，这为未来开源语言模型的发展提供了新思路。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容概述","slug":"内容概述","link":"#内容概述","children":[]},{"level":2,"title":"模型结构","slug":"模型结构","link":"#模型结构","children":[]},{"level":2,"title":"训练过程","slug":"训练过程","link":"#训练过程","children":[{"level":3,"title":"SFT训练","slug":"sft训练","link":"#sft训练","children":[]},{"level":3,"title":"DPO训练","slug":"dpo训练","link":"#dpo训练","children":[]}]},{"level":2,"title":"数据表格","slug":"数据表格","link":"#数据表格","children":[]},{"level":2,"title":"警告区块","slug":"警告区块","link":"#警告区块","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
