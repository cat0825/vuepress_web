{"content":"<p>元数据：</p>\n<p>分类：人工智能技术</p>\n<p>标签：FlashAttention, Transformer, 计算优化, 显存节省, IO感知</p>\n<p>日期：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>FlashAttention提出了一种创新的注意力机制，通过加速计算和节省显存来优化Transformer模型。它的设计旨在解决随着序列长度 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 的二次增长所带来的资源和效率问题。与传统注意力机制不同，FlashAttention在保持结果精确的同时减少了显存复杂度和计算时间。</p>\n<h2 id=\"重点段落与数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落与数据\"><span>重点段落与数据</span></a></h2>\n<h3 id=\"加速计算-fast\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#加速计算-fast\"><span>加速计算（Fast）</span></a></h3>\n<ul>\n<li>FlashAttention通过IO感知减少HBM访问次数来加快计算速度，而不是减少计算量（FLOPs）。</li>\n<li>使用了分块技术（tiling）和算子融合来实现这一目标。</li>\n</ul>\n<h3 id=\"显存节省-memory-efficient\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存节省-memory-efficient\"><span>显存节省（Memory-efficient）</span></a></h3>\n<ul>\n<li>通过引入统计量，改变注意力机制的计算顺序，避免实例化注意力矩阵。</li>\n<li>显存复杂度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(N^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 降低到了 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n<h3 id=\"精确注意力-exact-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#精确注意力-exact-attention\"><span>精确注意力（Exact Attention）</span></a></h3>\n<ul>\n<li>FlashAttention与原生注意力的结果完全等价，不同于稀疏注意力，它只是分块计算，而不是近似计算。</li>\n</ul>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>IO感知</strong>：指的是在计算过程中优化输入输出操作以减少延迟。</li>\n<li><strong>HBM访问</strong>：指的是高带宽内存的访问次数，减少这些访问可以提高计算效率。</li>\n<li><strong>分块技术（Tiling）</strong>：将大任务分成小块来处理，以提高效率。</li>\n<li><strong>算子融合</strong>：将多个计算步骤合并为一个，以减少中间步骤和内存使用。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 使用分块技术进行计算任务分割。</li>\n<li>⚠ 避免实例化完整的注意力矩阵。</li>\n<li>❗ 引入统计量以优化计算顺序。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意避免在实现过程中忽略了IO感知的重要性，这可能导致计算效率未能达到预期。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<p>FlashAttention的创新在于它通过改变计算顺序和数据处理方式来优化资源使用，而不是简单地减少计算量。这种方法为其他领域的优化提供了新的思路。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究FlashAttention在不同模型中的应用效果。</li>\n<li>探索分块技术如何在其他计算任务中应用。</li>\n<li>分析显存节省对大型模型训练的影响。</li>\n</ul>\n<blockquote>\n<p>来源：论文《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》及相关知乎文章。(https://zhuanlan.zhihu.com/p/669926191 ， https://zhuanlan.zhihu.com/p/676655352 ， https://zhuanlan.zhihu.com/p/663932651)</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/训练推理优化/FlashAttention/介绍.md","filePathRelative":"notes_bak/大语言模型学习/训练推理优化/FlashAttention/介绍.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/训练推理优化/FlashAttention/介绍","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/训练推理优化/FlashAttention/介绍/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-28T14:19:52.000Z","updated":"2025-04-29T03:00:58.000Z","title":"介绍","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>元数据：</p>\n<p>分类：人工智能技术</p>\n<p>标签：FlashAttention, Transformer, 计算优化, 显存节省, IO感知</p>\n<p>日期：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>FlashAttention提出了一种创新的注意力机制，通过加速计算和节省显存来优化Transformer模型。它的设计旨在解决随着序列长度 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 的二次增长所带来的资源和效率问题。与传统注意力机制不同，FlashAttention在保持结果精确的同时减少了显存复杂度和计算时间。</p>\n<h2 id=\"重点段落与数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落与数据\"><span>重点段落与数据</span></a></h2>\n<h3 id=\"加速计算-fast\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#加速计算-fast\"><span>加速计算（Fast）</span></a></h3>\n<ul>\n<li>FlashAttention通过IO感知减少HBM访问次数来加快计算速度，而不是减少计算量（FLOPs）。</li>\n<li>使用了分块技术（tiling）和算子融合来实现这一目标。</li>\n</ul>\n<h3 id=\"显存节省-memory-efficient\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存节省-memory-efficient\"><span>显存节省（Memory-efficient）</span></a></h3>\n<ul>\n<li>通过引入统计量，改变注意力机制的计算顺序，避免实例化注意力矩阵。</li>\n<li>显存复杂度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(N^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 降低到了 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n<h3 id=\"精确注意力-exact-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#精确注意力-exact-attention\"><span>精确注意力（Exact Attention）</span></a></h3>\n<ul>\n<li>FlashAttention与原生注意力的结果完全等价，不同于稀疏注意力，它只是分块计算，而不是近似计算。</li>\n</ul>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>IO感知</strong>：指的是在计算过程中优化输入输出操作以减少延迟。</li>\n<li><strong>HBM访问</strong>：指的是高带宽内存的访问次数，减少这些访问可以提高计算效率。</li>\n<li><strong>分块技术（Tiling）</strong>：将大任务分成小块来处理，以提高效率。</li>\n<li><strong>算子融合</strong>：将多个计算步骤合并为一个，以减少中间步骤和内存使用。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 使用分块技术进行计算任务分割。</li>\n<li>⚠ 避免实例化完整的注意力矩阵。</li>\n<li>❗ 引入统计量以优化计算顺序。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意避免在实现过程中忽略了IO感知的重要性，这可能导致计算效率未能达到预期。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<p>FlashAttention的创新在于它通过改变计算顺序和数据处理方式来优化资源使用，而不是简单地减少计算量。这种方法为其他领域的优化提供了新的思路。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究FlashAttention在不同模型中的应用效果。</li>\n<li>探索分块技术如何在其他计算任务中应用。</li>\n<li>分析显存节省对大型模型训练的影响。</li>\n</ul>\n<blockquote>\n<p>来源：论文《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》及相关知乎文章。(https://zhuanlan.zhihu.com/p/669926191 ， https://zhuanlan.zhihu.com/p/676655352 ， https://zhuanlan.zhihu.com/p/663932651)</p>\n</blockquote>\n</template>","contentStripped":"<p>元数据：</p>\n<p>分类：人工智能技术</p>\n<p>标签：FlashAttention, Transformer, 计算优化, 显存节省, IO感知</p>\n<p>日期：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>FlashAttention提出了一种创新的注意力机制，通过加速计算和节省显存来优化Transformer模型。它的设计旨在解决随着序列长度 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 的二次增长所带来的资源和效率问题。与传统注意力机制不同，FlashAttention在保持结果精确的同时减少了显存复杂度和计算时间。</p>\n<h2 id=\"重点段落与数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落与数据\"><span>重点段落与数据</span></a></h2>\n<h3 id=\"加速计算-fast\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#加速计算-fast\"><span>加速计算（Fast）</span></a></h3>\n<ul>\n<li>FlashAttention通过IO感知减少HBM访问次数来加快计算速度，而不是减少计算量（FLOPs）。</li>\n<li>使用了分块技术（tiling）和算子融合来实现这一目标。</li>\n</ul>\n<h3 id=\"显存节省-memory-efficient\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存节省-memory-efficient\"><span>显存节省（Memory-efficient）</span></a></h3>\n<ul>\n<li>通过引入统计量，改变注意力机制的计算顺序，避免实例化注意力矩阵。</li>\n<li>显存复杂度从 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(N^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 降低到了 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n<h3 id=\"精确注意力-exact-attention\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#精确注意力-exact-attention\"><span>精确注意力（Exact Attention）</span></a></h3>\n<ul>\n<li>FlashAttention与原生注意力的结果完全等价，不同于稀疏注意力，它只是分块计算，而不是近似计算。</li>\n</ul>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>IO感知</strong>：指的是在计算过程中优化输入输出操作以减少延迟。</li>\n<li><strong>HBM访问</strong>：指的是高带宽内存的访问次数，减少这些访问可以提高计算效率。</li>\n<li><strong>分块技术（Tiling）</strong>：将大任务分成小块来处理，以提高效率。</li>\n<li><strong>算子融合</strong>：将多个计算步骤合并为一个，以减少中间步骤和内存使用。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 使用分块技术进行计算任务分割。</li>\n<li>⚠ 避免实例化完整的注意力矩阵。</li>\n<li>❗ 引入统计量以优化计算顺序。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意避免在实现过程中忽略了IO感知的重要性，这可能导致计算效率未能达到预期。</p>\n</blockquote>\n<h2 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h2>\n<p>FlashAttention的创新在于它通过改变计算顺序和数据处理方式来优化资源使用，而不是简单地减少计算量。这种方法为其他领域的优化提供了新的思路。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究FlashAttention在不同模型中的应用效果。</li>\n<li>探索分块技术如何在其他计算任务中应用。</li>\n<li>分析显存节省对大型模型训练的影响。</li>\n</ul>\n<blockquote>\n<p>来源：论文《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》及相关知乎文章。(https://zhuanlan.zhihu.com/p/669926191 ， https://zhuanlan.zhihu.com/p/676655352 ， https://zhuanlan.zhihu.com/p/663932651)</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"元数据：\n\n分类：人工智能技术\n\n标签：FlashAttention, Transformer, 计算优化, 显存节省, IO感知\n\n日期：2025年4月12日\n\n## 核心观点总结\nFlashAttention提出了一种创新的注意力机制，通过加速计算和节省显存来优化Transformer模型。它的设计旨在解决随着序列长度 $N$ 的二次增长所带来的资源和效率问题。与传统注意力机制不同，FlashAttention在保持结果精确的同时减少了显存复杂度和计算时间。\n\n\n## 重点段落与数据\n\n### 加速计算（Fast）\n- FlashAttention通过IO感知减少HBM访问次数来加快计算速度，而不是减少计算量（FLOPs）。\n- 使用了分块技术（tiling）和算子融合来实现这一目标。\n\n\n### 显存节省（Memory-efficient）\n- 通过引入统计量，改变注意力机制的计算顺序，避免实例化注意力矩阵。\n- 显存复杂度从 $O(N^2)$ 降低到了 $O(N)$。\n\n\n### 精确注意力（Exact Attention）\n- FlashAttention与原生注意力的结果完全等价，不同于稀疏注意力，它只是分块计算，而不是近似计算。\n\n\n## 技术术语通俗解释\n- **IO感知**：指的是在计算过程中优化输入输出操作以减少延迟。\n- **HBM访问**：指的是高带宽内存的访问次数，减少这些访问可以提高计算效率。\n- **分块技术（Tiling）**：将大任务分成小块来处理，以提高效率。\n- **算子融合**：将多个计算步骤合并为一个，以减少中间步骤和内存使用。\n\n\n## 操作步骤\n1. ✅ 使用分块技术进行计算任务分割。\n2. ⚠ 避免实例化完整的注意力矩阵。\n3. ❗ 引入统计量以优化计算顺序。\n\n\n## 常见错误\n> 注意避免在实现过程中忽略了IO感知的重要性，这可能导致计算效率未能达到预期。\n\n\n## 💡 启发点\nFlashAttention的创新在于它通过改变计算顺序和数据处理方式来优化资源使用，而不是简单地减少计算量。这种方法为其他领域的优化提供了新的思路。\n\n\n## 行动清单\n- 研究FlashAttention在不同模型中的应用效果。\n- 探索分块技术如何在其他计算任务中应用。\n- 分析显存节省对大型模型训练的影响。\n\n> 来源：论文《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》及相关知乎文章。(https://zhuanlan.zhihu.com/p/669926191 ， https://zhuanlan.zhihu.com/p/676655352 ， https://zhuanlan.zhihu.com/p/663932651)","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落与数据","slug":"重点段落与数据","link":"#重点段落与数据","children":[{"level":3,"title":"加速计算（Fast）","slug":"加速计算-fast","link":"#加速计算-fast","children":[]},{"level":3,"title":"显存节省（Memory-efficient）","slug":"显存节省-memory-efficient","link":"#显存节省-memory-efficient","children":[]},{"level":3,"title":"精确注意力（Exact Attention）","slug":"精确注意力-exact-attention","link":"#精确注意力-exact-attention","children":[]}]},{"level":2,"title":"技术术语通俗解释","slug":"技术术语通俗解释","link":"#技术术语通俗解释","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
