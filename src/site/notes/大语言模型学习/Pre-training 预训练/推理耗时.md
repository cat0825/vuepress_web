---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/推理耗时","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/推理耗时/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-10T21:47:51.000+08:00","updated":"2025-04-13T13:06:02.000+08:00"}
---



# 推理耗时与机制分析：优化大模型生成速度
分类：人工智能，深度学习

标签：推理机制，矩阵计算，时延优化

日期：2023年10月25日

## 推理机制概述
传统的推理方式采用逐个token生成的方法，无法并行化处理。推理过程主要包括：

- **矩阵-向量乘法**：一个大矩阵（如8192x8192）与一个向量相乘，得到另一个向量。
- **Attention计算**：通过KV-cache进行推理。

### 关键瓶颈
- 矩阵-向量乘法的浮点运算：每个矩阵元素执行一次乘加运算（2 FLOPs）。
- Attention计算需要对每个key和value执行一次乘加。


## 时延计算

### 数据量分析
在模型使用FP16格式时，每生成一个token需要加载的数据总量为14.2 GB。虽然下一个token生成时可以复用每个矩阵，但由于硬件缓存大小有限，速度受限于显存带宽。


### KV-cache读取
假设是7B大小的LLM，每个token对应的KV-cache数据量为130KB。例如，第1000个token需要读取130MB的数据，与总数据量14.2GB相比，这部分影响可以忽略不计。


### 计算时延
在NVIDIA RTX 4090上，读取14.2GB (FP16)数据需要约14.1ms，因此每个位置靠前的token大约需要14.1ms。使用8bit权重则需7.0ms。这是生成每个token的理论最小时间。


## 公式总结
模型的预测时间可以近似为：
$$
y = kx + b
$$
其中，$$b$$是首个token的耗时，$$k$$是后续每个token的耗时，$$x$$是生成token的总数量。


## 常见错误
> ⚠️ 在进行推理机制优化时，容易忽视KV-cache对整体速度的微小影响，导致错误评估性能瓶颈。


## 💡启发点
- 矩阵-向量乘法和attention计算是推理过程中的核心操作，优化这些步骤可以显著提升模型性能。


## 行动清单
- 探索更高效的矩阵计算方法。
- 优化KV-cache使用策略以减少不必要的数据读取。


## 📈趋势预测
未来，大模型的推理速度将继续提升，通过硬件和算法的协同优化实现更快的响应时间。


## 后续追踪
- 研究新型硬件架构对推理速度的影响。
- 探索更高效的attention机制以进一步减少推理时延。

> 来源：《LLM inference speed of light》
