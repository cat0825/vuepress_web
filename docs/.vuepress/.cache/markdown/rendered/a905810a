{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习算法</li>\n<li>标签：价值迭代，动态规划，贝尔曼方程，策略评估</li>\n<li>日期：2025年4月10日</li>\n</ul>\n<h2 id=\"内容处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容处理\"><span>内容处理</span></a></h2>\n<h3 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h3>\n<p>价值迭代算法是一种用于求解马尔可夫决策过程（MDP）的动态规划方法。它通过反复更新状态价值函数，直到收敛到一个最优值。然后，利用这个最优值来提取最优策略。与策略迭代不同，价值迭代直接利用最优贝尔曼方程进行更新。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ol>\n<li>\n<p><strong>价值迭代公式</strong>：\n价值迭代的核心公式是：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mrow><mo fence=\"true\">{</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>γ</mi><munder><mo>∑</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></munder><mi>P</mi><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mi mathvariant=\"normal\">∣</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo><msub><mi>V</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo stretchy=\"false\">)</mo><mo fence=\"true\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V_{k+1}(s) = \\max_{a \\in A} \\left\\{ r(s, a) + \\gamma \\sum_{s&#x27;} P(s&#x27; | s, a) V_k(s&#x27;) \\right\\}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.044em;vertical-align:-1.294em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4306em;\"><span style=\"top:-2.3557em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mrel mtight\">∈</span><span class=\"mord mathnormal mtight\">A</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\">max</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7717em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">{</span></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.05em;\"><span style=\"top:-1.856em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6828em;\"><span style=\"top:-2.786em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.294em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8019em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8019em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">}</span></span></span></span></span></span></span></p>\n</li>\n<li>\n<p><strong>算法流程</strong>：</p>\n<ul>\n<li>随机初始化状态价值函数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">V(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li>反复更新每个状态的价值，直到相邻两次迭代的变化小于给定阈值。</li>\n<li>提取最优策略 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n</li>\n<li>\n<p><strong>代码示例</strong>：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div></li>\n</ol>\n<p>class ValueIteration:\n&quot;&quot;&quot; 价值迭代算法 &quot;&quot;&quot;\nfor s in range(self.env.ncol * self.env.nrow):\nqsa_list = [] # 开始计算状态s下的所有Q(s,a)价值\nfor a in range(4):\nqsa = 0\nfor res in self.env.P[s][a]:\np, next_state, r, done = res\nqsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\nqsa_list.append(qsa) # 这一行和下一行代码是价值迭代和策略迭代的主要区别\nnew_v[s] = max(qsa_list)\nmax_diff = max(max_diff, abs(new_v[s] - self.v[s]))\nself.v = new_v\nif max_diff &lt; self.theta: break # 满足收敛条件,退出评估迭代\ncnt += 1\nprint(&quot;价值迭代一共进行%d轮&quot; % cnt)\nself.get_policy()\ndef get_policy(self): # 根据价值函数导出一个贪婪策略\nfor s in range(self.env.nrow * self.env.ncol):\nqsa_list = []\nfor a in range(4):\nqsa = 0\nfor res in self.env.P[s][a]:\np, next_state, r, done = res\nqsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\nqsa_list.append(qsa)\nmaxq = max(qsa_list)\ncntq = qsa_list.count(maxq) # 计算有几个动作得到了最大的Q值\n# 让这些动作均分概率\nself.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]</p>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>### 技术术语解释</span></span>\n<span class=\"line\"><span>- **最优贝尔曼方程**：一种用于确定最优策略的方程，通过最大化期望回报来更新状态价值。</span></span>\n<span class=\"line\"><span>- **状态价值函数**：表示在给定策略下，从某一状态开始的期望回报。</span></span>\n<span class=\"line\"><span>- **策略评估**：计算在某策略下，每个状态的期望回报。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 操作步骤</span></span>\n<span class=\"line\"><span>1. ✅ 随机初始化状态价值函数 $V(s)$。</span></span>\n<span class=\"line\"><span>2. ⚠ 检查相邻两次迭代的变化是否小于阈值。</span></span>\n<span class=\"line\"><span>3. ❗ 提取最优策略 $\\pi(s)$。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 常见错误</span></span>\n<span class=\"line\"><span>> **警告**：在更新过程中，确保所有状态都被正确地遍历和更新，以免导致收敛不正确。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 💡启发点</span></span>\n<span class=\"line\"><span>- 价值迭代直接利用最优贝尔曼方程，减少了策略评估与策略更新的交替过程。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 行动清单</span></span>\n<span class=\"line\"><span>- 实现一个简单的价值迭代算法。</span></span>\n<span class=\"line\"><span>- 测试不同阈值对收敛速度的影响。</span></span>\n<span class=\"line\"><span>- 比较价值迭代与策略迭代的效率。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 📈趋势预测</span></span>\n<span class=\"line\"><span>随着计算能力的提升，价值迭代算法在更大规模问题上的应用将更加广泛，并且可能会与其他优化算法结合使用以提高效率。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 后续追踪</span></span>\n<span class=\"line\"><span>- 探索价值迭代在非确定性环境中的应用。</span></span>\n<span class=\"line\"><span>- 研究结合深度学习的价值迭代方法。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## [思考]板块</span></span>\n<span class=\"line\"><span>1. 如何选择合适的收敛阈值以平衡计算效率与结果精度？</span></span>\n<span class=\"line\"><span>2. 在实际应用中，如何处理状态空间过大的问题？</span></span>\n<span class=\"line\"><span>3. 价值迭代能否与其他强化学习算法结合使用以提高性能？</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>> 来源：本文内容基于对价值迭代算法的解析，原始出处未提供。</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div>","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/价值迭代算法.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/价值迭代算法.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/价值迭代算法","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/价值迭代算法/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-11T05:31:34.000Z","updated":"2025-04-13T05:06:02.000Z","title":"价值迭代算法","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习算法</li>\n<li>标签：价值迭代，动态规划，贝尔曼方程，策略评估</li>\n<li>日期：2025年4月10日</li>\n</ul>\n<h2 id=\"内容处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容处理\"><span>内容处理</span></a></h2>\n<h3 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h3>\n<p>价值迭代算法是一种用于求解马尔可夫决策过程（MDP）的动态规划方法。它通过反复更新状态价值函数，直到收敛到一个最优值。然后，利用这个最优值来提取最优策略。与策略迭代不同，价值迭代直接利用最优贝尔曼方程进行更新。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ol>\n<li>\n<p><strong>价值迭代公式</strong>：\n价值迭代的核心公式是：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mrow><mo fence=\"true\">{</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>γ</mi><munder><mo>∑</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></munder><mi>P</mi><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mi mathvariant=\"normal\">∣</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo><msub><mi>V</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo stretchy=\"false\">)</mo><mo fence=\"true\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V_{k+1}(s) = \\max_{a \\in A} \\left\\{ r(s, a) + \\gamma \\sum_{s&#x27;} P(s&#x27; | s, a) V_k(s&#x27;) \\right\\}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.044em;vertical-align:-1.294em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4306em;\"><span style=\"top:-2.3557em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mrel mtight\">∈</span><span class=\"mord mathnormal mtight\">A</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\">max</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7717em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">{</span></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.05em;\"><span style=\"top:-1.856em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6828em;\"><span style=\"top:-2.786em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.294em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8019em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8019em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">}</span></span></span></span></span></span></span></p>\n</li>\n<li>\n<p><strong>算法流程</strong>：</p>\n<ul>\n<li>随机初始化状态价值函数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">V(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li>反复更新每个状态的价值，直到相邻两次迭代的变化小于给定阈值。</li>\n<li>提取最优策略 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n</li>\n<li>\n<p><strong>代码示例</strong>：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div></li>\n</ol>\n<p>class ValueIteration:\n&quot;&quot;&quot; 价值迭代算法 &quot;&quot;&quot;\nfor s in range(self.env.ncol * self.env.nrow):\nqsa_list = [] # 开始计算状态s下的所有Q(s,a)价值\nfor a in range(4):\nqsa = 0\nfor res in self.env.P[s][a]:\np, next_state, r, done = res\nqsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\nqsa_list.append(qsa) # 这一行和下一行代码是价值迭代和策略迭代的主要区别\nnew_v[s] = max(qsa_list)\nmax_diff = max(max_diff, abs(new_v[s] - self.v[s]))\nself.v = new_v\nif max_diff &lt; self.theta: break # 满足收敛条件,退出评估迭代\ncnt += 1\nprint(&quot;价值迭代一共进行%d轮&quot; % cnt)\nself.get_policy()\ndef get_policy(self): # 根据价值函数导出一个贪婪策略\nfor s in range(self.env.nrow * self.env.ncol):\nqsa_list = []\nfor a in range(4):\nqsa = 0\nfor res in self.env.P[s][a]:\np, next_state, r, done = res\nqsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\nqsa_list.append(qsa)\nmaxq = max(qsa_list)\ncntq = qsa_list.count(maxq) # 计算有几个动作得到了最大的Q值\n# 让这些动作均分概率\nself.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]</p>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>### 技术术语解释</span></span>\n<span class=\"line\"><span>- **最优贝尔曼方程**：一种用于确定最优策略的方程，通过最大化期望回报来更新状态价值。</span></span>\n<span class=\"line\"><span>- **状态价值函数**：表示在给定策略下，从某一状态开始的期望回报。</span></span>\n<span class=\"line\"><span>- **策略评估**：计算在某策略下，每个状态的期望回报。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 操作步骤</span></span>\n<span class=\"line\"><span>1. ✅ 随机初始化状态价值函数 $V(s)$。</span></span>\n<span class=\"line\"><span>2. ⚠ 检查相邻两次迭代的变化是否小于阈值。</span></span>\n<span class=\"line\"><span>3. ❗ 提取最优策略 $\\pi(s)$。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 常见错误</span></span>\n<span class=\"line\"><span>> **警告**：在更新过程中，确保所有状态都被正确地遍历和更新，以免导致收敛不正确。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 💡启发点</span></span>\n<span class=\"line\"><span>- 价值迭代直接利用最优贝尔曼方程，减少了策略评估与策略更新的交替过程。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 行动清单</span></span>\n<span class=\"line\"><span>- 实现一个简单的价值迭代算法。</span></span>\n<span class=\"line\"><span>- 测试不同阈值对收敛速度的影响。</span></span>\n<span class=\"line\"><span>- 比较价值迭代与策略迭代的效率。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 📈趋势预测</span></span>\n<span class=\"line\"><span>随着计算能力的提升，价值迭代算法在更大规模问题上的应用将更加广泛，并且可能会与其他优化算法结合使用以提高效率。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 后续追踪</span></span>\n<span class=\"line\"><span>- 探索价值迭代在非确定性环境中的应用。</span></span>\n<span class=\"line\"><span>- 研究结合深度学习的价值迭代方法。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## [思考]板块</span></span>\n<span class=\"line\"><span>1. 如何选择合适的收敛阈值以平衡计算效率与结果精度？</span></span>\n<span class=\"line\"><span>2. 在实际应用中，如何处理状态空间过大的问题？</span></span>\n<span class=\"line\"><span>3. 价值迭代能否与其他强化学习算法结合使用以提高性能？</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>> 来源：本文内容基于对价值迭代算法的解析，原始出处未提供。</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div></template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习算法</li>\n<li>标签：价值迭代，动态规划，贝尔曼方程，策略评估</li>\n<li>日期：2025年4月10日</li>\n</ul>\n<h2 id=\"内容处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容处理\"><span>内容处理</span></a></h2>\n<h3 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h3>\n<p>价值迭代算法是一种用于求解马尔可夫决策过程（MDP）的动态规划方法。它通过反复更新状态价值函数，直到收敛到一个最优值。然后，利用这个最优值来提取最优策略。与策略迭代不同，价值迭代直接利用最优贝尔曼方程进行更新。</p>\n<h3 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h3>\n<ol>\n<li>\n<p><strong>价值迭代公式</strong>：\n价值迭代的核心公式是：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mrow><mo fence=\"true\">{</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>γ</mi><munder><mo>∑</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></munder><mi>P</mi><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mi mathvariant=\"normal\">∣</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo><msub><mi>V</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo stretchy=\"false\">)</mo><mo fence=\"true\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V_{k+1}(s) = \\max_{a \\in A} \\left\\{ r(s, a) + \\gamma \\sum_{s&#x27;} P(s&#x27; | s, a) V_k(s&#x27;) \\right\\}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.044em;vertical-align:-1.294em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4306em;\"><span style=\"top:-2.3557em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mrel mtight\">∈</span><span class=\"mord mathnormal mtight\">A</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\">max</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7717em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">{</span></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.05em;\"><span style=\"top:-1.856em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6828em;\"><span style=\"top:-2.786em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.294em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8019em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8019em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">}</span></span></span></span></span></span></span></p>\n</li>\n<li>\n<p><strong>算法流程</strong>：</p>\n<ul>\n<li>随机初始化状态价值函数 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">V(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li>反复更新每个状态的价值，直到相邻两次迭代的变化小于给定阈值。</li>\n<li>提取最优策略 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n</li>\n<li>\n<p><strong>代码示例</strong>：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div></li>\n</ol>\n<p>class ValueIteration:\n&quot;&quot;&quot; 价值迭代算法 &quot;&quot;&quot;\nfor s in range(self.env.ncol * self.env.nrow):\nqsa_list = [] # 开始计算状态s下的所有Q(s,a)价值\nfor a in range(4):\nqsa = 0\nfor res in self.env.P[s][a]:\np, next_state, r, done = res\nqsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\nqsa_list.append(qsa) # 这一行和下一行代码是价值迭代和策略迭代的主要区别\nnew_v[s] = max(qsa_list)\nmax_diff = max(max_diff, abs(new_v[s] - self.v[s]))\nself.v = new_v\nif max_diff &lt; self.theta: break # 满足收敛条件,退出评估迭代\ncnt += 1\nprint(&quot;价值迭代一共进行%d轮&quot; % cnt)\nself.get_policy()\ndef get_policy(self): # 根据价值函数导出一个贪婪策略\nfor s in range(self.env.nrow * self.env.ncol):\nqsa_list = []\nfor a in range(4):\nqsa = 0\nfor res in self.env.P[s][a]:\np, next_state, r, done = res\nqsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\nqsa_list.append(qsa)\nmaxq = max(qsa_list)\ncntq = qsa_list.count(maxq) # 计算有几个动作得到了最大的Q值\n# 让这些动作均分概率\nself.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]</p>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>### 技术术语解释</span></span>\n<span class=\"line\"><span>- **最优贝尔曼方程**：一种用于确定最优策略的方程，通过最大化期望回报来更新状态价值。</span></span>\n<span class=\"line\"><span>- **状态价值函数**：表示在给定策略下，从某一状态开始的期望回报。</span></span>\n<span class=\"line\"><span>- **策略评估**：计算在某策略下，每个状态的期望回报。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 操作步骤</span></span>\n<span class=\"line\"><span>1. ✅ 随机初始化状态价值函数 $V(s)$。</span></span>\n<span class=\"line\"><span>2. ⚠ 检查相邻两次迭代的变化是否小于阈值。</span></span>\n<span class=\"line\"><span>3. ❗ 提取最优策略 $\\pi(s)$。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 常见错误</span></span>\n<span class=\"line\"><span>> **警告**：在更新过程中，确保所有状态都被正确地遍历和更新，以免导致收敛不正确。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 💡启发点</span></span>\n<span class=\"line\"><span>- 价值迭代直接利用最优贝尔曼方程，减少了策略评估与策略更新的交替过程。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 行动清单</span></span>\n<span class=\"line\"><span>- 实现一个简单的价值迭代算法。</span></span>\n<span class=\"line\"><span>- 测试不同阈值对收敛速度的影响。</span></span>\n<span class=\"line\"><span>- 比较价值迭代与策略迭代的效率。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 📈趋势预测</span></span>\n<span class=\"line\"><span>随着计算能力的提升，价值迭代算法在更大规模问题上的应用将更加广泛，并且可能会与其他优化算法结合使用以提高效率。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## 后续追踪</span></span>\n<span class=\"line\"><span>- 探索价值迭代在非确定性环境中的应用。</span></span>\n<span class=\"line\"><span>- 研究结合深度学习的价值迭代方法。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>## [思考]板块</span></span>\n<span class=\"line\"><span>1. 如何选择合适的收敛阈值以平衡计算效率与结果精度？</span></span>\n<span class=\"line\"><span>2. 在实际应用中，如何处理状态空间过大的问题？</span></span>\n<span class=\"line\"><span>3. 价值迭代能否与其他强化学习算法结合使用以提高性能？</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>> 来源：本文内容基于对价值迭代算法的解析，原始出处未提供。</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div>","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：机器学习算法\n- 标签：价值迭代，动态规划，贝尔曼方程，策略评估\n- 日期：2025年4月10日\n\n\n## 内容处理\n\n### 核心观点\n价值迭代算法是一种用于求解马尔可夫决策过程（MDP）的动态规划方法。它通过反复更新状态价值函数，直到收敛到一个最优值。然后，利用这个最优值来提取最优策略。与策略迭代不同，价值迭代直接利用最优贝尔曼方程进行更新。\n\n\n### 重点段落\n1. **价值迭代公式**：\n   价值迭代的核心公式是：\n   $$\n   V_{k+1}(s) = \\max_{a \\in A} \\left\\{ r(s, a) + \\gamma \\sum_{s'} P(s' | s, a) V_k(s') \\right\\}\n   $$\n\n2. **算法流程**：\n   - 随机初始化状态价值函数 $V(s)$。\n   - 反复更新每个状态的价值，直到相邻两次迭代的变化小于给定阈值。\n   - 提取最优策略 $\\pi(s)$。\n\n3. **代码示例**：\n   ```python\nclass ValueIteration:\n    \"\"\" 价值迭代算法 \"\"\"\n    for s in range(self.env.ncol * self.env.nrow):\n        qsa_list = [] # 开始计算状态s下的所有Q(s,a)价值\n        for a in range(4):\n            qsa = 0\n            for res in self.env.P[s][a]:\n                p, next_state, r, done = res\n                qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\n            qsa_list.append(qsa) # 这一行和下一行代码是价值迭代和策略迭代的主要区别\n        new_v[s] = max(qsa_list)\n        max_diff = max(max_diff, abs(new_v[s] - self.v[s]))\n    self.v = new_v\n    if max_diff < self.theta: break # 满足收敛条件,退出评估迭代\n    cnt += 1\n    print(\"价值迭代一共进行%d轮\" % cnt)\n    self.get_policy()\ndef get_policy(self): # 根据价值函数导出一个贪婪策略\n    for s in range(self.env.nrow * self.env.ncol):\n        qsa_list = []\n        for a in range(4):\n            qsa = 0\n            for res in self.env.P[s][a]:\n                p, next_state, r, done = res\n                qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\n            qsa_list.append(qsa)\n        maxq = max(qsa_list)\n        cntq = qsa_list.count(maxq) # 计算有几个动作得到了最大的Q值\n        # 让这些动作均分概率\n        self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]\n\n\n   ```\n\n\n### 技术术语解释\n- **最优贝尔曼方程**：一种用于确定最优策略的方程，通过最大化期望回报来更新状态价值。\n- **状态价值函数**：表示在给定策略下，从某一状态开始的期望回报。\n- **策略评估**：计算在某策略下，每个状态的期望回报。\n\n\n## 操作步骤\n1. ✅ 随机初始化状态价值函数 $V(s)$。\n2. ⚠ 检查相邻两次迭代的变化是否小于阈值。\n3. ❗ 提取最优策略 $\\pi(s)$。\n\n\n## 常见错误\n> **警告**：在更新过程中，确保所有状态都被正确地遍历和更新，以免导致收敛不正确。\n\n\n## 💡启发点\n- 价值迭代直接利用最优贝尔曼方程，减少了策略评估与策略更新的交替过程。\n\n\n## 行动清单\n- 实现一个简单的价值迭代算法。\n- 测试不同阈值对收敛速度的影响。\n- 比较价值迭代与策略迭代的效率。\n\n\n## 📈趋势预测\n随着计算能力的提升，价值迭代算法在更大规模问题上的应用将更加广泛，并且可能会与其他优化算法结合使用以提高效率。\n\n\n## 后续追踪\n- 探索价值迭代在非确定性环境中的应用。\n- 研究结合深度学习的价值迭代方法。\n\n\n## [思考]板块\n1. 如何选择合适的收敛阈值以平衡计算效率与结果精度？\n2. 在实际应用中，如何处理状态空间过大的问题？\n3. 价值迭代能否与其他强化学习算法结合使用以提高性能？\n\n> 来源：本文内容基于对价值迭代算法的解析，原始出处未提供。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容处理","slug":"内容处理","link":"#内容处理","children":[{"level":3,"title":"核心观点","slug":"核心观点","link":"#核心观点","children":[]},{"level":3,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[]}]}]}}
