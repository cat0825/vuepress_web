{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：BERT, 预训练模型, NLP, 深度学习</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容处理\"><span>内容处理</span></a></h2>\n<p>BERT（Bidirectional Encoder Representation from Transformers）是一种用于自然语言处理的预训练模型，主要用于替代传统的Word2Vec。通过两个核心任务：Masked Language Model (MLM) 和 Next Sentence Prediction (NSP)，BERT可以学习更丰富的文本表征。\n<img src=\"/img/user/附件/Pasted image 20250424113448.png\" alt=\"Pasted image 20250424113448.png\">\n<img src=\"/img/user/附件/Pasted image 20250424113459.png\" alt=\"Pasted image 20250424113459.png\"></p>\n<h3 id=\"bert-embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#bert-embedding\"><span>BERT Embedding</span></a></h3>\n<p>BERT的输入编码向量由三个嵌入特征组成：</p>\n<ul>\n<li><strong>位置嵌入</strong>：将单词的位置信息编码为特征向量，帮助模型理解单词之间的位置关系。</li>\n<li><strong>Token嵌入</strong>：将单词分解为更小的token，例如‘playing’被拆分成‘play’和‘ing’。</li>\n<li><strong>Segment嵌入</strong>：用于区分两个句子，例如判断句子B是否为句子A的后续部分。</li>\n</ul>\n<h3 id=\"masked-lm-mlm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#masked-lm-mlm\"><span>Masked LM (MLM)</span></a></h3>\n<p>在训练过程中，约15%的单词会被替换为[MASK]，并通过上下文预测这些被掩盖的单词。具体步骤包括：</p>\n<p>✅ 80%的tokens替换为[MASK]以融合双向语义信息。</p>\n<p>⚠ 10%的tokens替换为随机单词以增强纠错能力。</p>\n<p>❗ 10%的tokens保持不变以提供模型偏向。</p>\n<h3 id=\"next-sentence-prediction-nsp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#next-sentence-prediction-nsp\"><span>Next Sentence Prediction (NSP)</span></a></h3>\n<p>BERT通过成对的句子进行训练，预测第二个句子是否是原始文档中的后续句子。50%的句子对是前后关系，另50%是随机组合。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在使用BERT进行微调时，务必确保输入数据格式正确，否则可能导致不准确的预测结果。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>BERT通过双向语境学习解决了一词多义的问题。</li>\n<li>使用随机替换和保持不变的方法提高了模型的泛化能力。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>探索BERT在不同NLP任务中的应用。</li>\n<li>研究其他预训练模型与BERT的比较。</li>\n<li><img src=\"/img/user/附件/Pasted image 20250424113517.png\" alt=\"Pasted image 20250424113517.png\"></li>\n<li>实施BERT微调以提高特定任务的性能。</li>\n</ol>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>特征类型</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>位置嵌入</td>\n<td>编码单词位置信息</td>\n</tr>\n<tr>\n<td>Token嵌入</td>\n<td>将单词拆分为小单位</td>\n</tr>\n<tr>\n<td>Segment嵌入</td>\n<td>区分句子对中的两个句子</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原始来源：[选自提供文本内容]</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/BERT及其变体/介绍.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/BERT及其变体/介绍.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/BERT及其变体/介绍","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/BERT及其变体/介绍/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T03:31:54.000Z","updated":"2025-04-24T03:38:55.000Z","title":"介绍","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：BERT, 预训练模型, NLP, 深度学习</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容处理\"><span>内容处理</span></a></h2>\n<p>BERT（Bidirectional Encoder Representation from Transformers）是一种用于自然语言处理的预训练模型，主要用于替代传统的Word2Vec。通过两个核心任务：Masked Language Model (MLM) 和 Next Sentence Prediction (NSP)，BERT可以学习更丰富的文本表征。\n<img src=\"/img/user/附件/Pasted image 20250424113448.png\" alt=\"Pasted image 20250424113448.png\">\n<img src=\"/img/user/附件/Pasted image 20250424113459.png\" alt=\"Pasted image 20250424113459.png\"></p>\n<h3 id=\"bert-embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#bert-embedding\"><span>BERT Embedding</span></a></h3>\n<p>BERT的输入编码向量由三个嵌入特征组成：</p>\n<ul>\n<li><strong>位置嵌入</strong>：将单词的位置信息编码为特征向量，帮助模型理解单词之间的位置关系。</li>\n<li><strong>Token嵌入</strong>：将单词分解为更小的token，例如‘playing’被拆分成‘play’和‘ing’。</li>\n<li><strong>Segment嵌入</strong>：用于区分两个句子，例如判断句子B是否为句子A的后续部分。</li>\n</ul>\n<h3 id=\"masked-lm-mlm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#masked-lm-mlm\"><span>Masked LM (MLM)</span></a></h3>\n<p>在训练过程中，约15%的单词会被替换为[MASK]，并通过上下文预测这些被掩盖的单词。具体步骤包括：</p>\n<p>✅ 80%的tokens替换为[MASK]以融合双向语义信息。</p>\n<p>⚠ 10%的tokens替换为随机单词以增强纠错能力。</p>\n<p>❗ 10%的tokens保持不变以提供模型偏向。</p>\n<h3 id=\"next-sentence-prediction-nsp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#next-sentence-prediction-nsp\"><span>Next Sentence Prediction (NSP)</span></a></h3>\n<p>BERT通过成对的句子进行训练，预测第二个句子是否是原始文档中的后续句子。50%的句子对是前后关系，另50%是随机组合。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在使用BERT进行微调时，务必确保输入数据格式正确，否则可能导致不准确的预测结果。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>BERT通过双向语境学习解决了一词多义的问题。</li>\n<li>使用随机替换和保持不变的方法提高了模型的泛化能力。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>探索BERT在不同NLP任务中的应用。</li>\n<li>研究其他预训练模型与BERT的比较。</li>\n<li><img src=\"/img/user/附件/Pasted image 20250424113517.png\" alt=\"Pasted image 20250424113517.png\"></li>\n<li>实施BERT微调以提高特定任务的性能。</li>\n</ol>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>特征类型</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>位置嵌入</td>\n<td>编码单词位置信息</td>\n</tr>\n<tr>\n<td>Token嵌入</td>\n<td>将单词拆分为小单位</td>\n</tr>\n<tr>\n<td>Segment嵌入</td>\n<td>区分句子对中的两个句子</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原始来源：[选自提供文本内容]</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：BERT, 预训练模型, NLP, 深度学习</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容处理\"><span>内容处理</span></a></h2>\n<p>BERT（Bidirectional Encoder Representation from Transformers）是一种用于自然语言处理的预训练模型，主要用于替代传统的Word2Vec。通过两个核心任务：Masked Language Model (MLM) 和 Next Sentence Prediction (NSP)，BERT可以学习更丰富的文本表征。\n<img src=\"/img/user/附件/Pasted image 20250424113448.png\" alt=\"Pasted image 20250424113448.png\">\n<img src=\"/img/user/附件/Pasted image 20250424113459.png\" alt=\"Pasted image 20250424113459.png\"></p>\n<h3 id=\"bert-embedding\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#bert-embedding\"><span>BERT Embedding</span></a></h3>\n<p>BERT的输入编码向量由三个嵌入特征组成：</p>\n<ul>\n<li><strong>位置嵌入</strong>：将单词的位置信息编码为特征向量，帮助模型理解单词之间的位置关系。</li>\n<li><strong>Token嵌入</strong>：将单词分解为更小的token，例如‘playing’被拆分成‘play’和‘ing’。</li>\n<li><strong>Segment嵌入</strong>：用于区分两个句子，例如判断句子B是否为句子A的后续部分。</li>\n</ul>\n<h3 id=\"masked-lm-mlm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#masked-lm-mlm\"><span>Masked LM (MLM)</span></a></h3>\n<p>在训练过程中，约15%的单词会被替换为[MASK]，并通过上下文预测这些被掩盖的单词。具体步骤包括：</p>\n<p>✅ 80%的tokens替换为[MASK]以融合双向语义信息。</p>\n<p>⚠ 10%的tokens替换为随机单词以增强纠错能力。</p>\n<p>❗ 10%的tokens保持不变以提供模型偏向。</p>\n<h3 id=\"next-sentence-prediction-nsp\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#next-sentence-prediction-nsp\"><span>Next Sentence Prediction (NSP)</span></a></h3>\n<p>BERT通过成对的句子进行训练，预测第二个句子是否是原始文档中的后续句子。50%的句子对是前后关系，另50%是随机组合。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在使用BERT进行微调时，务必确保输入数据格式正确，否则可能导致不准确的预测结果。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>BERT通过双向语境学习解决了一词多义的问题。</li>\n<li>使用随机替换和保持不变的方法提高了模型的泛化能力。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>探索BERT在不同NLP任务中的应用。</li>\n<li>研究其他预训练模型与BERT的比较。</li>\n<li><img src=\"/img/user/附件/Pasted image 20250424113517.png\" alt=\"Pasted image 20250424113517.png\"></li>\n<li>实施BERT微调以提高特定任务的性能。</li>\n</ol>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>特征类型</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>位置嵌入</td>\n<td>编码单词位置信息</td>\n</tr>\n<tr>\n<td>Token嵌入</td>\n<td>将单词拆分为小单位</td>\n</tr>\n<tr>\n<td>Segment嵌入</td>\n<td>区分句子对中的两个句子</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>原始来源：[选自提供文本内容]</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：自然语言处理\n- 标签：BERT, 预训练模型, NLP, 深度学习\n- 日期：2025年4月12日\n\n\n## 内容处理\nBERT（Bidirectional Encoder Representation from Transformers）是一种用于自然语言处理的预训练模型，主要用于替代传统的Word2Vec。通过两个核心任务：Masked Language Model (MLM) 和 Next Sentence Prediction (NSP)，BERT可以学习更丰富的文本表征。\n![Pasted image 20250424113448.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424113448.png)\n![Pasted image 20250424113459.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424113459.png)\n\n### BERT Embedding\nBERT的输入编码向量由三个嵌入特征组成：\n\n- **位置嵌入**：将单词的位置信息编码为特征向量，帮助模型理解单词之间的位置关系。\n- **Token嵌入**：将单词分解为更小的token，例如‘playing’被拆分成‘play’和‘ing’。\n- **Segment嵌入**：用于区分两个句子，例如判断句子B是否为句子A的后续部分。\n\n\n### Masked LM (MLM)\n在训练过程中，约15%的单词会被替换为[MASK]，并通过上下文预测这些被掩盖的单词。具体步骤包括：\n\n✅ 80%的tokens替换为[MASK]以融合双向语义信息。\n\n⚠ 10%的tokens替换为随机单词以增强纠错能力。\n\n❗ 10%的tokens保持不变以提供模型偏向。\n\n\n### Next Sentence Prediction (NSP)\nBERT通过成对的句子进行训练，预测第二个句子是否是原始文档中的后续句子。50%的句子对是前后关系，另50%是随机组合。\n\n\n## 常见错误\n> ⚠ 在使用BERT进行微调时，务必确保输入数据格式正确，否则可能导致不准确的预测结果。\n\n\n## 💡启发点\n- BERT通过双向语境学习解决了一词多义的问题。\n- 使用随机替换和保持不变的方法提高了模型的泛化能力。\n\n\n## 行动清单\n1. 探索BERT在不同NLP任务中的应用。\n2. 研究其他预训练模型与BERT的比较。\n3. ![Pasted image 20250424113517.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424113517.png)\n4. 实施BERT微调以提高特定任务的性能。\n\n\n## 数据转换\n| 特征类型       | 描述                                    |\n|----------------|-----------------------------------------|\n| 位置嵌入       | 编码单词位置信息                         |\n| Token嵌入      | 将单词拆分为小单位                       |\n| Segment嵌入    | 区分句子对中的两个句子                   |\n\n\n## 来源标注\n> 原始来源：[选自提供文本内容]","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容处理","slug":"内容处理","link":"#内容处理","children":[{"level":3,"title":"BERT Embedding","slug":"bert-embedding","link":"#bert-embedding","children":[]},{"level":3,"title":"Masked LM (MLM)","slug":"masked-lm-mlm","link":"#masked-lm-mlm","children":[]},{"level":3,"title":"Next Sentence Prediction (NSP)","slug":"next-sentence-prediction-nsp","link":"#next-sentence-prediction-nsp","children":[]}]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"数据转换","slug":"数据转换","link":"#数据转换","children":[]},{"level":2,"title":"来源标注","slug":"来源标注","link":"#来源标注","children":[]}]}}
