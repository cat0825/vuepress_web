{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：BART, Transformer, NLP, 文本生成, 文本理解</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>BART是一种基于Transformer架构的模型，结合了双向和自回归的特性。它在文本生成任务中表现优异，同时也能在文本理解任务中取得领先的效果。\n<img src=\"/img/user/附件/Pasted image 20250424113608.png\" alt=\"Pasted image 20250424113608.png\"></p>\n<h3 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h3>\n<ul>\n<li><strong>模型架构</strong>：BART采用标准的encoder-decoder结构，并进行了若干调整。</li>\n<li><strong>激活函数与参数初始化</strong>：与GPT类似，BART使用GeLU激活函数，参数初始化服从正态分布 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0.02</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(0,0.02)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0.02</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li><strong>层数配置</strong>：BART base模型的Encoder和Decoder各有6层，而large模型则扩展到12层。</li>\n<li><strong>cross-attention</strong>：解码器的各层对编码器最终隐藏层额外执行cross-attention。</li>\n<li><strong>与BERT的区别</strong>：BERT在词预测之前使用了额外的Feed Forward Layer，而BART没有。</li>\n<li><strong>应用场景</strong>：相比GPT，BART增加了双向上下文语境信息，更适合文本生成。</li>\n</ul>\n<h3 id=\"技术术语简化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语简化\"><span>技术术语简化</span></a></h3>\n<ul>\n<li><strong>Encoder-Decoder</strong>：一种用于处理输入数据并生成输出数据的结构，类似于翻译系统。</li>\n<li><strong>GeLU激活函数</strong>：一种数学函数，用于帮助神经网络学习复杂模式。</li>\n<li><strong>cross-attention</strong>：一种机制，允许解码器更好地理解编码器生成的信息。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 将ReLU激活函数替换为GeLU。</li>\n<li>✅ 初始化参数为正态分布 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0.02</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(0,0.02)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0.02</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li>⚠ 确保解码器的各层执行cross-attention。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在实现BART时，容易忽略cross-attention机制，这会导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>BART在文本生成任务中的双向上下文语境信息是其优于GPT的一大创新点。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究BART在其他NLP任务中的应用潜力。</li>\n<li>比较BART与其他Transformer模型在不同任务中的表现。</li>\n<li>探索BART与其他激活函数的兼容性。</li>\n</ul>\n<blockquote>\n<p>来源：原始内容提供者不详，内容经过处理和总结。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/BERT及其变体/BART.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/BERT及其变体/BART.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/BERT及其变体/BART","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/BERT及其变体/BART/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T03:35:38.000Z","updated":"2025-04-24T03:38:55.000Z","title":"BART","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：BART, Transformer, NLP, 文本生成, 文本理解</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>BART是一种基于Transformer架构的模型，结合了双向和自回归的特性。它在文本生成任务中表现优异，同时也能在文本理解任务中取得领先的效果。\n<img src=\"/img/user/附件/Pasted image 20250424113608.png\" alt=\"Pasted image 20250424113608.png\"></p>\n<h3 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h3>\n<ul>\n<li><strong>模型架构</strong>：BART采用标准的encoder-decoder结构，并进行了若干调整。</li>\n<li><strong>激活函数与参数初始化</strong>：与GPT类似，BART使用GeLU激活函数，参数初始化服从正态分布 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0.02</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(0,0.02)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0.02</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li><strong>层数配置</strong>：BART base模型的Encoder和Decoder各有6层，而large模型则扩展到12层。</li>\n<li><strong>cross-attention</strong>：解码器的各层对编码器最终隐藏层额外执行cross-attention。</li>\n<li><strong>与BERT的区别</strong>：BERT在词预测之前使用了额外的Feed Forward Layer，而BART没有。</li>\n<li><strong>应用场景</strong>：相比GPT，BART增加了双向上下文语境信息，更适合文本生成。</li>\n</ul>\n<h3 id=\"技术术语简化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语简化\"><span>技术术语简化</span></a></h3>\n<ul>\n<li><strong>Encoder-Decoder</strong>：一种用于处理输入数据并生成输出数据的结构，类似于翻译系统。</li>\n<li><strong>GeLU激活函数</strong>：一种数学函数，用于帮助神经网络学习复杂模式。</li>\n<li><strong>cross-attention</strong>：一种机制，允许解码器更好地理解编码器生成的信息。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 将ReLU激活函数替换为GeLU。</li>\n<li>✅ 初始化参数为正态分布 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0.02</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(0,0.02)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0.02</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li>⚠ 确保解码器的各层执行cross-attention。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在实现BART时，容易忽略cross-attention机制，这会导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>BART在文本生成任务中的双向上下文语境信息是其优于GPT的一大创新点。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究BART在其他NLP任务中的应用潜力。</li>\n<li>比较BART与其他Transformer模型在不同任务中的表现。</li>\n<li>探索BART与其他激活函数的兼容性。</li>\n</ul>\n<blockquote>\n<p>来源：原始内容提供者不详，内容经过处理和总结。</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：BART, Transformer, NLP, 文本生成, 文本理解</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>BART是一种基于Transformer架构的模型，结合了双向和自回归的特性。它在文本生成任务中表现优异，同时也能在文本理解任务中取得领先的效果。\n<img src=\"/img/user/附件/Pasted image 20250424113608.png\" alt=\"Pasted image 20250424113608.png\"></p>\n<h3 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h3>\n<ul>\n<li><strong>模型架构</strong>：BART采用标准的encoder-decoder结构，并进行了若干调整。</li>\n<li><strong>激活函数与参数初始化</strong>：与GPT类似，BART使用GeLU激活函数，参数初始化服从正态分布 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0.02</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(0,0.02)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0.02</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li><strong>层数配置</strong>：BART base模型的Encoder和Decoder各有6层，而large模型则扩展到12层。</li>\n<li><strong>cross-attention</strong>：解码器的各层对编码器最终隐藏层额外执行cross-attention。</li>\n<li><strong>与BERT的区别</strong>：BERT在词预测之前使用了额外的Feed Forward Layer，而BART没有。</li>\n<li><strong>应用场景</strong>：相比GPT，BART增加了双向上下文语境信息，更适合文本生成。</li>\n</ul>\n<h3 id=\"技术术语简化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语简化\"><span>技术术语简化</span></a></h3>\n<ul>\n<li><strong>Encoder-Decoder</strong>：一种用于处理输入数据并生成输出数据的结构，类似于翻译系统。</li>\n<li><strong>GeLU激活函数</strong>：一种数学函数，用于帮助神经网络学习复杂模式。</li>\n<li><strong>cross-attention</strong>：一种机制，允许解码器更好地理解编码器生成的信息。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 将ReLU激活函数替换为GeLU。</li>\n<li>✅ 初始化参数为正态分布 <span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0.02</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(0,0.02)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0.02</span><span class=\"mclose\">)</span></span></span></span>。</li>\n<li>⚠ 确保解码器的各层执行cross-attention。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在实现BART时，容易忽略cross-attention机制，这会导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>BART在文本生成任务中的双向上下文语境信息是其优于GPT的一大创新点。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究BART在其他NLP任务中的应用潜力。</li>\n<li>比较BART与其他Transformer模型在不同任务中的表现。</li>\n<li>探索BART与其他激活函数的兼容性。</li>\n</ul>\n<blockquote>\n<p>来源：原始内容提供者不详，内容经过处理和总结。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：自然语言处理\n- 标签：BART, Transformer, NLP, 文本生成, 文本理解\n- 日期：2025年4月12日\n\n\n## 内容概述\nBART是一种基于Transformer架构的模型，结合了双向和自回归的特性。它在文本生成任务中表现优异，同时也能在文本理解任务中取得领先的效果。\n![Pasted image 20250424113608.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424113608.png)\n\n### 核心观点\n- **模型架构**：BART采用标准的encoder-decoder结构，并进行了若干调整。\n- **激活函数与参数初始化**：与GPT类似，BART使用GeLU激活函数，参数初始化服从正态分布 $N(0,0.02)$。\n- **层数配置**：BART base模型的Encoder和Decoder各有6层，而large模型则扩展到12层。\n- **cross-attention**：解码器的各层对编码器最终隐藏层额外执行cross-attention。\n- **与BERT的区别**：BERT在词预测之前使用了额外的Feed Forward Layer，而BART没有。\n- **应用场景**：相比GPT，BART增加了双向上下文语境信息，更适合文本生成。\n\n\n### 技术术语简化\n- **Encoder-Decoder**：一种用于处理输入数据并生成输出数据的结构，类似于翻译系统。\n- **GeLU激活函数**：一种数学函数，用于帮助神经网络学习复杂模式。\n- **cross-attention**：一种机制，允许解码器更好地理解编码器生成的信息。\n\n\n## 操作步骤\n1. ✅ 将ReLU激活函数替换为GeLU。\n2. ✅ 初始化参数为正态分布 $N(0,0.02)$。\n3. ⚠ 确保解码器的各层执行cross-attention。\n\n\n## 常见错误\n> ⚠ 在实现BART时，容易忽略cross-attention机制，这会导致模型性能下降。\n\n\n## 💡启发点\nBART在文本生成任务中的双向上下文语境信息是其优于GPT的一大创新点。\n\n\n## 行动清单\n- 研究BART在其他NLP任务中的应用潜力。\n- 比较BART与其他Transformer模型在不同任务中的表现。\n- 探索BART与其他激活函数的兼容性。\n\n> 来源：原始内容提供者不详，内容经过处理和总结。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容概述","slug":"内容概述","link":"#内容概述","children":[{"level":3,"title":"核心观点","slug":"核心观点","link":"#核心观点","children":[]},{"level":3,"title":"技术术语简化","slug":"技术术语简化","link":"#技术术语简化","children":[]}]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
