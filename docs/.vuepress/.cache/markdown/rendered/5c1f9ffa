{"content":"<ul>\n<li><strong>分类</strong>：自然语言处理</li>\n<li><strong>标签</strong>：强化学习，MDP建模，自然语言处理，优化目标，行为约束</li>\n<li><strong>日期</strong>：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>在自然语言处理（NLP）任务中应用强化学习（RL），需要进行马尔可夫决策过程（MDP）建模。通过明确定义代理、环境、状态和动作，可以实现对NLP任务的优化。强化学习的主要目标是最大化累积奖励的期望值，并通过行为约束来防止策略偏离。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"nlp-mdp建模\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlp-mdp建模\"><span>NLP MDP建模</span></a></h3>\n<p>强化学习应用于NLP任务时，需进行MDP建模。具体来说：</p>\n<ul>\n<li><strong>动作（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>）</strong>：生成的token，动作空间为整个词表。</li>\n<li><strong>策略（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi(a_t \\mid s_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>）</strong>：当前状态生成动作token的概率。</li>\n<li><strong>状态（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>）</strong>：上文及当前时刻前生成的所有token序列，初始状态为prompt的token序列。</li>\n<li><strong>状态转移</strong>：通过当前状态和动作的token序列确定下一个状态。</li>\n</ul>\n<p><img src=\"/img/user/附件/Pasted image 20250416102529.png\" alt=\"Pasted image 20250416102529.png\"></p>\n<h3 id=\"强化学习优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#强化学习优化目标\"><span>强化学习优化目标</span></a></h3>\n<p>强化学习旨在最大化累积奖励的期望值：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>π</mi><mi>max</mi><mo>⁡</mo></msub><msub><mi>E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator=\"true\">,</mo><mi>y</mi><mo>∼</mo><mi>π</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">x, y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span></span>分别表示prompt和response，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(x, y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>为奖励值。</p>\n<h3 id=\"行为约束优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行为约束优化目标\"><span>行为约束优化目标</span></a></h3>\n<p>带行为约束的优化目标通过修改奖励值来限制策略更新：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>π</mi><mi>max</mi><mo>⁡</mo></msub><msub><mi>E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator=\"true\">,</mo><mi>y</mi><mo>∼</mo><mi>π</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>−</mo><mi>β</mi><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>π</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>∥</mo><msub><mi>π</mi><mtext>ref</mtext></msub><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)] - \\beta D_{KL}[\\pi(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x)]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)]</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">ref</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>目的是约束策略在距离行为/参考策略不太远的范围内更新。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意：强化学习策略可能会因不当行为约束设置而偏离预期路径，应确保行为策略与参考策略一致。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>行为约束在强化学习中的应用如何影响模型的稳定性？</li>\n<li>如何选择合适的参考策略以避免策略偏离？</li>\n</ul>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>在不同NLP任务中，如何调整MDP建模以获得最佳结果？</li>\n<li>行为约束是否可以应用于其他领域的强化学习任务？</li>\n<li>如何评估行为约束对模型性能的影响？</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究MDP建模与强化学习在其他领域的应用。</li>\n<li>实验不同行为约束策略对模型效果的影响。</li>\n<li>开发更有效的奖励函数以提高模型性能。</li>\n</ul>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探讨如何在复杂NLP任务中实现更精确的MDP建模。</li>\n<li>研究行为约束对长期策略稳定性的影响。</li>\n</ul>\n<blockquote>\n<p>原始出处：[选取内容]</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RL在NLP场景下的拓展.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RL在NLP场景下的拓展.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RL在NLP场景下的拓展","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/RLHF基于人类反馈的强化学习/RL在NLP场景下的拓展/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-16T02:24:24.000Z","updated":"2025-04-16T02:25:32.000Z","title":"RL在NLP场景下的拓展","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><ul>\n<li><strong>分类</strong>：自然语言处理</li>\n<li><strong>标签</strong>：强化学习，MDP建模，自然语言处理，优化目标，行为约束</li>\n<li><strong>日期</strong>：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>在自然语言处理（NLP）任务中应用强化学习（RL），需要进行马尔可夫决策过程（MDP）建模。通过明确定义代理、环境、状态和动作，可以实现对NLP任务的优化。强化学习的主要目标是最大化累积奖励的期望值，并通过行为约束来防止策略偏离。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"nlp-mdp建模\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlp-mdp建模\"><span>NLP MDP建模</span></a></h3>\n<p>强化学习应用于NLP任务时，需进行MDP建模。具体来说：</p>\n<ul>\n<li><strong>动作（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>）</strong>：生成的token，动作空间为整个词表。</li>\n<li><strong>策略（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi(a_t \\mid s_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>）</strong>：当前状态生成动作token的概率。</li>\n<li><strong>状态（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>）</strong>：上文及当前时刻前生成的所有token序列，初始状态为prompt的token序列。</li>\n<li><strong>状态转移</strong>：通过当前状态和动作的token序列确定下一个状态。</li>\n</ul>\n<p><img src=\"/img/user/附件/Pasted image 20250416102529.png\" alt=\"Pasted image 20250416102529.png\"></p>\n<h3 id=\"强化学习优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#强化学习优化目标\"><span>强化学习优化目标</span></a></h3>\n<p>强化学习旨在最大化累积奖励的期望值：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>π</mi><mi>max</mi><mo>⁡</mo></msub><msub><mi>E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator=\"true\">,</mo><mi>y</mi><mo>∼</mo><mi>π</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">x, y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span></span>分别表示prompt和response，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(x, y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>为奖励值。</p>\n<h3 id=\"行为约束优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行为约束优化目标\"><span>行为约束优化目标</span></a></h3>\n<p>带行为约束的优化目标通过修改奖励值来限制策略更新：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>π</mi><mi>max</mi><mo>⁡</mo></msub><msub><mi>E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator=\"true\">,</mo><mi>y</mi><mo>∼</mo><mi>π</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>−</mo><mi>β</mi><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>π</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>∥</mo><msub><mi>π</mi><mtext>ref</mtext></msub><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)] - \\beta D_{KL}[\\pi(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x)]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)]</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">ref</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>目的是约束策略在距离行为/参考策略不太远的范围内更新。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意：强化学习策略可能会因不当行为约束设置而偏离预期路径，应确保行为策略与参考策略一致。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>行为约束在强化学习中的应用如何影响模型的稳定性？</li>\n<li>如何选择合适的参考策略以避免策略偏离？</li>\n</ul>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>在不同NLP任务中，如何调整MDP建模以获得最佳结果？</li>\n<li>行为约束是否可以应用于其他领域的强化学习任务？</li>\n<li>如何评估行为约束对模型性能的影响？</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究MDP建模与强化学习在其他领域的应用。</li>\n<li>实验不同行为约束策略对模型效果的影响。</li>\n<li>开发更有效的奖励函数以提高模型性能。</li>\n</ul>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探讨如何在复杂NLP任务中实现更精确的MDP建模。</li>\n<li>研究行为约束对长期策略稳定性的影响。</li>\n</ul>\n<blockquote>\n<p>原始出处：[选取内容]</p>\n</blockquote>\n</template>","contentStripped":"<ul>\n<li><strong>分类</strong>：自然语言处理</li>\n<li><strong>标签</strong>：强化学习，MDP建模，自然语言处理，优化目标，行为约束</li>\n<li><strong>日期</strong>：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>在自然语言处理（NLP）任务中应用强化学习（RL），需要进行马尔可夫决策过程（MDP）建模。通过明确定义代理、环境、状态和动作，可以实现对NLP任务的优化。强化学习的主要目标是最大化累积奖励的期望值，并通过行为约束来防止策略偏离。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"nlp-mdp建模\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlp-mdp建模\"><span>NLP MDP建模</span></a></h3>\n<p>强化学习应用于NLP任务时，需进行MDP建模。具体来说：</p>\n<ul>\n<li><strong>动作（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>）</strong>：生成的token，动作空间为整个词表。</li>\n<li><strong>策略（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi(a_t \\mid s_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>）</strong>：当前状态生成动作token的概率。</li>\n<li><strong>状态（<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>）</strong>：上文及当前时刻前生成的所有token序列，初始状态为prompt的token序列。</li>\n<li><strong>状态转移</strong>：通过当前状态和动作的token序列确定下一个状态。</li>\n</ul>\n<p><img src=\"/img/user/附件/Pasted image 20250416102529.png\" alt=\"Pasted image 20250416102529.png\"></p>\n<h3 id=\"强化学习优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#强化学习优化目标\"><span>强化学习优化目标</span></a></h3>\n<p>强化学习旨在最大化累积奖励的期望值：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>π</mi><mi>max</mi><mo>⁡</mo></msub><msub><mi>E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator=\"true\">,</mo><mi>y</mi><mo>∼</mo><mi>π</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">x, y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span></span>分别表示prompt和response，<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r(x, y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>为奖励值。</p>\n<h3 id=\"行为约束优化目标\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行为约束优化目标\"><span>行为约束优化目标</span></a></h3>\n<p>带行为约束的优化目标通过修改奖励值来限制策略更新：</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>π</mi><mi>max</mi><mo>⁡</mo></msub><msub><mi>E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator=\"true\">,</mo><mi>y</mi><mo>∼</mo><mi>π</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>−</mo><mi>β</mi><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>π</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>∥</mo><msub><mi>π</mi><mtext>ref</mtext></msub><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)] - \\beta D_{KL}[\\pi(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x)]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mop mtight\"><span class=\"mtight\">m</span><span class=\"mtight\">a</span><span class=\"mtight\">x</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel mtight\">∼</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)]</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∥</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">ref</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)]</span></span></span></span></span></p>\n<p>目的是约束策略在距离行为/参考策略不太远的范围内更新。</p>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>注意：强化学习策略可能会因不当行为约束设置而偏离预期路径，应确保行为策略与参考策略一致。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>行为约束在强化学习中的应用如何影响模型的稳定性？</li>\n<li>如何选择合适的参考策略以避免策略偏离？</li>\n</ul>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>在不同NLP任务中，如何调整MDP建模以获得最佳结果？</li>\n<li>行为约束是否可以应用于其他领域的强化学习任务？</li>\n<li>如何评估行为约束对模型性能的影响？</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究MDP建模与强化学习在其他领域的应用。</li>\n<li>实验不同行为约束策略对模型效果的影响。</li>\n<li>开发更有效的奖励函数以提高模型性能。</li>\n</ul>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探讨如何在复杂NLP任务中实现更精确的MDP建模。</li>\n<li>研究行为约束对长期策略稳定性的影响。</li>\n</ul>\n<blockquote>\n<p>原始出处：[选取内容]</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"- **分类**：自然语言处理\n- **标签**：强化学习，MDP建模，自然语言处理，优化目标，行为约束\n- **日期**：2025年4月12日\n\n## 核心观点总结\n在自然语言处理（NLP）任务中应用强化学习（RL），需要进行马尔可夫决策过程（MDP）建模。通过明确定义代理、环境、状态和动作，可以实现对NLP任务的优化。强化学习的主要目标是最大化累积奖励的期望值，并通过行为约束来防止策略偏离。\n\n\n## 重点段落\n\n### NLP MDP建模\n强化学习应用于NLP任务时，需进行MDP建模。具体来说：\n- **动作（$a_t$）**：生成的token，动作空间为整个词表。\n- **策略（$\\pi(a_t \\mid s_t)$）**：当前状态生成动作token的概率。\n- **状态（$s_t$）**：上文及当前时刻前生成的所有token序列，初始状态为prompt的token序列。\n- **状态转移**：通过当前状态和动作的token序列确定下一个状态。\n\n![Pasted image 20250416102529.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250416102529.png)\n\n\n### 强化学习优化目标\n强化学习旨在最大化累积奖励的期望值：\n$$\n\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)]\n$$\n其中$x, y$分别表示prompt和response，$r(x, y)$为奖励值。\n\n\n### 行为约束优化目标\n带行为约束的优化目标通过修改奖励值来限制策略更新：\n$$\n\\pi_{\\max} E_{x \\sim D, y \\sim \\pi} [r(x, y)] - \\beta D_{KL}[\\pi(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x)]\n$$\n目的是约束策略在距离行为/参考策略不太远的范围内更新。\n\n\n## 常见错误\n> 注意：强化学习策略可能会因不当行为约束设置而偏离预期路径，应确保行为策略与参考策略一致。\n\n\n## 💡启发点\n- 行为约束在强化学习中的应用如何影响模型的稳定性？\n- 如何选择合适的参考策略以避免策略偏离？\n\n\n## 思考\n- 在不同NLP任务中，如何调整MDP建模以获得最佳结果？\n- 行为约束是否可以应用于其他领域的强化学习任务？\n- 如何评估行为约束对模型性能的影响？\n\n\n## 行动清单\n- 研究MDP建模与强化学习在其他领域的应用。\n- 实验不同行为约束策略对模型效果的影响。\n- 开发更有效的奖励函数以提高模型性能。\n\n\n## 后续追踪\n- 探讨如何在复杂NLP任务中实现更精确的MDP建模。\n- 研究行为约束对长期策略稳定性的影响。\n\n> 原始出处：[选取内容]","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[{"level":3,"title":"NLP MDP建模","slug":"nlp-mdp建模","link":"#nlp-mdp建模","children":[]},{"level":3,"title":"强化学习优化目标","slug":"强化学习优化目标","link":"#强化学习优化目标","children":[]},{"level":3,"title":"行为约束优化目标","slug":"行为约束优化目标","link":"#行为约束优化目标","children":[]}]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"思考","slug":"思考","link":"#思考","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
