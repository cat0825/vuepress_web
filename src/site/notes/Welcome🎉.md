---
{"dg-publish":true,"dg-home":"true","permalink":"/welcome/","tags":["gardenEntry"],"dgPassFrontmatter":true,"noteIcon":"","created":"2024-12-24T13:56:28.000+08:00","updated":"2025-04-30T22:32:50.692+08:00"}
---



## **关于大语言模型学习导航**
- [[大语言模型学习\|大语言模型学习]]
  - [[Attention注意力机制\|Attention注意力机制]]
    - [[大语言模型学习/Attention注意力机制/Attention机制详解与应用\|Attention机制详解与应用]]
    - [[大语言模型学习/Attention注意力机制/DCA：长文本处理的新突破（Dual Chunk Attention）\|DCA：长文本处理的新突破（Dual Chunk Attention）]]
    - [[大语言模型学习/Attention注意力机制/KV Cache技术详解：优化Transformer自回归生成效率\|KV Cache技术详解：优化Transformer自回归生成效率]]
    - [[大语言模型学习/Attention注意力机制/Transformer中的Attention详解与应用指南\|Transformer中的Attention详解与应用指南]]
    - [[大语言模型学习/Attention注意力机制/【长上下文模型优化】基于Shifted Sparse Attention的创新方法\|【长上下文模型优化】基于Shifted Sparse Attention的创新方法]]
    - [[大语言模型学习/Attention注意力机制/优化Attention计算复杂度的技术探讨\|优化Attention计算复杂度的技术探讨]]
    - [[大语言模型学习/Attention注意力机制/深度学习中的注意力机制优化：从MHA到MLA\|深度学习中的注意力机制优化：从MHA到MLA]]
  - [[FFN、Add & LN 的作用与应用\|FFN、Add & LN 的作用与应用]]
    - [[大语言模型学习/FFN、Add & LN 的作用与应用/Transformer核心模块解析：FFN、Add & LN 的作用与应用\|Transformer核心模块解析：FFN、Add & LN 的作用与应用]]
    - [[大语言模型学习/FFN、Add & LN 的作用与应用/深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较\|深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较]]
    - [[大语言模型学习/FFN、Add & LN 的作用与应用/激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析\|激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析]]
    - [[大语言模型学习/FFN、Add & LN 的作用与应用/激活函数详解与比较：从Sigmoid到Swish\|激活函数详解与比较：从Sigmoid到Swish]]
  - [[Positional Encoding位置编码\|Positional Encoding位置编码]]
    - [[大语言模型学习/Positional Encoding位置编码/NTK插值方法解析与优化：从NTK-aware到NTK-by-parts\|NTK插值方法解析与优化：从NTK-aware到NTK-by-parts]]
    - [[大语言模型学习/Positional Encoding位置编码/YaRN方法解析：扩展RoPE嵌入与注意力优化的实践\|YaRN方法解析：扩展RoPE嵌入与注意力优化的实践]]
    - [[大语言模型学习/词嵌入/介绍\|位置编码介绍]]
    - [[大语言模型学习/Positional Encoding位置编码/位置内插法扩展语言模型上下文长度\|位置内插法扩展语言模型上下文长度]]
    - [[大语言模型学习/Positional Encoding位置编码/数字输入优化与外推方法解析\|数字输入优化与外推方法解析]]
    - [[大语言模型学习/Positional Encoding位置编码/旋转位置编码与ALiBi：深度学习中的位置嵌入优化\|旋转位置编码与ALiBi：深度学习中的位置嵌入优化]]
    - [[相对位置编码\|相对位置编码]]
      - [[大语言模型学习/Positional Encoding位置编码/相对位置编码/DeBERTa的相对位置编码与绝对位置编码解析\|DeBERTa的相对位置编码与绝对位置编码解析]]
      - [[大语言模型学习/Positional Encoding位置编码/相对位置编码/T5模型与相对位置编码优化解析\|T5模型与相对位置编码优化解析]]
      - [[大语言模型学习/Positional Encoding位置编码/相对位置编码/相对位置编码与XLNet位置编码详解 深入理解Transformer机制\|相对位置编码与XLNet位置编码详解 深入理解Transformer机制]]
    - [[绝对位置编码\|绝对位置编码]]
      - [[大语言模型学习/Positional Encoding位置编码/绝对位置编码/BERT与RNN位置编码的对比与应用\|BERT与RNN位置编码的对比与应用]]
      - [[大语言模型学习/Positional Encoding位置编码/绝对位置编码/Transformer绝对位置编码详解与改进分析\|Transformer绝对位置编码详解与改进分析]]
  - [[Pre-training 预训练\|Pre-training 预训练]]
    - [[大语言模型学习/Pre-training 预训练/推理耗时\|大语言模型学习/Pre-training 预训练/推理耗时]]
    - [[大语言模型学习/Pre-training 预训练/数据多样性与模型优化探索\|数据多样性与模型优化探索]]
    - [[大语言模型学习/Pre-training 预训练/数据清洗\|数据清洗]]
    - [[大语言模型学习/Pre-training 预训练/数据爬取\|数据爬取]]
    - [[大语言模型学习/Pre-training 预训练/数据配比与训练顺序优化指南\|数据配比与训练顺序优化指南]]
    - [[大语言模型学习/Pre-training 预训练/模型打分与数据去重\|模型打分与数据去重]]
    - [[大语言模型学习/Pre-training 预训练/深度学习中的显存优化与梯度处理方法\|深度学习中的显存优化与梯度处理方法]]
    - [[大语言模型学习/Pre-training 预训练/混合精度训练\|混合精度训练]]
    - [[大语言模型学习/Pre-training 预训练/继续预训练\|继续预训练]]
    - [[大语言模型学习/Pre-training 预训练/训练容灾及训练监控\|训练容灾及训练监控]]
    - [[大语言模型学习/Pre-training 预训练/预训练定义以及数据来源\|预训练定义以及数据来源]]
    - [[大语言模型学习/Pre-training 预训练/预训练评估\|预训练评估]]
    - [[大语言模型学习/Pre-training 预训练/预训练评估2\|预训练评估2]]
    - [[预训练过程\|预训练过程]]
      - [[大语言模型学习/Pre-training 预训练/预训练过程/训练Tokenizer\|训练Tokenizer]]
      - [[大语言模型学习/Pre-training 预训练/预训练过程/预训练的Scaling Law\|预训练的Scaling Law]]
      - [[大语言模型学习/Pre-training 预训练/预训练过程/预训练策略\|预训练策略]]
      - [[大语言模型学习/Pre-training 预训练/预训练过程/高效深度学习模型训练框架选择与优化指南\|高效深度学习模型训练框架选择与优化指南]]
  - [[RL强化学习基础\|RL强化学习基础]]
    - [[大语言模型学习/RL强化学习基础/SARSA-λ与Q-learning对比\|SARSA-λ与Q-learning对比]]
    - [[大语言模型学习/RL强化学习基础/SARSA算法\|SARSA算法]]
    - [[大语言模型学习/RL强化学习基础/价值迭代算法\|价值迭代算法]]
    - [[大语言模型学习/RL强化学习基础/强化学习分类\|强化学习分类]]
    - [[大语言模型学习/RL强化学习基础/强化学习的独特性\|强化学习的独特性]]
    - [[大语言模型学习/RL强化学习基础/强化学习问题,流程\|强化学习问题,流程]]
    - [[大语言模型学习/RL强化学习基础/时序差分算法\|时序差分算法]]
    - [[大语言模型学习/RL强化学习基础/深度Q网络\|深度Q网络]]
    - [[大语言模型学习/RL强化学习基础/策略迭代算法\|策略迭代算法]]
    - [[大语言模型学习/RL强化学习基础/蒙特卡洛方法\|蒙特卡洛方法]]
    - [[大语言模型学习/RL强化学习基础/贝尔曼方程\|贝尔曼方程]]
    - [[大语言模型学习/RL强化学习基础/马尔可夫决策过程\|马尔可夫决策过程]]
  - [[Structure & Decoding Policy 结构和解码策略\|Structure & Decoding Policy 结构和解码策略]]
    - [[大语言模型学习/Structure & Decoding Policy 结构和解码策略/大模型结构与混合专家（LLM & MoE）解析\|大模型结构与混合专家（LLM & MoE）解析]]
    - [[大语言模型学习/Structure & Decoding Policy 结构和解码策略/深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略\|深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略]]
    - [[大语言模型学习/Structure & Decoding Policy 结构和解码策略/解码采样策略：Greedy Search与Beam Search的实现与优化\|解码采样策略：Greedy Search与Beam Search的实现与优化]]
  - [[分词\|分词]]
    - [[大语言模型学习/分词/BBPE：字节级别的BPE分词技术解析与应用\|BBPE：字节级别的BPE分词技术解析与应用]]
    - [[大语言模型学习/分词/WordPiece分词算法解析与实践\|WordPiece分词算法解析与实践]]
    - [[大语言模型学习/分词/使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践\|使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践]]
    - [[大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践\|使用Unigram语言模型（ULM）优化分词算法：核心思路与实践]]
    - [[大语言模型学习/分词/分词算法的比较\|分词算法的比较]]
    - [[大语言模型学习/分词/常用分词库\|常用分词库]]
  - [[后训练\|后训练]]
    - [[SFT监督微调\|SFT监督微调]]
      - [[SFT数据及处理\|SFT数据及处理]]
        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/开源数据集\|开源数据集]]
        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据多样性探索\|数据多样性探索]]
        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据生产合成与质量过滤\|数据生产合成与质量过滤]]
        - [[大语言模型学习/后训练/SFT监督微调/SFT数据及处理/数据飞轮在SFT中的应用与优化\|数据飞轮在SFT中的应用与优化]]
      - [[STF训练\|STF训练]]
        - [[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升\|多轮对话专项提升]]
        - [[大语言模型学习/后训练/SFT监督微调/STF训练/多轮对话专项提升2\|多轮对话专项提升2]]
        - [[大语言模型学习/后训练/SFT监督微调/STF训练/训练启动脚本\|训练启动脚本]]
        - [[大语言模型学习/后训练/SFT监督微调/STF训练/训练技巧和训练策略\|训练技巧和训练策略]]
        - [[大语言模型学习/后训练/SFT监督微调/STF训练/训练框架及参数设置\|训练框架及参数设置]]
      - [[大语言模型学习/后训练/SFT监督微调/监督微调与预训练的区别\|监督微调与预训练的区别]]
  - [[词嵌入\|词嵌入]]
    - [[大语言模型学习/词嵌入/FastText\|FastText]]
    - [[大语言模型学习/词嵌入/oneHot\|oneHot]]
    - [[大语言模型学习/词嵌入/Word2Vec\|Word2Vec]]
    - [[大语言模型学习/词嵌入/介绍\|词嵌入介绍]]
