{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：深度学习、自然语言处理</li>\n<li><strong>标签</strong>：Transformer、前馈网络、层归一化、残差连接、深度学习优化</li>\n<li><strong>日期</strong>：2025年3月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>Transformer 中的 FFN（前馈网络）、Add（残差连接）和 LN（层归一化）是构建其强大性能的关键模块。这些模块分别承担了以下功能：</p>\n<ul>\n<li><strong>FFN 前馈网络</strong>：在多头注意力（MHA）后，独立处理每个 token 的信息，完成更深层次的学习与计算。</li>\n<li><strong>Add 残差连接</strong>：缓解深层网络中的梯度消失问题，确保更好的梯度回传。</li>\n<li><strong>LN 层归一化</strong>：加速模型收敛，避免梯度爆炸/消失问题，特别适合 NLP 场景。</li>\n</ul>\n<hr>\n<h2 id=\"详细解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#详细解析\"><span>详细解析</span></a></h2>\n<h3 id=\"✅-ffn-前馈网络-独立计算的核心\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-ffn-前馈网络-独立计算的核心\"><span>✅ <strong>FFN 前馈网络：独立计算的核心</strong></span></a></h3>\n<p>FeedForward Network 是 Transformer 中的关键组件。MHA（多头注意力）模块汇聚了不同 token 之间的信息，但 FFN 则负责让每个 token 独立思考这些信息。</p>\n<ul>\n<li><strong>作用</strong>：模拟“交流之后的个人思考”，通过非线性变换进一步提取特征。</li>\n<li><strong>特点</strong>：每个 token 的处理是独立的，不涉及其他 token 的交互。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：FFN 是 Transformer 的“计算引擎”，让模型不仅能看，还能“思考”。</p>\n<hr>\n<h3 id=\"⚠️-add-残差连接-优化深层网络的梯度回传\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-add-残差连接-优化深层网络的梯度回传\"><span>⚠️ <strong>Add 残差连接：优化深层网络的梯度回传</strong></span></a></h3>\n<p>深度神经网络常面临梯度消失或爆炸问题，而残差连接为梯度提供了一条“高速通道”：</p>\n<ul>\n<li><strong>作用</strong>：通过直接加法操作，将输入信息直接传递到后续层，确保即使网络很深，梯度依然能顺畅地回传。</li>\n<li><strong>好处</strong>：\n<ul>\n<li>初始阶段，残差块影响较小，但为梯度提供了稳定路径。</li>\n<li>随着训练进行，残差块逐渐增强其作用。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：残差连接是深层网络训练成功的关键之一。</p>\n<hr>\n<h3 id=\"❗️-ln-层归一化-nlp-中的收敛加速器\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#❗️-ln-层归一化-nlp-中的收敛加速器\"><span>❗️ <strong>LN 层归一化：NLP 中的收敛加速器</strong></span></a></h3>\n<p>Layer Normalization 在 NLP 任务中尤为重要，与 Batch Normalization 不同，它针对每个样本的特定维度进行归一化：</p>\n<ul>\n<li><strong>区别</strong>：\n<ul>\n<li>Batch Norm 在样本批次维度（N）上归一化，适合计算机视觉任务。</li>\n<li>Layer Norm 在通道维度（C）上归一化，更适合自然语言处理任务。</li>\n</ul>\n</li>\n<li><strong>应用场景</strong>：\n<ul>\n<li>NLP 中需要保留句子内的分布信息，因此 Layer Norm 是更优选择。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：Layer Norm 能更好地保留上下文语义，是 NLP 模型的标配。</p>\n<hr>\n<h3 id=\"📈-数据对比表-batch-norm-vs-layer-norm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-数据对比表-batch-norm-vs-layer-norm\"><span>📈 数据对比表：Batch Norm vs Layer Norm</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>Batch Norm</th>\n<th>Layer Norm</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>归一化维度</td>\n<td>样本批次维度（N）</td>\n<td>通道维度（C）</td>\n</tr>\n<tr>\n<td>应用场景</td>\n<td>计算机视觉（CV）</td>\n<td>自然语言处理（NLP）</td>\n</tr>\n<tr>\n<td>是否保留上下文信息</td>\n<td>否</td>\n<td>是</td>\n</tr>\n<tr>\n<td>举例</td>\n<td>图像 RGB 通道归一化</td>\n<td>每句话独立归一化</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"常见错误警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告区块\"><span>常见错误警告区块</span></a></h2>\n<p>⚠️ <strong>常见错误</strong>：</p>\n<ol>\n<li>将 Batch Norm 用于 NLP 模型，导致上下文信息丢失。</li>\n<li>忽略残差连接的重要性，导致深层网络难以优化。</li>\n<li>忽视 FFN 的独立性，误以为它也会进行 token 间的交互。</li>\n</ol>\n<hr>\n<h2 id=\"行动清单-📋\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-📋\"><span>行动清单 📋</span></a></h2>\n<ol>\n<li>在构建 NLP 模型时，优先选择 Layer Norm 而非 Batch Norm。</li>\n<li>确保残差连接在深层网络中被正确实现，以提升训练稳定性。</li>\n<li>深入理解 FFN 的独立性，并结合实验验证其在特征提取中的作用。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>残差连接是否适用于任何类型的深度学习模型？在什么情况下可能不适用？</li>\n<li>除了 Layer Norm 和 Batch Norm，还有哪些归一化方法可以优化 NLP 模型？</li>\n<li>FFN 能否进一步改进，比如加入更多的上下文交互？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原文参考自 Transformer 核心模块解析材料。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/FFN、Add & LN 的作用与应用/Transformer核心模块解析：FFN、Add & LN 的作用与应用.md","filePathRelative":"notes_bak/大语言模型学习/FFN、Add & LN 的作用与应用/Transformer核心模块解析：FFN、Add & LN 的作用与应用.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Attention注意力机制/Transformer核心模块解析：FFN、Add-&-LN-的作用与应用","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Attention注意力机制/Transformer核心模块解析：FFN、Add-&-LN-的作用与应用/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-04T04:53:26.000Z","updated":"2025-04-13T05:06:02.000Z","title":"Transformer核心模块解析：FFN、Add & LN 的作用与应用","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：深度学习、自然语言处理</li>\n<li><strong>标签</strong>：Transformer、前馈网络、层归一化、残差连接、深度学习优化</li>\n<li><strong>日期</strong>：2025年3月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>Transformer 中的 FFN（前馈网络）、Add（残差连接）和 LN（层归一化）是构建其强大性能的关键模块。这些模块分别承担了以下功能：</p>\n<ul>\n<li><strong>FFN 前馈网络</strong>：在多头注意力（MHA）后，独立处理每个 token 的信息，完成更深层次的学习与计算。</li>\n<li><strong>Add 残差连接</strong>：缓解深层网络中的梯度消失问题，确保更好的梯度回传。</li>\n<li><strong>LN 层归一化</strong>：加速模型收敛，避免梯度爆炸/消失问题，特别适合 NLP 场景。</li>\n</ul>\n<hr>\n<h2 id=\"详细解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#详细解析\"><span>详细解析</span></a></h2>\n<h3 id=\"✅-ffn-前馈网络-独立计算的核心\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-ffn-前馈网络-独立计算的核心\"><span>✅ <strong>FFN 前馈网络：独立计算的核心</strong></span></a></h3>\n<p>FeedForward Network 是 Transformer 中的关键组件。MHA（多头注意力）模块汇聚了不同 token 之间的信息，但 FFN 则负责让每个 token 独立思考这些信息。</p>\n<ul>\n<li><strong>作用</strong>：模拟“交流之后的个人思考”，通过非线性变换进一步提取特征。</li>\n<li><strong>特点</strong>：每个 token 的处理是独立的，不涉及其他 token 的交互。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：FFN 是 Transformer 的“计算引擎”，让模型不仅能看，还能“思考”。</p>\n<hr>\n<h3 id=\"⚠️-add-残差连接-优化深层网络的梯度回传\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-add-残差连接-优化深层网络的梯度回传\"><span>⚠️ <strong>Add 残差连接：优化深层网络的梯度回传</strong></span></a></h3>\n<p>深度神经网络常面临梯度消失或爆炸问题，而残差连接为梯度提供了一条“高速通道”：</p>\n<ul>\n<li><strong>作用</strong>：通过直接加法操作，将输入信息直接传递到后续层，确保即使网络很深，梯度依然能顺畅地回传。</li>\n<li><strong>好处</strong>：\n<ul>\n<li>初始阶段，残差块影响较小，但为梯度提供了稳定路径。</li>\n<li>随着训练进行，残差块逐渐增强其作用。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：残差连接是深层网络训练成功的关键之一。</p>\n<hr>\n<h3 id=\"❗️-ln-层归一化-nlp-中的收敛加速器\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#❗️-ln-层归一化-nlp-中的收敛加速器\"><span>❗️ <strong>LN 层归一化：NLP 中的收敛加速器</strong></span></a></h3>\n<p>Layer Normalization 在 NLP 任务中尤为重要，与 Batch Normalization 不同，它针对每个样本的特定维度进行归一化：</p>\n<ul>\n<li><strong>区别</strong>：\n<ul>\n<li>Batch Norm 在样本批次维度（N）上归一化，适合计算机视觉任务。</li>\n<li>Layer Norm 在通道维度（C）上归一化，更适合自然语言处理任务。</li>\n</ul>\n</li>\n<li><strong>应用场景</strong>：\n<ul>\n<li>NLP 中需要保留句子内的分布信息，因此 Layer Norm 是更优选择。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：Layer Norm 能更好地保留上下文语义，是 NLP 模型的标配。</p>\n<hr>\n<h3 id=\"📈-数据对比表-batch-norm-vs-layer-norm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-数据对比表-batch-norm-vs-layer-norm\"><span>📈 数据对比表：Batch Norm vs Layer Norm</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>Batch Norm</th>\n<th>Layer Norm</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>归一化维度</td>\n<td>样本批次维度（N）</td>\n<td>通道维度（C）</td>\n</tr>\n<tr>\n<td>应用场景</td>\n<td>计算机视觉（CV）</td>\n<td>自然语言处理（NLP）</td>\n</tr>\n<tr>\n<td>是否保留上下文信息</td>\n<td>否</td>\n<td>是</td>\n</tr>\n<tr>\n<td>举例</td>\n<td>图像 RGB 通道归一化</td>\n<td>每句话独立归一化</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"常见错误警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告区块\"><span>常见错误警告区块</span></a></h2>\n<p>⚠️ <strong>常见错误</strong>：</p>\n<ol>\n<li>将 Batch Norm 用于 NLP 模型，导致上下文信息丢失。</li>\n<li>忽略残差连接的重要性，导致深层网络难以优化。</li>\n<li>忽视 FFN 的独立性，误以为它也会进行 token 间的交互。</li>\n</ol>\n<hr>\n<h2 id=\"行动清单-📋\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-📋\"><span>行动清单 📋</span></a></h2>\n<ol>\n<li>在构建 NLP 模型时，优先选择 Layer Norm 而非 Batch Norm。</li>\n<li>确保残差连接在深层网络中被正确实现，以提升训练稳定性。</li>\n<li>深入理解 FFN 的独立性，并结合实验验证其在特征提取中的作用。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>残差连接是否适用于任何类型的深度学习模型？在什么情况下可能不适用？</li>\n<li>除了 Layer Norm 和 Batch Norm，还有哪些归一化方法可以优化 NLP 模型？</li>\n<li>FFN 能否进一步改进，比如加入更多的上下文交互？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原文参考自 Transformer 核心模块解析材料。</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：深度学习、自然语言处理</li>\n<li><strong>标签</strong>：Transformer、前馈网络、层归一化、残差连接、深度学习优化</li>\n<li><strong>日期</strong>：2025年3月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>Transformer 中的 FFN（前馈网络）、Add（残差连接）和 LN（层归一化）是构建其强大性能的关键模块。这些模块分别承担了以下功能：</p>\n<ul>\n<li><strong>FFN 前馈网络</strong>：在多头注意力（MHA）后，独立处理每个 token 的信息，完成更深层次的学习与计算。</li>\n<li><strong>Add 残差连接</strong>：缓解深层网络中的梯度消失问题，确保更好的梯度回传。</li>\n<li><strong>LN 层归一化</strong>：加速模型收敛，避免梯度爆炸/消失问题，特别适合 NLP 场景。</li>\n</ul>\n<hr>\n<h2 id=\"详细解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#详细解析\"><span>详细解析</span></a></h2>\n<h3 id=\"✅-ffn-前馈网络-独立计算的核心\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-ffn-前馈网络-独立计算的核心\"><span>✅ <strong>FFN 前馈网络：独立计算的核心</strong></span></a></h3>\n<p>FeedForward Network 是 Transformer 中的关键组件。MHA（多头注意力）模块汇聚了不同 token 之间的信息，但 FFN 则负责让每个 token 独立思考这些信息。</p>\n<ul>\n<li><strong>作用</strong>：模拟“交流之后的个人思考”，通过非线性变换进一步提取特征。</li>\n<li><strong>特点</strong>：每个 token 的处理是独立的，不涉及其他 token 的交互。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：FFN 是 Transformer 的“计算引擎”，让模型不仅能看，还能“思考”。</p>\n<hr>\n<h3 id=\"⚠️-add-残差连接-优化深层网络的梯度回传\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-add-残差连接-优化深层网络的梯度回传\"><span>⚠️ <strong>Add 残差连接：优化深层网络的梯度回传</strong></span></a></h3>\n<p>深度神经网络常面临梯度消失或爆炸问题，而残差连接为梯度提供了一条“高速通道”：</p>\n<ul>\n<li><strong>作用</strong>：通过直接加法操作，将输入信息直接传递到后续层，确保即使网络很深，梯度依然能顺畅地回传。</li>\n<li><strong>好处</strong>：\n<ul>\n<li>初始阶段，残差块影响较小，但为梯度提供了稳定路径。</li>\n<li>随着训练进行，残差块逐渐增强其作用。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：残差连接是深层网络训练成功的关键之一。</p>\n<hr>\n<h3 id=\"❗️-ln-层归一化-nlp-中的收敛加速器\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#❗️-ln-层归一化-nlp-中的收敛加速器\"><span>❗️ <strong>LN 层归一化：NLP 中的收敛加速器</strong></span></a></h3>\n<p>Layer Normalization 在 NLP 任务中尤为重要，与 Batch Normalization 不同，它针对每个样本的特定维度进行归一化：</p>\n<ul>\n<li><strong>区别</strong>：\n<ul>\n<li>Batch Norm 在样本批次维度（N）上归一化，适合计算机视觉任务。</li>\n<li>Layer Norm 在通道维度（C）上归一化，更适合自然语言处理任务。</li>\n</ul>\n</li>\n<li><strong>应用场景</strong>：\n<ul>\n<li>NLP 中需要保留句子内的分布信息，因此 Layer Norm 是更优选择。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：Layer Norm 能更好地保留上下文语义，是 NLP 模型的标配。</p>\n<hr>\n<h3 id=\"📈-数据对比表-batch-norm-vs-layer-norm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-数据对比表-batch-norm-vs-layer-norm\"><span>📈 数据对比表：Batch Norm vs Layer Norm</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>Batch Norm</th>\n<th>Layer Norm</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>归一化维度</td>\n<td>样本批次维度（N）</td>\n<td>通道维度（C）</td>\n</tr>\n<tr>\n<td>应用场景</td>\n<td>计算机视觉（CV）</td>\n<td>自然语言处理（NLP）</td>\n</tr>\n<tr>\n<td>是否保留上下文信息</td>\n<td>否</td>\n<td>是</td>\n</tr>\n<tr>\n<td>举例</td>\n<td>图像 RGB 通道归一化</td>\n<td>每句话独立归一化</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"常见错误警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告区块\"><span>常见错误警告区块</span></a></h2>\n<p>⚠️ <strong>常见错误</strong>：</p>\n<ol>\n<li>将 Batch Norm 用于 NLP 模型，导致上下文信息丢失。</li>\n<li>忽略残差连接的重要性，导致深层网络难以优化。</li>\n<li>忽视 FFN 的独立性，误以为它也会进行 token 间的交互。</li>\n</ol>\n<hr>\n<h2 id=\"行动清单-📋\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单-📋\"><span>行动清单 📋</span></a></h2>\n<ol>\n<li>在构建 NLP 模型时，优先选择 Layer Norm 而非 Batch Norm。</li>\n<li>确保残差连接在深层网络中被正确实现，以提升训练稳定性。</li>\n<li>深入理解 FFN 的独立性，并结合实验验证其在特征提取中的作用。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>残差连接是否适用于任何类型的深度学习模型？在什么情况下可能不适用？</li>\n<li>除了 Layer Norm 和 Batch Norm，还有哪些归一化方法可以优化 NLP 模型？</li>\n<li>FFN 能否进一步改进，比如加入更多的上下文交互？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原文参考自 Transformer 核心模块解析材料。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 元数据\n- **分类**：深度学习、自然语言处理\n- **标签**：Transformer、前馈网络、层归一化、残差连接、深度学习优化\n- **日期**：2025年3月2日  \n\n---\n\n\n\n## 核心内容总结\nTransformer 中的 FFN（前馈网络）、Add（残差连接）和 LN（层归一化）是构建其强大性能的关键模块。这些模块分别承担了以下功能：\n- **FFN 前馈网络**：在多头注意力（MHA）后，独立处理每个 token 的信息，完成更深层次的学习与计算。\n- **Add 残差连接**：缓解深层网络中的梯度消失问题，确保更好的梯度回传。\n- **LN 层归一化**：加速模型收敛，避免梯度爆炸/消失问题，特别适合 NLP 场景。\n\n---\n\n\n\n## 详细解析\n\n### ✅ **FFN 前馈网络：独立计算的核心**\nFeedForward Network 是 Transformer 中的关键组件。MHA（多头注意力）模块汇聚了不同 token 之间的信息，但 FFN 则负责让每个 token 独立思考这些信息。\n- **作用**：模拟“交流之后的个人思考”，通过非线性变换进一步提取特征。\n- **特点**：每个 token 的处理是独立的，不涉及其他 token 的交互。\n\n💡 **启发点**：FFN 是 Transformer 的“计算引擎”，让模型不仅能看，还能“思考”。\n\n---\n\n\n### ⚠️ **Add 残差连接：优化深层网络的梯度回传**\n深度神经网络常面临梯度消失或爆炸问题，而残差连接为梯度提供了一条“高速通道”：\n- **作用**：通过直接加法操作，将输入信息直接传递到后续层，确保即使网络很深，梯度依然能顺畅地回传。\n- **好处**：\n  - 初始阶段，残差块影响较小，但为梯度提供了稳定路径。\n  - 随着训练进行，残差块逐渐增强其作用。\n\n💡 **启发点**：残差连接是深层网络训练成功的关键之一。\n\n---\n\n\n### ❗️ **LN 层归一化：NLP 中的收敛加速器**\nLayer Normalization 在 NLP 任务中尤为重要，与 Batch Normalization 不同，它针对每个样本的特定维度进行归一化：\n- **区别**：\n  - Batch Norm 在样本批次维度（N）上归一化，适合计算机视觉任务。\n  - Layer Norm 在通道维度（C）上归一化，更适合自然语言处理任务。\n- **应用场景**：\n  - NLP 中需要保留句子内的分布信息，因此 Layer Norm 是更优选择。\n\n💡 **启发点**：Layer Norm 能更好地保留上下文语义，是 NLP 模型的标配。\n\n---\n\n\n### 📈 数据对比表：Batch Norm vs Layer Norm\n| 特性               | Batch Norm                  | Layer Norm                  |\n|--------------------|----------------------------|-----------------------------|\n| 归一化维度         | 样本批次维度（N）           | 通道维度（C）               |\n| 应用场景           | 计算机视觉（CV）            | 自然语言处理（NLP）         |\n| 是否保留上下文信息 | 否                         | 是                          |\n| 举例               | 图像 RGB 通道归一化         | 每句话独立归一化            |\n\n---\n\n\n\n## 常见错误警告区块\n⚠️ **常见错误**：\n1. 将 Batch Norm 用于 NLP 模型，导致上下文信息丢失。\n2. 忽略残差连接的重要性，导致深层网络难以优化。\n3. 忽视 FFN 的独立性，误以为它也会进行 token 间的交互。\n\n---\n\n\n\n## 行动清单 📋\n1. 在构建 NLP 模型时，优先选择 Layer Norm 而非 Batch Norm。\n2. 确保残差连接在深层网络中被正确实现，以提升训练稳定性。\n3. 深入理解 FFN 的独立性，并结合实验验证其在特征提取中的作用。\n\n---\n\n\n\n## [思考] 延伸问题\n1. 残差连接是否适用于任何类型的深度学习模型？在什么情况下可能不适用？\n2. 除了 Layer Norm 和 Batch Norm，还有哪些归一化方法可以优化 NLP 模型？\n3. FFN 能否进一步改进，比如加入更多的上下文交互？\n\n---\n\n> 原文参考自 Transformer 核心模块解析材料。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心内容总结","slug":"核心内容总结","link":"#核心内容总结","children":[]},{"level":2,"title":"详细解析","slug":"详细解析","link":"#详细解析","children":[{"level":3,"title":"✅ FFN 前馈网络：独立计算的核心","slug":"✅-ffn-前馈网络-独立计算的核心","link":"#✅-ffn-前馈网络-独立计算的核心","children":[]},{"level":3,"title":"⚠️ Add 残差连接：优化深层网络的梯度回传","slug":"⚠️-add-残差连接-优化深层网络的梯度回传","link":"#⚠️-add-残差连接-优化深层网络的梯度回传","children":[]},{"level":3,"title":"❗️ LN 层归一化：NLP 中的收敛加速器","slug":"❗️-ln-层归一化-nlp-中的收敛加速器","link":"#❗️-ln-层归一化-nlp-中的收敛加速器","children":[]},{"level":3,"title":"📈 数据对比表：Batch Norm vs Layer Norm","slug":"📈-数据对比表-batch-norm-vs-layer-norm","link":"#📈-数据对比表-batch-norm-vs-layer-norm","children":[]}]},{"level":2,"title":"常见错误警告区块","slug":"常见错误警告区块","link":"#常见错误警告区块","children":[]},{"level":2,"title":"行动清单 📋","slug":"行动清单-📋","link":"#行动清单-📋","children":[]},{"level":2,"title":"[思考] 延伸问题","slug":"思考-延伸问题","link":"#思考-延伸问题","children":[]}]}}
