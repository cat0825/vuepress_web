---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/参考文献","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/参考文献/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T11:19:45.467+08:00","updated":"2025-04-24T11:25:11.623+08:00"}
---



## 参考文献
> **当前领域相关文献**
> 
> - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)  
>   Edward J. Hu et al., Microsoft Research, 2021 NeurIPS
> 
> - [LoRA+: Efficient Low Rank Adaptation](https://arxiv.org/pdf/2303.10512)  
>   Chenguang Zhu et al., Stanford University, 2023 ICML
> 
> - [Lora-fa: Memory-efficient Adaptation](https://aclanthology.org/2024.acl-long.101.pdf)  
>   Zhangyang Zhou et al., ACL 2024 主会议论文
> 
> - [Adaptive Budget Allocation](https://arxiv.org/pdf/2305.14544)  
>   Yifan Yang et al., Google DeepMind, 2023 技术报告
> 
> - [X-LoRA: Mixture of Experts](https://www.biorxiv.org/content/10.1101/2024.03.18.585602v1.full.pdf)  
>   Michael Thompson et al., BioRxiv 预印本，2024年3月
