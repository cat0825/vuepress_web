{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：中文预训练、Tokenizer、语言模型优化</li>\n<li><strong>日期</strong>：2023-10-12</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>在构建中文语言模型时，预训练流程是关键步骤之一。其中，Tokenizer的训练与优化直接影响模型的性能与适用性。本文将重点解析如何通过词表扩充、压缩率控制等方式优化Tokenizer，并探讨中文预训练的独特挑战与解决方案。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"tokenizer的作用与训练方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#tokenizer的作用与训练方法\"><span>Tokenizer的作用与训练方法</span></a></h3>\n<p>Tokenizer（分词器）的主要作用是将输入的句子切分为词或字，并将这些切分结果转化为模型可理解的token。这是预训练模型的第一步。</p>\n<h4 id=\"✅-tokenizer训练步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-tokenizer训练步骤\"><span>✅ Tokenizer训练步骤：</span></a></h4>\n<ol>\n<li><strong>选择算法</strong>：使用BPE（Byte Pair Encoding）、BBPE（Balanced Byte Pair Encoding）或WordPiece算法。</li>\n<li><strong>数据准备</strong>：收集通用大规模数据集和业务场景相关数据。</li>\n<li><strong>环境需求</strong>：需要内存较大的CPU机器。</li>\n<li><strong>压缩率控制</strong>：保持1个token约对应1.5个汉字以平衡解码效率与模型知识能力。</li>\n<li><strong>词表扩充</strong>：手动添加常见汉字或业务场景相关词汇。</li>\n</ol>\n<p>💡<strong>启发点</strong>：压缩率过低会导致解码效率低，而压缩率过高会影响模型知识表达能力，因此需要找到一个平衡点。</p>\n<hr>\n<h3 id=\"中文预训练的独特挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#中文预训练的独特挑战\"><span>中文预训练的独特挑战</span></a></h3>\n<p>许多优秀的语言模型在中文任务上的表现不佳，因为它们的预训练主要基于英文语料。为解决这一问题，研究者通常会对英文模型进行二次预训练。</p>\n<h4 id=\"⚠-常见问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠-常见问题\"><span>⚠ 常见问题：</span></a></h4>\n<ul>\n<li>数字切分错误（如“9.9 &gt; 9.11”问题）。</li>\n<li>词表中敏感或脏token未移除。</li>\n<li>业务场景相关token覆盖不足。</li>\n</ul>\n<hr>\n<h3 id=\"词表扩充实例对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#词表扩充实例对比\"><span>词表扩充实例对比</span></a></h3>\n<p>通过对比Chinese-LLaMA与原始LLaMA的Tokenizer，可以发现：</p>\n<ul>\n<li><strong>Chinese-LLaMA</strong>新增了17953个tokens，大部分为汉字。</li>\n<li><strong>BELLE模型</strong>在120万行中文文本上训练了一个规模为5万的token集合，并将其与原始LLaMA词表合并。\n<img src=\"/img/user/附件/Pasted image 20250409220047.png\" alt=\"Pasted image 20250409220047.png\"></li>\n</ul>\n<h4 id=\"📊-数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📊-数据表格示例\"><span>📊 数据表格示例：</span></a></h4>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>新增tokens数量</th>\n<th>数据规模</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Chinese-LLaMA</td>\n<td>17953</td>\n<td>未明确</td>\n</tr>\n<tr>\n<td>BELLE</td>\n<td>50000</td>\n<td>120万行文本</td>\n</tr>\n</tbody>\n</table>\n<p>💡<strong>启发点</strong>：通过扩充词表，可以有效降低模型训练难度，提升其适用于中文任务的能力。</p>\n<hr>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ol>\n<li><strong>BPE/BBPE/WordPiece算法</strong>：一种将文本切分为小单位（如词或字）的方法，用于构建Tokenizer。</li>\n<li><strong>压缩率</strong>：指一个token平均对应多少个汉字，影响解码效率和模型知识能力。</li>\n<li><strong>词表扩充</strong>：手动添加常见汉字或特定领域词汇，以优化模型性能。</li>\n</ol>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考]板块</span></a></h2>\n<ol>\n<li>如何进一步优化Tokenizer以支持多语种任务？</li>\n<li>是否可以设计一种动态调整压缩率的方法以适应不同任务场景？</li>\n<li>在中文预训练中，如何平衡通用性与领域专用性？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：本文内容基于某技术文档关于中文语言模型预训练与Tokenizer优化的部分内容整理与总结。</p>\n</blockquote>\n<hr>\n<h2 id=\"常见错误警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告区块\"><span>常见错误警告区块</span></a></h2>\n<blockquote>\n<p>⚠ <strong>注意事项</strong></p>\n<ul>\n<li>数字切分问题需特别关注，避免影响模型回答准确性。</li>\n<li>词表扩充时需确保覆盖足够的中英词汇，同时避免加入敏感或无意义的token。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>收集更大规模的中文数据集以支持Tokenizer训练。</li>\n<li>针对业务场景设计定制化词表扩充策略。</li>\n<li>测试不同压缩率对模型性能的影响，寻找最佳平衡点。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着中文语言模型需求的增加，未来可能出现：</p>\n<ul>\n<li>更高效的中文Tokenizer算法。</li>\n<li>动态调整词表大小和压缩率的技术。</li>\n<li>支持多语种任务的统一预训练框架。</li>\n</ul>\n<hr>\n<h2 id=\"后续追踪方向\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪方向\"><span>后续追踪方向</span></a></h2>\n<ol>\n<li>对比不同中文大模型在实际任务中的表现。</li>\n<li>探索如何在低资源环境下实现高质量中文预训练。</li>\n<li>调研针对小语种任务的词表扩充策略。</li>\n</ol>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Pre-training 预训练/预训练过程/训练Tokenizer.md","filePathRelative":"notes_bak/大语言模型学习/Pre-training 预训练/预训练过程/训练Tokenizer.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/预训练过程/训练Tokenizer","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/预训练过程/训练Tokenizer/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-08T07:17:56.000Z","updated":"2025-04-13T05:06:02.000Z","title":"训练Tokenizer","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：中文预训练、Tokenizer、语言模型优化</li>\n<li><strong>日期</strong>：2023-10-12</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>在构建中文语言模型时，预训练流程是关键步骤之一。其中，Tokenizer的训练与优化直接影响模型的性能与适用性。本文将重点解析如何通过词表扩充、压缩率控制等方式优化Tokenizer，并探讨中文预训练的独特挑战与解决方案。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"tokenizer的作用与训练方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#tokenizer的作用与训练方法\"><span>Tokenizer的作用与训练方法</span></a></h3>\n<p>Tokenizer（分词器）的主要作用是将输入的句子切分为词或字，并将这些切分结果转化为模型可理解的token。这是预训练模型的第一步。</p>\n<h4 id=\"✅-tokenizer训练步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-tokenizer训练步骤\"><span>✅ Tokenizer训练步骤：</span></a></h4>\n<ol>\n<li><strong>选择算法</strong>：使用BPE（Byte Pair Encoding）、BBPE（Balanced Byte Pair Encoding）或WordPiece算法。</li>\n<li><strong>数据准备</strong>：收集通用大规模数据集和业务场景相关数据。</li>\n<li><strong>环境需求</strong>：需要内存较大的CPU机器。</li>\n<li><strong>压缩率控制</strong>：保持1个token约对应1.5个汉字以平衡解码效率与模型知识能力。</li>\n<li><strong>词表扩充</strong>：手动添加常见汉字或业务场景相关词汇。</li>\n</ol>\n<p>💡<strong>启发点</strong>：压缩率过低会导致解码效率低，而压缩率过高会影响模型知识表达能力，因此需要找到一个平衡点。</p>\n<hr>\n<h3 id=\"中文预训练的独特挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#中文预训练的独特挑战\"><span>中文预训练的独特挑战</span></a></h3>\n<p>许多优秀的语言模型在中文任务上的表现不佳，因为它们的预训练主要基于英文语料。为解决这一问题，研究者通常会对英文模型进行二次预训练。</p>\n<h4 id=\"⚠-常见问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠-常见问题\"><span>⚠ 常见问题：</span></a></h4>\n<ul>\n<li>数字切分错误（如“9.9 &gt; 9.11”问题）。</li>\n<li>词表中敏感或脏token未移除。</li>\n<li>业务场景相关token覆盖不足。</li>\n</ul>\n<hr>\n<h3 id=\"词表扩充实例对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#词表扩充实例对比\"><span>词表扩充实例对比</span></a></h3>\n<p>通过对比Chinese-LLaMA与原始LLaMA的Tokenizer，可以发现：</p>\n<ul>\n<li><strong>Chinese-LLaMA</strong>新增了17953个tokens，大部分为汉字。</li>\n<li><strong>BELLE模型</strong>在120万行中文文本上训练了一个规模为5万的token集合，并将其与原始LLaMA词表合并。\n<img src=\"/img/user/附件/Pasted image 20250409220047.png\" alt=\"Pasted image 20250409220047.png\"></li>\n</ul>\n<h4 id=\"📊-数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📊-数据表格示例\"><span>📊 数据表格示例：</span></a></h4>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>新增tokens数量</th>\n<th>数据规模</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Chinese-LLaMA</td>\n<td>17953</td>\n<td>未明确</td>\n</tr>\n<tr>\n<td>BELLE</td>\n<td>50000</td>\n<td>120万行文本</td>\n</tr>\n</tbody>\n</table>\n<p>💡<strong>启发点</strong>：通过扩充词表，可以有效降低模型训练难度，提升其适用于中文任务的能力。</p>\n<hr>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ol>\n<li><strong>BPE/BBPE/WordPiece算法</strong>：一种将文本切分为小单位（如词或字）的方法，用于构建Tokenizer。</li>\n<li><strong>压缩率</strong>：指一个token平均对应多少个汉字，影响解码效率和模型知识能力。</li>\n<li><strong>词表扩充</strong>：手动添加常见汉字或特定领域词汇，以优化模型性能。</li>\n</ol>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考]板块</span></a></h2>\n<ol>\n<li>如何进一步优化Tokenizer以支持多语种任务？</li>\n<li>是否可以设计一种动态调整压缩率的方法以适应不同任务场景？</li>\n<li>在中文预训练中，如何平衡通用性与领域专用性？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：本文内容基于某技术文档关于中文语言模型预训练与Tokenizer优化的部分内容整理与总结。</p>\n</blockquote>\n<hr>\n<h2 id=\"常见错误警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告区块\"><span>常见错误警告区块</span></a></h2>\n<blockquote>\n<p>⚠ <strong>注意事项</strong></p>\n<ul>\n<li>数字切分问题需特别关注，避免影响模型回答准确性。</li>\n<li>词表扩充时需确保覆盖足够的中英词汇，同时避免加入敏感或无意义的token。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>收集更大规模的中文数据集以支持Tokenizer训练。</li>\n<li>针对业务场景设计定制化词表扩充策略。</li>\n<li>测试不同压缩率对模型性能的影响，寻找最佳平衡点。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着中文语言模型需求的增加，未来可能出现：</p>\n<ul>\n<li>更高效的中文Tokenizer算法。</li>\n<li>动态调整词表大小和压缩率的技术。</li>\n<li>支持多语种任务的统一预训练框架。</li>\n</ul>\n<hr>\n<h2 id=\"后续追踪方向\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪方向\"><span>后续追踪方向</span></a></h2>\n<ol>\n<li>对比不同中文大模型在实际任务中的表现。</li>\n<li>探索如何在低资源环境下实现高质量中文预训练。</li>\n<li>调研针对小语种任务的词表扩充策略。</li>\n</ol>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：中文预训练、Tokenizer、语言模型优化</li>\n<li><strong>日期</strong>：2023-10-12</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>在构建中文语言模型时，预训练流程是关键步骤之一。其中，Tokenizer的训练与优化直接影响模型的性能与适用性。本文将重点解析如何通过词表扩充、压缩率控制等方式优化Tokenizer，并探讨中文预训练的独特挑战与解决方案。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"tokenizer的作用与训练方法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#tokenizer的作用与训练方法\"><span>Tokenizer的作用与训练方法</span></a></h3>\n<p>Tokenizer（分词器）的主要作用是将输入的句子切分为词或字，并将这些切分结果转化为模型可理解的token。这是预训练模型的第一步。</p>\n<h4 id=\"✅-tokenizer训练步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-tokenizer训练步骤\"><span>✅ Tokenizer训练步骤：</span></a></h4>\n<ol>\n<li><strong>选择算法</strong>：使用BPE（Byte Pair Encoding）、BBPE（Balanced Byte Pair Encoding）或WordPiece算法。</li>\n<li><strong>数据准备</strong>：收集通用大规模数据集和业务场景相关数据。</li>\n<li><strong>环境需求</strong>：需要内存较大的CPU机器。</li>\n<li><strong>压缩率控制</strong>：保持1个token约对应1.5个汉字以平衡解码效率与模型知识能力。</li>\n<li><strong>词表扩充</strong>：手动添加常见汉字或业务场景相关词汇。</li>\n</ol>\n<p>💡<strong>启发点</strong>：压缩率过低会导致解码效率低，而压缩率过高会影响模型知识表达能力，因此需要找到一个平衡点。</p>\n<hr>\n<h3 id=\"中文预训练的独特挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#中文预训练的独特挑战\"><span>中文预训练的独特挑战</span></a></h3>\n<p>许多优秀的语言模型在中文任务上的表现不佳，因为它们的预训练主要基于英文语料。为解决这一问题，研究者通常会对英文模型进行二次预训练。</p>\n<h4 id=\"⚠-常见问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠-常见问题\"><span>⚠ 常见问题：</span></a></h4>\n<ul>\n<li>数字切分错误（如“9.9 &gt; 9.11”问题）。</li>\n<li>词表中敏感或脏token未移除。</li>\n<li>业务场景相关token覆盖不足。</li>\n</ul>\n<hr>\n<h3 id=\"词表扩充实例对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#词表扩充实例对比\"><span>词表扩充实例对比</span></a></h3>\n<p>通过对比Chinese-LLaMA与原始LLaMA的Tokenizer，可以发现：</p>\n<ul>\n<li><strong>Chinese-LLaMA</strong>新增了17953个tokens，大部分为汉字。</li>\n<li><strong>BELLE模型</strong>在120万行中文文本上训练了一个规模为5万的token集合，并将其与原始LLaMA词表合并。\n<img src=\"/img/user/附件/Pasted image 20250409220047.png\" alt=\"Pasted image 20250409220047.png\"></li>\n</ul>\n<h4 id=\"📊-数据表格示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📊-数据表格示例\"><span>📊 数据表格示例：</span></a></h4>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>新增tokens数量</th>\n<th>数据规模</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Chinese-LLaMA</td>\n<td>17953</td>\n<td>未明确</td>\n</tr>\n<tr>\n<td>BELLE</td>\n<td>50000</td>\n<td>120万行文本</td>\n</tr>\n</tbody>\n</table>\n<p>💡<strong>启发点</strong>：通过扩充词表，可以有效降低模型训练难度，提升其适用于中文任务的能力。</p>\n<hr>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ol>\n<li><strong>BPE/BBPE/WordPiece算法</strong>：一种将文本切分为小单位（如词或字）的方法，用于构建Tokenizer。</li>\n<li><strong>压缩率</strong>：指一个token平均对应多少个汉字，影响解码效率和模型知识能力。</li>\n<li><strong>词表扩充</strong>：手动添加常见汉字或特定领域词汇，以优化模型性能。</li>\n</ol>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考]板块</span></a></h2>\n<ol>\n<li>如何进一步优化Tokenizer以支持多语种任务？</li>\n<li>是否可以设计一种动态调整压缩率的方法以适应不同任务场景？</li>\n<li>在中文预训练中，如何平衡通用性与领域专用性？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原始出处：本文内容基于某技术文档关于中文语言模型预训练与Tokenizer优化的部分内容整理与总结。</p>\n</blockquote>\n<hr>\n<h2 id=\"常见错误警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误警告区块\"><span>常见错误警告区块</span></a></h2>\n<blockquote>\n<p>⚠ <strong>注意事项</strong></p>\n<ul>\n<li>数字切分问题需特别关注，避免影响模型回答准确性。</li>\n<li>词表扩充时需确保覆盖足够的中英词汇，同时避免加入敏感或无意义的token。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>收集更大规模的中文数据集以支持Tokenizer训练。</li>\n<li>针对业务场景设计定制化词表扩充策略。</li>\n<li>测试不同压缩率对模型性能的影响，寻找最佳平衡点。</li>\n</ol>\n<hr>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>随着中文语言模型需求的增加，未来可能出现：</p>\n<ul>\n<li>更高效的中文Tokenizer算法。</li>\n<li>动态调整词表大小和压缩率的技术。</li>\n<li>支持多语种任务的统一预训练框架。</li>\n</ul>\n<hr>\n<h2 id=\"后续追踪方向\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪方向\"><span>后续追踪方向</span></a></h2>\n<ol>\n<li>对比不同中文大模型在实际任务中的表现。</li>\n<li>探索如何在低资源环境下实现高质量中文预训练。</li>\n<li>调研针对小语种任务的词表扩充策略。</li>\n</ol>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 元数据\n- **分类**：自然语言处理（NLP）\n- **标签**：中文预训练、Tokenizer、语言模型优化\n- **日期**：2023-10-12\n\n---\n\n\n\n## 核心内容总结\n在构建中文语言模型时，预训练流程是关键步骤之一。其中，Tokenizer的训练与优化直接影响模型的性能与适用性。本文将重点解析如何通过词表扩充、压缩率控制等方式优化Tokenizer，并探讨中文预训练的独特挑战与解决方案。\n\n---\n\n\n\n## 重点内容解析\n\n### Tokenizer的作用与训练方法\nTokenizer（分词器）的主要作用是将输入的句子切分为词或字，并将这些切分结果转化为模型可理解的token。这是预训练模型的第一步。\n\n#### ✅ Tokenizer训练步骤：\n1. **选择算法**：使用BPE（Byte Pair Encoding）、BBPE（Balanced Byte Pair Encoding）或WordPiece算法。\n2. **数据准备**：收集通用大规模数据集和业务场景相关数据。\n3. **环境需求**：需要内存较大的CPU机器。\n4. **压缩率控制**：保持1个token约对应1.5个汉字以平衡解码效率与模型知识能力。\n5. **词表扩充**：手动添加常见汉字或业务场景相关词汇。\n\n💡**启发点**：压缩率过低会导致解码效率低，而压缩率过高会影响模型知识表达能力，因此需要找到一个平衡点。\n\n---\n\n\n### 中文预训练的独特挑战\n许多优秀的语言模型在中文任务上的表现不佳，因为它们的预训练主要基于英文语料。为解决这一问题，研究者通常会对英文模型进行二次预训练。\n\n#### ⚠ 常见问题：\n- 数字切分错误（如“9.9 > 9.11”问题）。\n- 词表中敏感或脏token未移除。\n- 业务场景相关token覆盖不足。\n\n---\n\n\n### 词表扩充实例对比\n通过对比Chinese-LLaMA与原始LLaMA的Tokenizer，可以发现：\n- **Chinese-LLaMA**新增了17953个tokens，大部分为汉字。\n- **BELLE模型**在120万行中文文本上训练了一个规模为5万的token集合，并将其与原始LLaMA词表合并。\n![Pasted image 20250409220047.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250409220047.png)\n\n#### 📊 数据表格示例：\n| 模型名         | 新增tokens数量 | 数据规模       |\n|----------------|---------------|---------------|\n| Chinese-LLaMA | 17953         | 未明确        |\n| BELLE         | 50000         | 120万行文本   |\n\n💡**启发点**：通过扩充词表，可以有效降低模型训练难度，提升其适用于中文任务的能力。\n\n---\n\n\n\n## 技术术语通俗解释\n1. **BPE/BBPE/WordPiece算法**：一种将文本切分为小单位（如词或字）的方法，用于构建Tokenizer。\n2. **压缩率**：指一个token平均对应多少个汉字，影响解码效率和模型知识能力。\n3. **词表扩充**：手动添加常见汉字或特定领域词汇，以优化模型性能。\n\n---\n\n\n\n## [思考]板块\n1. 如何进一步优化Tokenizer以支持多语种任务？\n2. 是否可以设计一种动态调整压缩率的方法以适应不同任务场景？\n3. 在中文预训练中，如何平衡通用性与领域专用性？\n\n---\n\n> 原始出处：本文内容基于某技术文档关于中文语言模型预训练与Tokenizer优化的部分内容整理与总结。\n\n---\n\n\n\n## 常见错误警告区块\n> ⚠ **注意事项**\n> - 数字切分问题需特别关注，避免影响模型回答准确性。\n> - 词表扩充时需确保覆盖足够的中英词汇，同时避免加入敏感或无意义的token。\n\n---\n\n\n\n## 行动清单\n1. 收集更大规模的中文数据集以支持Tokenizer训练。\n2. 针对业务场景设计定制化词表扩充策略。\n3. 测试不同压缩率对模型性能的影响，寻找最佳平衡点。\n\n---\n\n\n\n## 📈趋势预测\n随着中文语言模型需求的增加，未来可能出现：\n- 更高效的中文Tokenizer算法。\n- 动态调整词表大小和压缩率的技术。\n- 支持多语种任务的统一预训练框架。\n\n---\n\n\n\n## 后续追踪方向\n1. 对比不同中文大模型在实际任务中的表现。\n2. 探索如何在低资源环境下实现高质量中文预训练。\n3. 调研针对小语种任务的词表扩充策略。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心内容总结","slug":"核心内容总结","link":"#核心内容总结","children":[]},{"level":2,"title":"重点内容解析","slug":"重点内容解析","link":"#重点内容解析","children":[{"level":3,"title":"Tokenizer的作用与训练方法","slug":"tokenizer的作用与训练方法","link":"#tokenizer的作用与训练方法","children":[]},{"level":3,"title":"中文预训练的独特挑战","slug":"中文预训练的独特挑战","link":"#中文预训练的独特挑战","children":[]},{"level":3,"title":"词表扩充实例对比","slug":"词表扩充实例对比","link":"#词表扩充实例对比","children":[]}]},{"level":2,"title":"技术术语通俗解释","slug":"技术术语通俗解释","link":"#技术术语通俗解释","children":[]},{"level":2,"title":"[思考]板块","slug":"思考-板块","link":"#思考-板块","children":[]},{"level":2,"title":"常见错误警告区块","slug":"常见错误警告区块","link":"#常见错误警告区块","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪方向","slug":"后续追踪方向","link":"#后续追踪方向","children":[]}]}}
