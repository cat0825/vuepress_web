{"content":"<p><strong>分类</strong>：人工智能<br>\n<strong>标签</strong>：大型语言模型, BERT, GPT, 深度学习<br>\n<strong>日期</strong>：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>大型语言模型（LLM）的发展迅速，众多模型如BERT、GPT、Llama等在学术界和工业界中广泛应用。这些模型不仅推动了自然语言处理技术的进步，也成为面试中的常见考点。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424113136.png\" alt=\"Pasted image 20250424113136.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>BERT及其变体</strong>：<br>\nBERT（Bidirectional Encoder Representations from Transformers）是Google在2018年提出的预训练语言模型，通过双向编码器架构实现上下文信息的捕捉。其变体包括RoBERTa、DistilBERT等，进一步优化了训练效率和性能。</p>\n</li>\n<li>\n<p><strong>GPT系列</strong>：<br>\nGPT（Generative Pre-trained Transformer）由OpenAI开发，以生成式任务为主。GPT系列模型通过无监督学习进行预训练，并在特定任务中进行微调，展现了强大的文本生成能力。</p>\n</li>\n<li>\n<p><strong>Llama系列</strong>：<br>\nLlama系列是另一类重要的语言模型，以其高效的参数使用和出色的性能而闻名。它们在处理多任务和大规模数据集时表现出色。</p>\n</li>\n</ol>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 选择适合的语言模型（如BERT、GPT）进行初步研究。</li>\n<li>⚠ 分析模型的优缺点及适用场景。</li>\n<li>❗ 在实际项目中应用并调整模型参数以优化性能。</li>\n</ol>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究最新的LLM论文，保持技术前沿。</li>\n<li>在项目中实施并测试不同的模型架构。</li>\n<li>关注LLM在不同领域的应用案例，扩展知识面。</li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于“Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges”论文。链接：<a href=\"https://arxiv.org/pdf/2412.03220\" target=\"_blank\" rel=\"noopener noreferrer\">arxiv.org/pdf/2412.03220</a></p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/发展历史.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/发展历史.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/发展历史","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/发展历史/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T03:29:15.000Z","updated":"2025-04-24T03:38:55.000Z","title":"发展历史","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><p><strong>分类</strong>：人工智能<br>\n<strong>标签</strong>：大型语言模型, BERT, GPT, 深度学习<br>\n<strong>日期</strong>：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>大型语言模型（LLM）的发展迅速，众多模型如BERT、GPT、Llama等在学术界和工业界中广泛应用。这些模型不仅推动了自然语言处理技术的进步，也成为面试中的常见考点。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424113136.png\" alt=\"Pasted image 20250424113136.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>BERT及其变体</strong>：<br>\nBERT（Bidirectional Encoder Representations from Transformers）是Google在2018年提出的预训练语言模型，通过双向编码器架构实现上下文信息的捕捉。其变体包括RoBERTa、DistilBERT等，进一步优化了训练效率和性能。</p>\n</li>\n<li>\n<p><strong>GPT系列</strong>：<br>\nGPT（Generative Pre-trained Transformer）由OpenAI开发，以生成式任务为主。GPT系列模型通过无监督学习进行预训练，并在特定任务中进行微调，展现了强大的文本生成能力。</p>\n</li>\n<li>\n<p><strong>Llama系列</strong>：<br>\nLlama系列是另一类重要的语言模型，以其高效的参数使用和出色的性能而闻名。它们在处理多任务和大规模数据集时表现出色。</p>\n</li>\n</ol>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 选择适合的语言模型（如BERT、GPT）进行初步研究。</li>\n<li>⚠ 分析模型的优缺点及适用场景。</li>\n<li>❗ 在实际项目中应用并调整模型参数以优化性能。</li>\n</ol>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究最新的LLM论文，保持技术前沿。</li>\n<li>在项目中实施并测试不同的模型架构。</li>\n<li>关注LLM在不同领域的应用案例，扩展知识面。</li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于“Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges”论文。链接：<a href=\"https://arxiv.org/pdf/2412.03220\" target=\"_blank\" rel=\"noopener noreferrer\">arxiv.org/pdf/2412.03220</a></p>\n</blockquote>\n</template>","contentStripped":"<p><strong>分类</strong>：人工智能<br>\n<strong>标签</strong>：大型语言模型, BERT, GPT, 深度学习<br>\n<strong>日期</strong>：2025年4月12日</p>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>大型语言模型（LLM）的发展迅速，众多模型如BERT、GPT、Llama等在学术界和工业界中广泛应用。这些模型不仅推动了自然语言处理技术的进步，也成为面试中的常见考点。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424113136.png\" alt=\"Pasted image 20250424113136.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<ol>\n<li>\n<p><strong>BERT及其变体</strong>：<br>\nBERT（Bidirectional Encoder Representations from Transformers）是Google在2018年提出的预训练语言模型，通过双向编码器架构实现上下文信息的捕捉。其变体包括RoBERTa、DistilBERT等，进一步优化了训练效率和性能。</p>\n</li>\n<li>\n<p><strong>GPT系列</strong>：<br>\nGPT（Generative Pre-trained Transformer）由OpenAI开发，以生成式任务为主。GPT系列模型通过无监督学习进行预训练，并在特定任务中进行微调，展现了强大的文本生成能力。</p>\n</li>\n<li>\n<p><strong>Llama系列</strong>：<br>\nLlama系列是另一类重要的语言模型，以其高效的参数使用和出色的性能而闻名。它们在处理多任务和大规模数据集时表现出色。</p>\n</li>\n</ol>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 选择适合的语言模型（如BERT、GPT）进行初步研究。</li>\n<li>⚠ 分析模型的优缺点及适用场景。</li>\n<li>❗ 在实际项目中应用并调整模型参数以优化性能。</li>\n</ol>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究最新的LLM论文，保持技术前沿。</li>\n<li>在项目中实施并测试不同的模型架构。</li>\n<li>关注LLM在不同领域的应用案例，扩展知识面。</li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于“Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges”论文。链接：<a href=\"https://arxiv.org/pdf/2412.03220\" target=\"_blank\" rel=\"noopener noreferrer\">arxiv.org/pdf/2412.03220</a></p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"**分类**：人工智能  \n**标签**：大型语言模型, BERT, GPT, 深度学习  \n**日期**：2025年4月12日\n\n## 核心观点总结\n大型语言模型（LLM）的发展迅速，众多模型如BERT、GPT、Llama等在学术界和工业界中广泛应用。这些模型不仅推动了自然语言处理技术的进步，也成为面试中的常见考点。\n\n![Pasted image 20250424113136.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424113136.png)\n\n\n## 重点段落\n1. **BERT及其变体**：  \n   BERT（Bidirectional Encoder Representations from Transformers）是Google在2018年提出的预训练语言模型，通过双向编码器架构实现上下文信息的捕捉。其变体包括RoBERTa、DistilBERT等，进一步优化了训练效率和性能。\n\n2. **GPT系列**：  \n   GPT（Generative Pre-trained Transformer）由OpenAI开发，以生成式任务为主。GPT系列模型通过无监督学习进行预训练，并在特定任务中进行微调，展现了强大的文本生成能力。\n\n3. **Llama系列**：  \n   Llama系列是另一类重要的语言模型，以其高效的参数使用和出色的性能而闻名。它们在处理多任务和大规模数据集时表现出色。\n\n\n## 操作步骤\n1. ✅ 选择适合的语言模型（如BERT、GPT）进行初步研究。\n2. ⚠ 分析模型的优缺点及适用场景。\n3. ❗ 在实际项目中应用并调整模型参数以优化性能。\n\n\n## 行动清单\n- 研究最新的LLM论文，保持技术前沿。\n- 在项目中实施并测试不同的模型架构。\n- 关注LLM在不同领域的应用案例，扩展知识面。\n\n> 来源：本文内容基于“Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges”论文。链接：[arxiv.org/pdf/2412.03220](https://arxiv.org/pdf/2412.03220)","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
