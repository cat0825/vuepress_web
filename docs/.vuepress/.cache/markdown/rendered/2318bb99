{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：X-Lora, 预训练模型, 动态缩放, 低秩适应</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>X-Lora通过结合多个不同领域的预训练的Lora模型，并通过一个可训练的缩放头来动态调整每个Lora模型的贡献。这种方法允许在不同任务中灵活应用多个低秩适应技术，以提高模型的效率和性能。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424111942.png\" alt=\"Pasted image 20250424111942.png\"></p>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"动态缩放机制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#动态缩放机制\"><span>动态缩放机制</span></a></h3>\n<p>X-Lora使用一个动态缩放机制来调整每个Lora模型在最终输出中的贡献。这个过程包括：</p>\n<ol>\n<li><strong>获取缩放值</strong>：从给定的缩放矩阵中提取特定层的缩放值。</li>\n<li><strong>选择Top-K缩放</strong>：根据配置，选择贡献最大的Top-K个缩放值。</li>\n<li><strong>应用Softmax</strong>：如果启用，使用Softmax函数对非零缩放值进行归一化处理。</li>\n</ol>\n<h3 id=\"xloralinearlayer的前向传播\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#xloralinearlayer的前向传播\"><span>XLoraLinearLayer的前向传播</span></a></h3>\n<p>在XLoraLinearLayer类中，前向传播过程如下：</p>\n<ol>\n<li>检查目标是否已合并。</li>\n<li>遍历每个活跃的适配器，获取相应的Lora权重和缩放参数。</li>\n<li>根据需要应用缩放调整。</li>\n<li>计算结果并返回。</li>\n</ol>\n<h3 id=\"代码示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码示例\"><span>代码示例</span></a></h3>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> get_maybe_topk_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> -></span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">Tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    xlora_scalings </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[:,</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> :,</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">layer_number</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> :]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    if</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">top_k_lora </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">is</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> not</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> None</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        _</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> topk_indices </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">topk</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> k</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">top_k_lora</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dim</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">zeros_like</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dtype</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">bool</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">scatter_</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> topk_indices</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> True</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        xlora_scalings </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">*</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">to</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">dtype</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    if</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">enable_softmax_topk</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        nonzero_mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">!=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 0</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        softmax_res_nonzero </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">softmax</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">nonzero_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">],</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dim</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">nonzero_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> softmax_res_nonzero</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    return</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h3>\n<p>这种方法不仅提高了模型在特定任务上的适应能力，还可以在不显著增加计算成本的情况下，利用多个领域的知识。</p>\n<h2 id=\"警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#警告区块\"><span>警告区块</span></a></h2>\n<p>⚠️ 常见错误：确保在应用缩放时，非零掩码和Softmax函数正确配置，否则可能导致缩放不准确。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更多的低秩适应技术与X-Lora结合的可能性。</li>\n<li>评估X-Lora在不同领域任务中的性能表现。</li>\n<li>优化动态缩放头的训练策略以提高模型效率。</li>\n</ul>\n<blockquote>\n<p>原始出处：<a href=\"https://example.com\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA: Low-Rank Adaptation of Large Language Models</a></p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/X-LoRA.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/X-LoRA.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/X-LoRA","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/X-LoRA/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T03:19:18.000Z","updated":"2025-04-24T03:21:31.000Z","title":"X-LoRA","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：X-Lora, 预训练模型, 动态缩放, 低秩适应</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>X-Lora通过结合多个不同领域的预训练的Lora模型，并通过一个可训练的缩放头来动态调整每个Lora模型的贡献。这种方法允许在不同任务中灵活应用多个低秩适应技术，以提高模型的效率和性能。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424111942.png\" alt=\"Pasted image 20250424111942.png\"></p>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"动态缩放机制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#动态缩放机制\"><span>动态缩放机制</span></a></h3>\n<p>X-Lora使用一个动态缩放机制来调整每个Lora模型在最终输出中的贡献。这个过程包括：</p>\n<ol>\n<li><strong>获取缩放值</strong>：从给定的缩放矩阵中提取特定层的缩放值。</li>\n<li><strong>选择Top-K缩放</strong>：根据配置，选择贡献最大的Top-K个缩放值。</li>\n<li><strong>应用Softmax</strong>：如果启用，使用Softmax函数对非零缩放值进行归一化处理。</li>\n</ol>\n<h3 id=\"xloralinearlayer的前向传播\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#xloralinearlayer的前向传播\"><span>XLoraLinearLayer的前向传播</span></a></h3>\n<p>在XLoraLinearLayer类中，前向传播过程如下：</p>\n<ol>\n<li>检查目标是否已合并。</li>\n<li>遍历每个活跃的适配器，获取相应的Lora权重和缩放参数。</li>\n<li>根据需要应用缩放调整。</li>\n<li>计算结果并返回。</li>\n</ol>\n<h3 id=\"代码示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码示例\"><span>代码示例</span></a></h3>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> get_maybe_topk_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> -></span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">Tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    xlora_scalings </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[:,</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> :,</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">layer_number</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> :]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    if</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">top_k_lora </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">is</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> not</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> None</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        _</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> topk_indices </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">topk</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> k</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">top_k_lora</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dim</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">zeros_like</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dtype</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">bool</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">scatter_</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> topk_indices</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> True</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        xlora_scalings </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">*</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">to</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">dtype</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    if</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">enable_softmax_topk</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        nonzero_mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">!=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 0</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        softmax_res_nonzero </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">softmax</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">nonzero_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">],</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dim</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">nonzero_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> softmax_res_nonzero</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    return</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h3>\n<p>这种方法不仅提高了模型在特定任务上的适应能力，还可以在不显著增加计算成本的情况下，利用多个领域的知识。</p>\n<h2 id=\"警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#警告区块\"><span>警告区块</span></a></h2>\n<p>⚠️ 常见错误：确保在应用缩放时，非零掩码和Softmax函数正确配置，否则可能导致缩放不准确。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更多的低秩适应技术与X-Lora结合的可能性。</li>\n<li>评估X-Lora在不同领域任务中的性能表现。</li>\n<li>优化动态缩放头的训练策略以提高模型效率。</li>\n</ul>\n<blockquote>\n<p>原始出处：<a href=\"https://example.com\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA: Low-Rank Adaptation of Large Language Models</a></p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：X-Lora, 预训练模型, 动态缩放, 低秩适应</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>X-Lora通过结合多个不同领域的预训练的Lora模型，并通过一个可训练的缩放头来动态调整每个Lora模型的贡献。这种方法允许在不同任务中灵活应用多个低秩适应技术，以提高模型的效率和性能。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424111942.png\" alt=\"Pasted image 20250424111942.png\"></p>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"动态缩放机制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#动态缩放机制\"><span>动态缩放机制</span></a></h3>\n<p>X-Lora使用一个动态缩放机制来调整每个Lora模型在最终输出中的贡献。这个过程包括：</p>\n<ol>\n<li><strong>获取缩放值</strong>：从给定的缩放矩阵中提取特定层的缩放值。</li>\n<li><strong>选择Top-K缩放</strong>：根据配置，选择贡献最大的Top-K个缩放值。</li>\n<li><strong>应用Softmax</strong>：如果启用，使用Softmax函数对非零缩放值进行归一化处理。</li>\n</ol>\n<h3 id=\"xloralinearlayer的前向传播\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#xloralinearlayer的前向传播\"><span>XLoraLinearLayer的前向传播</span></a></h3>\n<p>在XLoraLinearLayer类中，前向传播过程如下：</p>\n<ol>\n<li>检查目标是否已合并。</li>\n<li>遍历每个活跃的适配器，获取相应的Lora权重和缩放参数。</li>\n<li>根据需要应用缩放调整。</li>\n<li>计算结果并返回。</li>\n</ol>\n<h3 id=\"代码示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#代码示例\"><span>代码示例</span></a></h3>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> get_maybe_topk_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> -></span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">Tensor</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    xlora_scalings </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[:,</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> :,</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">layer_number</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> :]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    if</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">top_k_lora </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">is</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> not</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> None</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        _</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> topk_indices </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">topk</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> k</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">top_k_lora</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dim</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">zeros_like</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dtype</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">bool</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">scatter_</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> topk_indices</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\"> True</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        xlora_scalings </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">*</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">to</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">dtype</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    if</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">config</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">enable_softmax_topk</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        nonzero_mask </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings </span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">!=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 0</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        softmax_res_nonzero </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> torch</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">softmax</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">nonzero_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">],</span><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\"> dim</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">-</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">1</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">        xlora_scalings</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">nonzero_mask</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> =</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> softmax_res_nonzero</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">    return</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> xlora_scalings</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"💡-启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-启发点\"><span>💡 启发点</span></a></h3>\n<p>这种方法不仅提高了模型在特定任务上的适应能力，还可以在不显著增加计算成本的情况下，利用多个领域的知识。</p>\n<h2 id=\"警告区块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#警告区块\"><span>警告区块</span></a></h2>\n<p>⚠️ 常见错误：确保在应用缩放时，非零掩码和Softmax函数正确配置，否则可能导致缩放不准确。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>探索更多的低秩适应技术与X-Lora结合的可能性。</li>\n<li>评估X-Lora在不同领域任务中的性能表现。</li>\n<li>优化动态缩放头的训练策略以提高模型效率。</li>\n</ul>\n<blockquote>\n<p>原始出处：<a href=\"https://example.com\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA: Low-Rank Adaptation of Large Language Models</a></p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：机器学习\n- 标签：X-Lora, 预训练模型, 动态缩放, 低秩适应\n- 日期：2025年4月12日\n\n\n## 核心观点\nX-Lora通过结合多个不同领域的预训练的Lora模型，并通过一个可训练的缩放头来动态调整每个Lora模型的贡献。这种方法允许在不同任务中灵活应用多个低秩适应技术，以提高模型的效率和性能。\n\n![Pasted image 20250424111942.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424111942.png)\n\n\n## 重点内容\n\n### 动态缩放机制\nX-Lora使用一个动态缩放机制来调整每个Lora模型在最终输出中的贡献。这个过程包括：\n\n1. **获取缩放值**：从给定的缩放矩阵中提取特定层的缩放值。\n2. **选择Top-K缩放**：根据配置，选择贡献最大的Top-K个缩放值。\n3. **应用Softmax**：如果启用，使用Softmax函数对非零缩放值进行归一化处理。\n\n\n### XLoraLinearLayer的前向传播\n在XLoraLinearLayer类中，前向传播过程如下：\n\n1. 检查目标是否已合并。\n2. 遍历每个活跃的适配器，获取相应的Lora权重和缩放参数。\n3. 根据需要应用缩放调整。\n4. 计算结果并返回。\n\n\n### 代码示例\n```python\ndef get_maybe_topk_scalings(self, scalings) -> torch.Tensor:\n    xlora_scalings = scalings[:, :, self.layer_number, :]\n    if self.config.top_k_lora is not None:\n        _, topk_indices = torch.topk(xlora_scalings, k=self.config.top_k_lora, dim=-1)\n        mask = torch.zeros_like(xlora_scalings, dtype=torch.bool)\n        mask.scatter_(-1, topk_indices, True)\n        xlora_scalings = xlora_scalings * mask.to(xlora_scalings.dtype)\n\n    if self.config.enable_softmax_topk:\n        nonzero_mask = xlora_scalings != 0\n        softmax_res_nonzero = torch.softmax(xlora_scalings[nonzero_mask], dim=-1)\n        xlora_scalings[nonzero_mask] = softmax_res_nonzero\n\n    return xlora_scalings\n```\n\n\n### 💡 启发点\n这种方法不仅提高了模型在特定任务上的适应能力，还可以在不显著增加计算成本的情况下，利用多个领域的知识。\n\n\n## 警告区块\n⚠️ 常见错误：确保在应用缩放时，非零掩码和Softmax函数正确配置，否则可能导致缩放不准确。\n\n\n## 行动清单\n- 探索更多的低秩适应技术与X-Lora结合的可能性。\n- 评估X-Lora在不同领域任务中的性能表现。\n- 优化动态缩放头的训练策略以提高模型效率。\n\n> 原始出处：[LoRA: Low-Rank Adaptation of Large Language Models](https://example.com)","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心观点","slug":"核心观点","link":"#核心观点","children":[]},{"level":2,"title":"重点内容","slug":"重点内容","link":"#重点内容","children":[{"level":3,"title":"动态缩放机制","slug":"动态缩放机制","link":"#动态缩放机制","children":[]},{"level":3,"title":"XLoraLinearLayer的前向传播","slug":"xloralinearlayer的前向传播","link":"#xloralinearlayer的前向传播","children":[]},{"level":3,"title":"代码示例","slug":"代码示例","link":"#代码示例","children":[]},{"level":3,"title":"💡 启发点","slug":"💡-启发点","link":"#💡-启发点","children":[]}]},{"level":2,"title":"警告区块","slug":"警告区块","link":"#警告区块","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
