{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理 (NLP)</li>\n<li><strong>标签</strong>：分词方法、WordPiece、BPE、ULM</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"_1️⃣-核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1️⃣-核心观点总结\"><span>1️⃣ 核心观点总结</span></a></h2>\n<p>分词是自然语言处理中的基础步骤，不同的分词方法会显著影响模型性能。本文对比了三种常见的分词方法：WordPiece、BPE（Byte Pair Encoding）和ULM（Unigram Language Model），并分析了它们在词表生成策略和合并规则上的差异。</p>\n<hr>\n<h2 id=\"_2️⃣-重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2️⃣-重点内容解析\"><span>2️⃣ 重点内容解析</span></a></h2>\n<h3 id=\"💡-wordpiece与bpe的对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-wordpiece与bpe的对比\"><span>💡 <strong>WordPiece与BPE的对比</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>共同点</strong>：<br>\n两者都基于“合并”的思想，先将语料拆分为最小单位（如英文中的26个字母和符号），再逐步合并，生成从小到大的词表。</p>\n</li>\n<li>\n<p><strong>区别</strong>：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>合并依据</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>WordPiece</strong></td>\n<td>基于词与词之间的互信息（MI）</td>\n</tr>\n<tr>\n<td><strong>BPE</strong></td>\n<td>基于词的共现频率</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：随着更多上下文感知模型的引入，基于互信息的WordPiece可能更受欢迎。</p>\n<hr>\n<h3 id=\"💡-wordpiece与ulm的对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-wordpiece与ulm的对比\"><span>💡 <strong>WordPiece与ULM的对比</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>共同点</strong>：<br>\n两者都使用语言模型来选择子词，基于概率评估分词效果。</p>\n</li>\n<li>\n<p><strong>区别</strong>：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>词表构建策略</th>\n<th>输出结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>WordPiece</strong></td>\n<td>从小到大逐步合并</td>\n<td>单一分词方案</td>\n</tr>\n<tr>\n<td><strong>ULM</strong></td>\n<td>从大到小逐步删除</td>\n<td>多个带概率的分词结果</td>\n</tr>\n</tbody>\n</table>\n<p>✅ <strong>启发点</strong>：ULM通过保留多个分词可能性，为下游任务提供更多灵活性。</p>\n<hr>\n<h3 id=\"⚠️-常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-常见错误\"><span>⚠️ <strong>常见错误</strong></span></a></h3>\n<ol>\n<li>将BPE误认为是基于互信息的方法，而实际上它是基于共现频率。</li>\n<li>忽略ULM输出的多样性，错误地将其与其他单一分词方法混为一谈。</li>\n</ol>\n<hr>\n<h2 id=\"_3️⃣-技术术语通俗解读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3️⃣-技术术语通俗解读\"><span>3️⃣ 技术术语通俗解读</span></a></h2>\n<ul>\n<li><strong>互信息（Mutual Information, MI）</strong>：用来衡量两个词同时出现时的信息增益，类似于“关联强度”。</li>\n<li><strong>共现频率</strong>：统计两个词在语料中一起出现的次数。</li>\n<li><strong>语言模型（Language Model）</strong>：预测句子中某个词出现的概率模型。</li>\n</ul>\n<hr>\n<h2 id=\"_4️⃣-行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4️⃣-行动清单\"><span>4️⃣ 行动清单</span></a></h2>\n<ol>\n<li>✅ 探索实际项目中不同分词方法对模型性能的影响。</li>\n<li>✅ 实现一个简单的BPE算法，理解其合并过程。</li>\n<li>✅ 深入研究ULM对多样性分词输出的具体应用场景。</li>\n</ol>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考] 板块</span></a></h2>\n<ol>\n<li>如何在实际应用中选择适合的分词方法？是否需要根据任务动态调整？</li>\n<li>ULM输出多个分词结果是否会增加模型复杂度？如何权衡？</li>\n<li>是否可以结合WordPiece和ULM的方法，既保留互信息的优势，又实现多样性输出？</li>\n</ol>\n<hr>\n<blockquote>\n<p>来源：原文内容整理自自然语言处理领域基础知识。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/分词/分词算法的比较.md","filePathRelative":"notes_bak/大语言模型学习/分词/分词算法的比较.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/分词/分词算法的比较","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/分词/分词算法的比较/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-03-27T04:53:34.473Z","updated":"2025-04-13T09:57:13.445Z","title":"分词算法的比较","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理 (NLP)</li>\n<li><strong>标签</strong>：分词方法、WordPiece、BPE、ULM</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"_1️⃣-核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1️⃣-核心观点总结\"><span>1️⃣ 核心观点总结</span></a></h2>\n<p>分词是自然语言处理中的基础步骤，不同的分词方法会显著影响模型性能。本文对比了三种常见的分词方法：WordPiece、BPE（Byte Pair Encoding）和ULM（Unigram Language Model），并分析了它们在词表生成策略和合并规则上的差异。</p>\n<hr>\n<h2 id=\"_2️⃣-重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2️⃣-重点内容解析\"><span>2️⃣ 重点内容解析</span></a></h2>\n<h3 id=\"💡-wordpiece与bpe的对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-wordpiece与bpe的对比\"><span>💡 <strong>WordPiece与BPE的对比</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>共同点</strong>：<br>\n两者都基于“合并”的思想，先将语料拆分为最小单位（如英文中的26个字母和符号），再逐步合并，生成从小到大的词表。</p>\n</li>\n<li>\n<p><strong>区别</strong>：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>合并依据</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>WordPiece</strong></td>\n<td>基于词与词之间的互信息（MI）</td>\n</tr>\n<tr>\n<td><strong>BPE</strong></td>\n<td>基于词的共现频率</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：随着更多上下文感知模型的引入，基于互信息的WordPiece可能更受欢迎。</p>\n<hr>\n<h3 id=\"💡-wordpiece与ulm的对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-wordpiece与ulm的对比\"><span>💡 <strong>WordPiece与ULM的对比</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>共同点</strong>：<br>\n两者都使用语言模型来选择子词，基于概率评估分词效果。</p>\n</li>\n<li>\n<p><strong>区别</strong>：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>词表构建策略</th>\n<th>输出结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>WordPiece</strong></td>\n<td>从小到大逐步合并</td>\n<td>单一分词方案</td>\n</tr>\n<tr>\n<td><strong>ULM</strong></td>\n<td>从大到小逐步删除</td>\n<td>多个带概率的分词结果</td>\n</tr>\n</tbody>\n</table>\n<p>✅ <strong>启发点</strong>：ULM通过保留多个分词可能性，为下游任务提供更多灵活性。</p>\n<hr>\n<h3 id=\"⚠️-常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-常见错误\"><span>⚠️ <strong>常见错误</strong></span></a></h3>\n<ol>\n<li>将BPE误认为是基于互信息的方法，而实际上它是基于共现频率。</li>\n<li>忽略ULM输出的多样性，错误地将其与其他单一分词方法混为一谈。</li>\n</ol>\n<hr>\n<h2 id=\"_3️⃣-技术术语通俗解读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3️⃣-技术术语通俗解读\"><span>3️⃣ 技术术语通俗解读</span></a></h2>\n<ul>\n<li><strong>互信息（Mutual Information, MI）</strong>：用来衡量两个词同时出现时的信息增益，类似于“关联强度”。</li>\n<li><strong>共现频率</strong>：统计两个词在语料中一起出现的次数。</li>\n<li><strong>语言模型（Language Model）</strong>：预测句子中某个词出现的概率模型。</li>\n</ul>\n<hr>\n<h2 id=\"_4️⃣-行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4️⃣-行动清单\"><span>4️⃣ 行动清单</span></a></h2>\n<ol>\n<li>✅ 探索实际项目中不同分词方法对模型性能的影响。</li>\n<li>✅ 实现一个简单的BPE算法，理解其合并过程。</li>\n<li>✅ 深入研究ULM对多样性分词输出的具体应用场景。</li>\n</ol>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考] 板块</span></a></h2>\n<ol>\n<li>如何在实际应用中选择适合的分词方法？是否需要根据任务动态调整？</li>\n<li>ULM输出多个分词结果是否会增加模型复杂度？如何权衡？</li>\n<li>是否可以结合WordPiece和ULM的方法，既保留互信息的优势，又实现多样性输出？</li>\n</ol>\n<hr>\n<blockquote>\n<p>来源：原文内容整理自自然语言处理领域基础知识。</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理 (NLP)</li>\n<li><strong>标签</strong>：分词方法、WordPiece、BPE、ULM</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"_1️⃣-核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1️⃣-核心观点总结\"><span>1️⃣ 核心观点总结</span></a></h2>\n<p>分词是自然语言处理中的基础步骤，不同的分词方法会显著影响模型性能。本文对比了三种常见的分词方法：WordPiece、BPE（Byte Pair Encoding）和ULM（Unigram Language Model），并分析了它们在词表生成策略和合并规则上的差异。</p>\n<hr>\n<h2 id=\"_2️⃣-重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2️⃣-重点内容解析\"><span>2️⃣ 重点内容解析</span></a></h2>\n<h3 id=\"💡-wordpiece与bpe的对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-wordpiece与bpe的对比\"><span>💡 <strong>WordPiece与BPE的对比</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>共同点</strong>：<br>\n两者都基于“合并”的思想，先将语料拆分为最小单位（如英文中的26个字母和符号），再逐步合并，生成从小到大的词表。</p>\n</li>\n<li>\n<p><strong>区别</strong>：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>合并依据</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>WordPiece</strong></td>\n<td>基于词与词之间的互信息（MI）</td>\n</tr>\n<tr>\n<td><strong>BPE</strong></td>\n<td>基于词的共现频率</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：随着更多上下文感知模型的引入，基于互信息的WordPiece可能更受欢迎。</p>\n<hr>\n<h3 id=\"💡-wordpiece与ulm的对比\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡-wordpiece与ulm的对比\"><span>💡 <strong>WordPiece与ULM的对比</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>共同点</strong>：<br>\n两者都使用语言模型来选择子词，基于概率评估分词效果。</p>\n</li>\n<li>\n<p><strong>区别</strong>：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>词表构建策略</th>\n<th>输出结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>WordPiece</strong></td>\n<td>从小到大逐步合并</td>\n<td>单一分词方案</td>\n</tr>\n<tr>\n<td><strong>ULM</strong></td>\n<td>从大到小逐步删除</td>\n<td>多个带概率的分词结果</td>\n</tr>\n</tbody>\n</table>\n<p>✅ <strong>启发点</strong>：ULM通过保留多个分词可能性，为下游任务提供更多灵活性。</p>\n<hr>\n<h3 id=\"⚠️-常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-常见错误\"><span>⚠️ <strong>常见错误</strong></span></a></h3>\n<ol>\n<li>将BPE误认为是基于互信息的方法，而实际上它是基于共现频率。</li>\n<li>忽略ULM输出的多样性，错误地将其与其他单一分词方法混为一谈。</li>\n</ol>\n<hr>\n<h2 id=\"_3️⃣-技术术语通俗解读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3️⃣-技术术语通俗解读\"><span>3️⃣ 技术术语通俗解读</span></a></h2>\n<ul>\n<li><strong>互信息（Mutual Information, MI）</strong>：用来衡量两个词同时出现时的信息增益，类似于“关联强度”。</li>\n<li><strong>共现频率</strong>：统计两个词在语料中一起出现的次数。</li>\n<li><strong>语言模型（Language Model）</strong>：预测句子中某个词出现的概率模型。</li>\n</ul>\n<hr>\n<h2 id=\"_4️⃣-行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4️⃣-行动清单\"><span>4️⃣ 行动清单</span></a></h2>\n<ol>\n<li>✅ 探索实际项目中不同分词方法对模型性能的影响。</li>\n<li>✅ 实现一个简单的BPE算法，理解其合并过程。</li>\n<li>✅ 深入研究ULM对多样性分词输出的具体应用场景。</li>\n</ol>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考] 板块</span></a></h2>\n<ol>\n<li>如何在实际应用中选择适合的分词方法？是否需要根据任务动态调整？</li>\n<li>ULM输出多个分词结果是否会增加模型复杂度？如何权衡？</li>\n<li>是否可以结合WordPiece和ULM的方法，既保留互信息的优势，又实现多样性输出？</li>\n</ol>\n<hr>\n<blockquote>\n<p>来源：原文内容整理自自然语言处理领域基础知识。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- **分类**：自然语言处理 (NLP)\n- **标签**：分词方法、WordPiece、BPE、ULM\n- **日期**：2025年4月2日  \n\n---\n\n\n## 1️⃣ 核心观点总结\n分词是自然语言处理中的基础步骤，不同的分词方法会显著影响模型性能。本文对比了三种常见的分词方法：WordPiece、BPE（Byte Pair Encoding）和ULM（Unigram Language Model），并分析了它们在词表生成策略和合并规则上的差异。\n\n---\n\n\n## 2️⃣ 重点内容解析\n\n### 💡 **WordPiece与BPE的对比**\n- **共同点**：  \n  两者都基于“合并”的思想，先将语料拆分为最小单位（如英文中的26个字母和符号），再逐步合并，生成从小到大的词表。\n  \n- **区别**：\n\n| 方法            | 合并依据            |\n| ------------- | --------------- |\n| **WordPiece** | 基于词与词之间的互信息（MI） |\n| **BPE**       | 基于词的共现频率        |\n|               |                 |\n\n\n  📈 **趋势预测**：随着更多上下文感知模型的引入，基于互信息的WordPiece可能更受欢迎。\n\n---\n\n\n### 💡 **WordPiece与ULM的对比**\n- **共同点**：  \n  两者都使用语言模型来选择子词，基于概率评估分词效果。\n  \n- **区别**：\n \n\n| 方法            | 词表构建策略   | 输出结果       |\n| ------------- | -------- | ---------- |\n| **WordPiece** | 从小到大逐步合并 | 单一分词方案     |\n| **ULM**       | 从大到小逐步删除 | 多个带概率的分词结果 |\n\n  ✅ **启发点**：ULM通过保留多个分词可能性，为下游任务提供更多灵活性。\n\n---\n\n\n### ⚠️ **常见错误**\n1. 将BPE误认为是基于互信息的方法，而实际上它是基于共现频率。\n2. 忽略ULM输出的多样性，错误地将其与其他单一分词方法混为一谈。\n\n---\n\n\n## 3️⃣ 技术术语通俗解读\n- **互信息（Mutual Information, MI）**：用来衡量两个词同时出现时的信息增益，类似于“关联强度”。\n- **共现频率**：统计两个词在语料中一起出现的次数。\n- **语言模型（Language Model）**：预测句子中某个词出现的概率模型。\n\n---\n\n\n## 4️⃣ 行动清单\n1. ✅ 探索实际项目中不同分词方法对模型性能的影响。\n2. ✅ 实现一个简单的BPE算法，理解其合并过程。\n3. ✅ 深入研究ULM对多样性分词输出的具体应用场景。\n\n---\n\n\n## [思考] 板块\n1. 如何在实际应用中选择适合的分词方法？是否需要根据任务动态调整？\n2. ULM输出多个分词结果是否会增加模型复杂度？如何权衡？\n3. 是否可以结合WordPiece和ULM的方法，既保留互信息的优势，又实现多样性输出？\n\n---\n\n> 来源：原文内容整理自自然语言处理领域基础知识。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"1️⃣ 核心观点总结","slug":"_1️⃣-核心观点总结","link":"#_1️⃣-核心观点总结","children":[]},{"level":2,"title":"2️⃣ 重点内容解析","slug":"_2️⃣-重点内容解析","link":"#_2️⃣-重点内容解析","children":[{"level":3,"title":"💡 WordPiece与BPE的对比","slug":"💡-wordpiece与bpe的对比","link":"#💡-wordpiece与bpe的对比","children":[]},{"level":3,"title":"💡 WordPiece与ULM的对比","slug":"💡-wordpiece与ulm的对比","link":"#💡-wordpiece与ulm的对比","children":[]},{"level":3,"title":"⚠️ 常见错误","slug":"⚠️-常见错误","link":"#⚠️-常见错误","children":[]}]},{"level":2,"title":"3️⃣ 技术术语通俗解读","slug":"_3️⃣-技术术语通俗解读","link":"#_3️⃣-技术术语通俗解读","children":[]},{"level":2,"title":"4️⃣ 行动清单","slug":"_4️⃣-行动清单","link":"#_4️⃣-行动清单","children":[]},{"level":2,"title":"[思考] 板块","slug":"思考-板块","link":"#思考-板块","children":[]}]}}
