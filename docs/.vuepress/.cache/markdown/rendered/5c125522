{"content":"<p>元数据：</p>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：VeRA, LoRA, 参数优化, 随机矩阵</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>VeRA（Vector-based Random Matrix Adaptation）是一种创新方法，旨在通过引入共享的随机权值矩阵，显著减少LoRA（Low-Rank Adaptation）参数的大小。与传统方法不同，VeRA不直接训练矩阵A和B，而是用共享的随机权值初始化这些矩阵，并仅在微调时训练两个向量d和b。\n<img src=\"/img/user/附件/Pasted image 20250424111725.png\" alt=\"Pasted image 20250424111725.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"_1-vera的创新机制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-vera的创新机制\"><span>1. VeRA的创新机制</span></a></h3>\n<p>VeRA通过将所有层中的矩阵A和B初始化为相同的随机权值，从而减少了参数大小。这种共享权值的方法不仅降低了计算复杂度，还提高了模型的效率。</p>\n<h3 id=\"_2-微调过程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-微调过程\"><span>2. 微调过程</span></a></h3>\n<p>在微调过程中，VeRA只需训练两个新的向量d和b。这种简化的训练过程使得模型在保持性能的同时，大幅减少了计算资源的消耗。</p>\n<h3 id=\"_3-技术术语解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-技术术语解释\"><span>3. 技术术语解释</span></a></h3>\n<ul>\n<li><strong>随机矩阵</strong>：一种用随机数填充的矩阵，用于初始化模型参数。</li>\n<li><strong>微调</strong>：在已有模型上进行小幅度的训练，以适应新的数据或任务。</li>\n<li><strong>共享权值</strong>：指在不同层中使用相同的参数值，以减少模型复杂性。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始化所有矩阵A和B为相同的随机权值。</li>\n<li>⚠ 在微调阶段，仅训练向量d和b。</li>\n<li>❗ 确保共享权值的一致性，以避免层间不匹配。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在初始化随机矩阵时，确保所有层使用相同的权值。如果不一致，可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>VeRA通过共享权值和简化微调过程，为参数优化提供了新的思路，特别适合资源受限的环境。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究VeRA在不同模型架构中的应用效果。</li>\n<li>测试VeRA在实际任务中的性能表现。</li>\n<li>探索其他可能的参数共享策略。</li>\n</ul>\n<blockquote>\n<p>原文出处: 引用自提供的文本内容。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/VeRA.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/VeRA.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/VeRA","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/VeRA/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T03:16:34.713Z","updated":"2025-04-24T03:17:27.242Z","title":"VeRA","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>元数据：</p>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：VeRA, LoRA, 参数优化, 随机矩阵</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>VeRA（Vector-based Random Matrix Adaptation）是一种创新方法，旨在通过引入共享的随机权值矩阵，显著减少LoRA（Low-Rank Adaptation）参数的大小。与传统方法不同，VeRA不直接训练矩阵A和B，而是用共享的随机权值初始化这些矩阵，并仅在微调时训练两个向量d和b。\n<img src=\"/img/user/附件/Pasted image 20250424111725.png\" alt=\"Pasted image 20250424111725.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"_1-vera的创新机制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-vera的创新机制\"><span>1. VeRA的创新机制</span></a></h3>\n<p>VeRA通过将所有层中的矩阵A和B初始化为相同的随机权值，从而减少了参数大小。这种共享权值的方法不仅降低了计算复杂度，还提高了模型的效率。</p>\n<h3 id=\"_2-微调过程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-微调过程\"><span>2. 微调过程</span></a></h3>\n<p>在微调过程中，VeRA只需训练两个新的向量d和b。这种简化的训练过程使得模型在保持性能的同时，大幅减少了计算资源的消耗。</p>\n<h3 id=\"_3-技术术语解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-技术术语解释\"><span>3. 技术术语解释</span></a></h3>\n<ul>\n<li><strong>随机矩阵</strong>：一种用随机数填充的矩阵，用于初始化模型参数。</li>\n<li><strong>微调</strong>：在已有模型上进行小幅度的训练，以适应新的数据或任务。</li>\n<li><strong>共享权值</strong>：指在不同层中使用相同的参数值，以减少模型复杂性。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始化所有矩阵A和B为相同的随机权值。</li>\n<li>⚠ 在微调阶段，仅训练向量d和b。</li>\n<li>❗ 确保共享权值的一致性，以避免层间不匹配。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在初始化随机矩阵时，确保所有层使用相同的权值。如果不一致，可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>VeRA通过共享权值和简化微调过程，为参数优化提供了新的思路，特别适合资源受限的环境。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究VeRA在不同模型架构中的应用效果。</li>\n<li>测试VeRA在实际任务中的性能表现。</li>\n<li>探索其他可能的参数共享策略。</li>\n</ul>\n<blockquote>\n<p>原文出处: 引用自提供的文本内容。</p>\n</blockquote>\n</template>","contentStripped":"<p>元数据：</p>\n<ul>\n<li>分类：机器学习</li>\n<li>标签：VeRA, LoRA, 参数优化, 随机矩阵</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>VeRA（Vector-based Random Matrix Adaptation）是一种创新方法，旨在通过引入共享的随机权值矩阵，显著减少LoRA（Low-Rank Adaptation）参数的大小。与传统方法不同，VeRA不直接训练矩阵A和B，而是用共享的随机权值初始化这些矩阵，并仅在微调时训练两个向量d和b。\n<img src=\"/img/user/附件/Pasted image 20250424111725.png\" alt=\"Pasted image 20250424111725.png\"></p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"_1-vera的创新机制\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-vera的创新机制\"><span>1. VeRA的创新机制</span></a></h3>\n<p>VeRA通过将所有层中的矩阵A和B初始化为相同的随机权值，从而减少了参数大小。这种共享权值的方法不仅降低了计算复杂度，还提高了模型的效率。</p>\n<h3 id=\"_2-微调过程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-微调过程\"><span>2. 微调过程</span></a></h3>\n<p>在微调过程中，VeRA只需训练两个新的向量d和b。这种简化的训练过程使得模型在保持性能的同时，大幅减少了计算资源的消耗。</p>\n<h3 id=\"_3-技术术语解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-技术术语解释\"><span>3. 技术术语解释</span></a></h3>\n<ul>\n<li><strong>随机矩阵</strong>：一种用随机数填充的矩阵，用于初始化模型参数。</li>\n<li><strong>微调</strong>：在已有模型上进行小幅度的训练，以适应新的数据或任务。</li>\n<li><strong>共享权值</strong>：指在不同层中使用相同的参数值，以减少模型复杂性。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始化所有矩阵A和B为相同的随机权值。</li>\n<li>⚠ 在微调阶段，仅训练向量d和b。</li>\n<li>❗ 确保共享权值的一致性，以避免层间不匹配。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p><strong>警告</strong>：在初始化随机矩阵时，确保所有层使用相同的权值。如果不一致，可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>VeRA通过共享权值和简化微调过程，为参数优化提供了新的思路，特别适合资源受限的环境。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究VeRA在不同模型架构中的应用效果。</li>\n<li>测试VeRA在实际任务中的性能表现。</li>\n<li>探索其他可能的参数共享策略。</li>\n</ul>\n<blockquote>\n<p>原文出处: 引用自提供的文本内容。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"元数据：\n\n- 分类：机器学习\n- 标签：VeRA, LoRA, 参数优化, 随机矩阵\n- 日期：2025年4月12日\n\n## 核心观点\nVeRA（Vector-based Random Matrix Adaptation）是一种创新方法，旨在通过引入共享的随机权值矩阵，显著减少LoRA（Low-Rank Adaptation）参数的大小。与传统方法不同，VeRA不直接训练矩阵A和B，而是用共享的随机权值初始化这些矩阵，并仅在微调时训练两个向量d和b。\n![Pasted image 20250424111725.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424111725.png)\n\n\n## 重点段落\n\n### 1. VeRA的创新机制\nVeRA通过将所有层中的矩阵A和B初始化为相同的随机权值，从而减少了参数大小。这种共享权值的方法不仅降低了计算复杂度，还提高了模型的效率。\n\n\n### 2. 微调过程\n在微调过程中，VeRA只需训练两个新的向量d和b。这种简化的训练过程使得模型在保持性能的同时，大幅减少了计算资源的消耗。\n\n\n### 3. 技术术语解释\n- **随机矩阵**：一种用随机数填充的矩阵，用于初始化模型参数。\n- **微调**：在已有模型上进行小幅度的训练，以适应新的数据或任务。\n- **共享权值**：指在不同层中使用相同的参数值，以减少模型复杂性。\n\n\n## 操作步骤\n1. ✅ 初始化所有矩阵A和B为相同的随机权值。\n2. ⚠ 在微调阶段，仅训练向量d和b。\n3. ❗ 确保共享权值的一致性，以避免层间不匹配。\n\n\n## 常见错误\n> **警告**：在初始化随机矩阵时，确保所有层使用相同的权值。如果不一致，可能导致模型性能下降。\n\n\n## 💡启发点\nVeRA通过共享权值和简化微调过程，为参数优化提供了新的思路，特别适合资源受限的环境。\n\n\n## 行动清单\n- 研究VeRA在不同模型架构中的应用效果。\n- 测试VeRA在实际任务中的性能表现。\n- 探索其他可能的参数共享策略。\n\n> 原文出处: 引用自提供的文本内容。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"核心观点","slug":"核心观点","link":"#核心观点","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[{"level":3,"title":"1. VeRA的创新机制","slug":"_1-vera的创新机制","link":"#_1-vera的创新机制","children":[]},{"level":3,"title":"2. 微调过程","slug":"_2-微调过程","link":"#_2-微调过程","children":[]},{"level":3,"title":"3. 技术术语解释","slug":"_3-技术术语解释","link":"#_3-技术术语解释","children":[]}]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
