{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：Unigram语言模型、分词算法、BPE、子词优化</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Unigram语言模型（ULM）是一种基于概率的分词算法，通过逐步优化词表，保留对整体损失影响较大的子词，从而生成更优的分词结果。其核心思想是利用语言模型的概率分布来衡量子词的重要性，最终构建一个高效且覆盖全面的词表。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"_1-ulm的核心思路\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-ulm的核心思路\"><span>1. <strong>ULM的核心思路</strong></span></a></h3>\n<p>ULM通过以下步骤实现子词优化：\n✅ <strong>初始化大词表</strong>：创建一个包含所有字符和高频n-gram的初始词表，确保覆盖语料中的所有单词。可以利用BPE（Byte Pair Encoding）等方法构建。<img src=\"/img/user/附件/Pasted image 20250327125052.png\" alt=\"Pasted image 20250327125052.png\">\n✅ <strong>计算子词概率</strong>：使用Unigram语言模型（如EM算法）估计每个子词的概率，并通过维特比算法寻找最优分词结果。<img src=\"/img/user/附件/Pasted image 20250327125114.png\" alt=\"Pasted image 20250327125114.png\">\n✅ <strong>评估子词重要性</strong>：计算删除每个子词对总损失（Loss）的影响，重要性越高的子词对Loss的影响越大。<img src=\"/img/user/附件/Pasted image 20250327125044.png\" alt=\"Pasted image 20250327125044.png\">\n✅ <strong>精简词表</strong>：按照Loss大小排序，移除对Loss影响最小的子词，重复上述步骤，直到词表缩减到目标大小。<img src=\"/img/user/附件/Pasted image 20250327125036.png\" alt=\"Pasted image 20250327125036.png\"></p>\n<hr>\n<h3 id=\"_2-ulm的优势与挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-ulm的优势与挑战\"><span>2. <strong>ULM的优势与挑战</strong></span></a></h3>\n<table>\n<thead>\n<tr>\n<th>优势 💡</th>\n<th>挑战 ⚠️</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>利用所有可能的分词结果，提高分词质量</td>\n<td>初始词表质量对结果影响显著</td>\n</tr>\n<tr>\n<td>能为多种分词结果赋予概率，处理噪声能力强</td>\n<td>算法实现较复杂</td>\n</tr>\n<tr>\n<td>支持生成带概率的分词结果，提高灵活性</td>\n<td>需要大量计算资源</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h3 id=\"_3-ulm操作流程示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-ulm操作流程示例\"><span>3. <strong>ULM操作流程示例</strong></span></a></h3>\n<p>以下是一个基于&quot;hug&quot;单词的具体操作示例：</p>\n<ol>\n<li><strong>初始分词方法</strong>：对于&quot;hug&quot;，可能有以下分词方式：\n<ul>\n<li><code v-pre>h,u,g</code></li>\n<li><code v-pre>hu,g</code></li>\n<li><code v-pre>h,ug</code></li>\n<li><code v-pre>hug</code>（不在初始词表中）</li>\n</ul>\n</li>\n<li><strong>概率计算</strong>：\n<ul>\n<li>假设前三种分词方式在初始词表中，其概率分别为：<code v-pre>P(h,u,g) = 0.005</code>, <code v-pre>P(hu,g) = 0.006</code>, <code v-pre>P(h,ug) = 0.011</code>。</li>\n</ul>\n</li>\n<li><strong>选择最优分词</strong>：\n<ul>\n<li>根据最大似然原则，选择<code v-pre>P(h,ug) = 0.011</code>作为最优分词结果。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"_4-数据与趋势📈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-数据与趋势📈\"><span>4. <strong>数据与趋势📈</strong></span></a></h3>\n<p>以下是ULM优化过程中需要关注的数据点：</p>\n<table>\n<thead>\n<tr>\n<th>数据点</th>\n<th>示例值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>初始Loss总值</td>\n<td>170.40</td>\n</tr>\n<tr>\n<td>子词<code v-pre>ug</code>的使用频率</td>\n<td>0</td>\n</tr>\n<tr>\n<td>子词<code v-pre>ug</code>对Loss影响</td>\n<td>最小</td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：随着迭代次数增加，低频或无用子词将逐步被移除，最终形成一个高效的紧凑型词表。</p>\n<hr>\n<h3 id=\"_5-常见错误⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-常见错误⚠️\"><span>5. <strong>常见错误⚠️</strong></span></a></h3>\n<ul>\n<li><strong>删除单字符子词</strong>：由于单字符子词是语料覆盖的基础，它们不能被移除，否则会导致OOV（Out of Vocabulary）问题。</li>\n<li><strong>初始词表不完整</strong>：如果初始词表不能覆盖语料中的所有单词，后续概率计算将失败。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>📌 使用BPE或字符级初始化构建大规模初始词表。</li>\n<li>📌 实现基于Unigram模型的EM算法，用于子词概率估计。</li>\n<li>📌 开发自动化流程，定期评估和精简子词表。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何平衡初始大词表的覆盖范围和计算复杂度？</li>\n<li>ULM是否适用于多语言场景？如何处理不同语言间的差异？</li>\n<li>能否结合深度学习模型进一步优化ULM的分词效果？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：本文基于Unigram语言模型（ULM）的技术文档整理与总结，原文详述了ULM在自然语言处理中的应用与实现细节。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践.md","filePathRelative":"notes_bak/大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/分词/使用Unigram语言模型（ULM）优化分词算法：核心思路与实践/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-03-27T04:49:06.967Z","updated":"2025-04-12T04:53:52.908Z","title":"使用Unigram语言模型（ULM）优化分词算法：核心思路与实践","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：Unigram语言模型、分词算法、BPE、子词优化</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Unigram语言模型（ULM）是一种基于概率的分词算法，通过逐步优化词表，保留对整体损失影响较大的子词，从而生成更优的分词结果。其核心思想是利用语言模型的概率分布来衡量子词的重要性，最终构建一个高效且覆盖全面的词表。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"_1-ulm的核心思路\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-ulm的核心思路\"><span>1. <strong>ULM的核心思路</strong></span></a></h3>\n<p>ULM通过以下步骤实现子词优化：\n✅ <strong>初始化大词表</strong>：创建一个包含所有字符和高频n-gram的初始词表，确保覆盖语料中的所有单词。可以利用BPE（Byte Pair Encoding）等方法构建。<img src=\"/img/user/附件/Pasted image 20250327125052.png\" alt=\"Pasted image 20250327125052.png\">\n✅ <strong>计算子词概率</strong>：使用Unigram语言模型（如EM算法）估计每个子词的概率，并通过维特比算法寻找最优分词结果。<img src=\"/img/user/附件/Pasted image 20250327125114.png\" alt=\"Pasted image 20250327125114.png\">\n✅ <strong>评估子词重要性</strong>：计算删除每个子词对总损失（Loss）的影响，重要性越高的子词对Loss的影响越大。<img src=\"/img/user/附件/Pasted image 20250327125044.png\" alt=\"Pasted image 20250327125044.png\">\n✅ <strong>精简词表</strong>：按照Loss大小排序，移除对Loss影响最小的子词，重复上述步骤，直到词表缩减到目标大小。<img src=\"/img/user/附件/Pasted image 20250327125036.png\" alt=\"Pasted image 20250327125036.png\"></p>\n<hr>\n<h3 id=\"_2-ulm的优势与挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-ulm的优势与挑战\"><span>2. <strong>ULM的优势与挑战</strong></span></a></h3>\n<table>\n<thead>\n<tr>\n<th>优势 💡</th>\n<th>挑战 ⚠️</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>利用所有可能的分词结果，提高分词质量</td>\n<td>初始词表质量对结果影响显著</td>\n</tr>\n<tr>\n<td>能为多种分词结果赋予概率，处理噪声能力强</td>\n<td>算法实现较复杂</td>\n</tr>\n<tr>\n<td>支持生成带概率的分词结果，提高灵活性</td>\n<td>需要大量计算资源</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h3 id=\"_3-ulm操作流程示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-ulm操作流程示例\"><span>3. <strong>ULM操作流程示例</strong></span></a></h3>\n<p>以下是一个基于&quot;hug&quot;单词的具体操作示例：</p>\n<ol>\n<li><strong>初始分词方法</strong>：对于&quot;hug&quot;，可能有以下分词方式：\n<ul>\n<li><code v-pre>h,u,g</code></li>\n<li><code v-pre>hu,g</code></li>\n<li><code v-pre>h,ug</code></li>\n<li><code v-pre>hug</code>（不在初始词表中）</li>\n</ul>\n</li>\n<li><strong>概率计算</strong>：\n<ul>\n<li>假设前三种分词方式在初始词表中，其概率分别为：<code v-pre>P(h,u,g) = 0.005</code>, <code v-pre>P(hu,g) = 0.006</code>, <code v-pre>P(h,ug) = 0.011</code>。</li>\n</ul>\n</li>\n<li><strong>选择最优分词</strong>：\n<ul>\n<li>根据最大似然原则，选择<code v-pre>P(h,ug) = 0.011</code>作为最优分词结果。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"_4-数据与趋势📈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-数据与趋势📈\"><span>4. <strong>数据与趋势📈</strong></span></a></h3>\n<p>以下是ULM优化过程中需要关注的数据点：</p>\n<table>\n<thead>\n<tr>\n<th>数据点</th>\n<th>示例值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>初始Loss总值</td>\n<td>170.40</td>\n</tr>\n<tr>\n<td>子词<code v-pre>ug</code>的使用频率</td>\n<td>0</td>\n</tr>\n<tr>\n<td>子词<code v-pre>ug</code>对Loss影响</td>\n<td>最小</td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：随着迭代次数增加，低频或无用子词将逐步被移除，最终形成一个高效的紧凑型词表。</p>\n<hr>\n<h3 id=\"_5-常见错误⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-常见错误⚠️\"><span>5. <strong>常见错误⚠️</strong></span></a></h3>\n<ul>\n<li><strong>删除单字符子词</strong>：由于单字符子词是语料覆盖的基础，它们不能被移除，否则会导致OOV（Out of Vocabulary）问题。</li>\n<li><strong>初始词表不完整</strong>：如果初始词表不能覆盖语料中的所有单词，后续概率计算将失败。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>📌 使用BPE或字符级初始化构建大规模初始词表。</li>\n<li>📌 实现基于Unigram模型的EM算法，用于子词概率估计。</li>\n<li>📌 开发自动化流程，定期评估和精简子词表。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何平衡初始大词表的覆盖范围和计算复杂度？</li>\n<li>ULM是否适用于多语言场景？如何处理不同语言间的差异？</li>\n<li>能否结合深度学习模型进一步优化ULM的分词效果？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：本文基于Unigram语言模型（ULM）的技术文档整理与总结，原文详述了ULM在自然语言处理中的应用与实现细节。</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：Unigram语言模型、分词算法、BPE、子词优化</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>Unigram语言模型（ULM）是一种基于概率的分词算法，通过逐步优化词表，保留对整体损失影响较大的子词，从而生成更优的分词结果。其核心思想是利用语言模型的概率分布来衡量子词的重要性，最终构建一个高效且覆盖全面的词表。</p>\n<hr>\n<h2 id=\"重点内容解析\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容解析\"><span>重点内容解析</span></a></h2>\n<h3 id=\"_1-ulm的核心思路\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-ulm的核心思路\"><span>1. <strong>ULM的核心思路</strong></span></a></h3>\n<p>ULM通过以下步骤实现子词优化：\n✅ <strong>初始化大词表</strong>：创建一个包含所有字符和高频n-gram的初始词表，确保覆盖语料中的所有单词。可以利用BPE（Byte Pair Encoding）等方法构建。<img src=\"/img/user/附件/Pasted image 20250327125052.png\" alt=\"Pasted image 20250327125052.png\">\n✅ <strong>计算子词概率</strong>：使用Unigram语言模型（如EM算法）估计每个子词的概率，并通过维特比算法寻找最优分词结果。<img src=\"/img/user/附件/Pasted image 20250327125114.png\" alt=\"Pasted image 20250327125114.png\">\n✅ <strong>评估子词重要性</strong>：计算删除每个子词对总损失（Loss）的影响，重要性越高的子词对Loss的影响越大。<img src=\"/img/user/附件/Pasted image 20250327125044.png\" alt=\"Pasted image 20250327125044.png\">\n✅ <strong>精简词表</strong>：按照Loss大小排序，移除对Loss影响最小的子词，重复上述步骤，直到词表缩减到目标大小。<img src=\"/img/user/附件/Pasted image 20250327125036.png\" alt=\"Pasted image 20250327125036.png\"></p>\n<hr>\n<h3 id=\"_2-ulm的优势与挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-ulm的优势与挑战\"><span>2. <strong>ULM的优势与挑战</strong></span></a></h3>\n<table>\n<thead>\n<tr>\n<th>优势 💡</th>\n<th>挑战 ⚠️</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>利用所有可能的分词结果，提高分词质量</td>\n<td>初始词表质量对结果影响显著</td>\n</tr>\n<tr>\n<td>能为多种分词结果赋予概率，处理噪声能力强</td>\n<td>算法实现较复杂</td>\n</tr>\n<tr>\n<td>支持生成带概率的分词结果，提高灵活性</td>\n<td>需要大量计算资源</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h3 id=\"_3-ulm操作流程示例\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-ulm操作流程示例\"><span>3. <strong>ULM操作流程示例</strong></span></a></h3>\n<p>以下是一个基于&quot;hug&quot;单词的具体操作示例：</p>\n<ol>\n<li><strong>初始分词方法</strong>：对于&quot;hug&quot;，可能有以下分词方式：\n<ul>\n<li><code v-pre>h,u,g</code></li>\n<li><code v-pre>hu,g</code></li>\n<li><code v-pre>h,ug</code></li>\n<li><code v-pre>hug</code>（不在初始词表中）</li>\n</ul>\n</li>\n<li><strong>概率计算</strong>：\n<ul>\n<li>假设前三种分词方式在初始词表中，其概率分别为：<code v-pre>P(h,u,g) = 0.005</code>, <code v-pre>P(hu,g) = 0.006</code>, <code v-pre>P(h,ug) = 0.011</code>。</li>\n</ul>\n</li>\n<li><strong>选择最优分词</strong>：\n<ul>\n<li>根据最大似然原则，选择<code v-pre>P(h,ug) = 0.011</code>作为最优分词结果。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"_4-数据与趋势📈\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-数据与趋势📈\"><span>4. <strong>数据与趋势📈</strong></span></a></h3>\n<p>以下是ULM优化过程中需要关注的数据点：</p>\n<table>\n<thead>\n<tr>\n<th>数据点</th>\n<th>示例值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>初始Loss总值</td>\n<td>170.40</td>\n</tr>\n<tr>\n<td>子词<code v-pre>ug</code>的使用频率</td>\n<td>0</td>\n</tr>\n<tr>\n<td>子词<code v-pre>ug</code>对Loss影响</td>\n<td>最小</td>\n</tr>\n</tbody>\n</table>\n<p>📈 <strong>趋势预测</strong>：随着迭代次数增加，低频或无用子词将逐步被移除，最终形成一个高效的紧凑型词表。</p>\n<hr>\n<h3 id=\"_5-常见错误⚠️\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_5-常见错误⚠️\"><span>5. <strong>常见错误⚠️</strong></span></a></h3>\n<ul>\n<li><strong>删除单字符子词</strong>：由于单字符子词是语料覆盖的基础，它们不能被移除，否则会导致OOV（Out of Vocabulary）问题。</li>\n<li><strong>初始词表不完整</strong>：如果初始词表不能覆盖语料中的所有单词，后续概率计算将失败。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ol>\n<li>📌 使用BPE或字符级初始化构建大规模初始词表。</li>\n<li>📌 实现基于Unigram模型的EM算法，用于子词概率估计。</li>\n<li>📌 开发自动化流程，定期评估和精简子词表。</li>\n</ol>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>如何平衡初始大词表的覆盖范围和计算复杂度？</li>\n<li>ULM是否适用于多语言场景？如何处理不同语言间的差异？</li>\n<li>能否结合深度学习模型进一步优化ULM的分词效果？</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>来源</strong>：本文基于Unigram语言模型（ULM）的技术文档整理与总结，原文详述了ULM在自然语言处理中的应用与实现细节。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 元数据\n- **分类**：自然语言处理（NLP）\n- **标签**：Unigram语言模型、分词算法、BPE、子词优化\n- **日期**：2025年4月2日  \n\n---\n\n\n\n## 核心观点总结\nUnigram语言模型（ULM）是一种基于概率的分词算法，通过逐步优化词表，保留对整体损失影响较大的子词，从而生成更优的分词结果。其核心思想是利用语言模型的概率分布来衡量子词的重要性，最终构建一个高效且覆盖全面的词表。\n\n---\n\n\n\n## 重点内容解析\n\n### 1. **ULM的核心思路**\nULM通过以下步骤实现子词优化：\n✅ **初始化大词表**：创建一个包含所有字符和高频n-gram的初始词表，确保覆盖语料中的所有单词。可以利用BPE（Byte Pair Encoding）等方法构建。![Pasted image 20250327125052.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250327125052.png)\n✅ **计算子词概率**：使用Unigram语言模型（如EM算法）估计每个子词的概率，并通过维特比算法寻找最优分词结果。![Pasted image 20250327125114.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250327125114.png)\n✅ **评估子词重要性**：计算删除每个子词对总损失（Loss）的影响，重要性越高的子词对Loss的影响越大。![Pasted image 20250327125044.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250327125044.png)\n✅ **精简词表**：按照Loss大小排序，移除对Loss影响最小的子词，重复上述步骤，直到词表缩减到目标大小。![Pasted image 20250327125036.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250327125036.png)\n\n---\n\n\n### 2. **ULM的优势与挑战**\n| 优势 💡 | 挑战 ⚠️ |\n|--------|---------|\n| 利用所有可能的分词结果，提高分词质量 | 初始词表质量对结果影响显著 |\n| 能为多种分词结果赋予概率，处理噪声能力强 | 算法实现较复杂 |\n| 支持生成带概率的分词结果，提高灵活性 | 需要大量计算资源 |\n\n---\n\n\n### 3. **ULM操作流程示例**\n以下是一个基于\"hug\"单词的具体操作示例：\n1. **初始分词方法**：对于\"hug\"，可能有以下分词方式：\n   - `h,u,g`\n   - `hu,g`\n   - `h,ug`\n   - `hug`（不在初始词表中）\n2. **概率计算**：\n   - 假设前三种分词方式在初始词表中，其概率分别为：`P(h,u,g) = 0.005`, `P(hu,g) = 0.006`, `P(h,ug) = 0.011`。\n3. **选择最优分词**：\n   - 根据最大似然原则，选择`P(h,ug) = 0.011`作为最优分词结果。\n\n---\n\n\n### 4. **数据与趋势📈**\n以下是ULM优化过程中需要关注的数据点：\n| 数据点           | 示例值   |\n|------------------|----------|\n| 初始Loss总值     | 170.40   |\n| 子词`ug`的使用频率 | 0        |\n| 子词`ug`对Loss影响 | 最小     |\n\n📈 **趋势预测**：随着迭代次数增加，低频或无用子词将逐步被移除，最终形成一个高效的紧凑型词表。\n\n---\n\n\n### 5. **常见错误⚠️**\n- **删除单字符子词**：由于单字符子词是语料覆盖的基础，它们不能被移除，否则会导致OOV（Out of Vocabulary）问题。\n- **初始词表不完整**：如果初始词表不能覆盖语料中的所有单词，后续概率计算将失败。\n\n---\n\n\n\n## 行动清单\n1. 📌 使用BPE或字符级初始化构建大规模初始词表。\n2. 📌 实现基于Unigram模型的EM算法，用于子词概率估计。\n3. 📌 开发自动化流程，定期评估和精简子词表。\n\n---\n\n\n\n## [思考] 延伸问题\n1. 如何平衡初始大词表的覆盖范围和计算复杂度？\n2. ULM是否适用于多语言场景？如何处理不同语言间的差异？\n3. 能否结合深度学习模型进一步优化ULM的分词效果？\n\n---\n\n> **来源**：本文基于Unigram语言模型（ULM）的技术文档整理与总结，原文详述了ULM在自然语言处理中的应用与实现细节。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点内容解析","slug":"重点内容解析","link":"#重点内容解析","children":[{"level":3,"title":"1. ULM的核心思路","slug":"_1-ulm的核心思路","link":"#_1-ulm的核心思路","children":[]},{"level":3,"title":"2. ULM的优势与挑战","slug":"_2-ulm的优势与挑战","link":"#_2-ulm的优势与挑战","children":[]},{"level":3,"title":"3. ULM操作流程示例","slug":"_3-ulm操作流程示例","link":"#_3-ulm操作流程示例","children":[]},{"level":3,"title":"4. 数据与趋势📈","slug":"_4-数据与趋势📈","link":"#_4-数据与趋势📈","children":[]},{"level":3,"title":"5. 常见错误⚠️","slug":"_5-常见错误⚠️","link":"#_5-常见错误⚠️","children":[]}]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"[思考] 延伸问题","slug":"思考-延伸问题","link":"#思考-延伸问题","children":[]}]}}
