{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：分词算法、SentencePiece、Tokenizers库、NLP工具、多语言支持</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span><strong>核心内容总结</strong></span></a></h2>\n<p>文本分词是自然语言处理（NLP）中的关键步骤，而<code v-pre>SentencePiece</code>和<code v-pre>Tokenizers库</code>提供了高效、灵活的解决方案。以下是两者的核心特点和功能总结：</p>\n<ul>\n<li><strong>SentencePiece</strong>：支持多种分词算法（如BPE、ULM），具备多语言支持、编解码可逆性，无需预分词，快速轻量。</li>\n<li><strong>Tokenizers库</strong>：提供完整的编码流程，包括数据标准化、预分词、分词模型和后处理，支持灵活定制。</li>\n</ul>\n<hr>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span><strong>重点内容</strong></span></a></h2>\n<h3 id=\"_1-sentencepiece-的核心特点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-sentencepiece-的核心特点\"><span><strong>1. SentencePiece 的核心特点</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>多分词粒度支持</strong>：<br>\n支持BPE（Byte Pair Encoding）、ULM子词算法，以及字符级（char）和单词级（word）分词。</p>\n</li>\n<li>\n<p><strong>多语言兼容性</strong>：<br>\n使用Unicode编码字符，统一处理不同语言的输入（如英文与中文），避免了多语言编码差异问题。</p>\n</li>\n<li>\n<p><strong>编解码的可逆性</strong>：<br>\n通过显式处理空格并用特殊符号“▁”（U+2581）转义，实现了简单且可逆的编解码流程。<br>\n示例公式：</p>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span>Decode(Encode(Normalized(text))) = Normalized(text)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div></li>\n<li>\n<p><strong>无需预分词（Pre-tokenization）</strong>：<br>\n不需要对原始文本进行预处理，直接从原始数据中进行训练。</p>\n</li>\n</ul>\n<hr>\n<h3 id=\"_2-tokenizers库的编码流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-tokenizers库的编码流程\"><span><strong>2. Tokenizers库的编码流程</strong></span></a></h3>\n<p>Tokenizers库在文本分词时，采用以下管道化流程：</p>\n<h4 id=\"✅-normalization-标准化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-normalization-标准化\"><span>✅ <strong>Normalization（标准化）</strong></span></a></h4>\n<p>对文本进行清理和格式化，包括：</p>\n<ul>\n<li>删除多余空格</li>\n<li>去除变音符号</li>\n<li>转换为小写</li>\n<li>Unicode正规化</li>\n</ul>\n<p>代码示例：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizers</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> NFD</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> StripAccents</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Lowercase</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 定义一个normalizer</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalizer </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">Sequence</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">([</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    NFD</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(),</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">           # Unicode正规化</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    StripAccents</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(),</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">  # 去除变音符号</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    Lowercase</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">      # 转小写</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 对字符串进行标准化</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizer</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalize_str</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">Héllò hôw are ü?</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">print</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">  # Output: 'hello how are u?'</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"⚠️-pre-tokenization-预分词\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-pre-tokenization-预分词\"><span>⚠️ <strong>Pre-tokenization（预分词）</strong></span></a></h4>\n<p>将文本拆分为更小的单位，例如单词或子词。<br>\n示例：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Whitespace</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenizer </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Whitespace</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pre_tokenizer</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenize_str</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">Hello! How are you? I'm fine, thank you.</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">print</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># Output: [('Hello', (0, 5)), ('!', (5, 6)), ('How', (7, 10)), ...]</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"❗️-模型分词与后处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#❗️-模型分词与后处理\"><span>❗️ <strong>模型分词与后处理</strong></span></a></h4>\n<ul>\n<li>使用具体分词算法（如BPE）。</li>\n<li>添加特殊标记（如[CLS]、[SEP]）以满足特定任务需求。</li>\n</ul>\n<hr>\n<h3 id=\"_3-常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-常见错误与注意事项\"><span><strong>3. 常见错误与注意事项</strong></span></a></h3>\n<blockquote>\n<p>⚠️ <strong>常见错误</strong>：</p>\n<ul>\n<li>忽略文本标准化步骤可能导致编码不一致。</li>\n<li>未正确设置预分词器或模型参数会影响最终分词效果。</li>\n<li>对多语言文本未使用Unicode处理，可能导致字符编码问题。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"作者观点-vs-个人观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#作者观点-vs-个人观点\"><span><strong>作者观点 vs 个人观点</strong></span></a></h2>\n<table>\n<thead>\n<tr>\n<th><strong>作者观点</strong></th>\n<th><strong>个人观点</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SentencePiece无需预分词，适合从零开始训练</td>\n<td>SentencePiece对小型数据集可能不够高效</td>\n</tr>\n<tr>\n<td>Tokenizers库灵活可扩展，适合多任务应用</td>\n<td>Tokenizers库的学习曲线稍陡，需要良好编程基础</td>\n</tr>\n<tr>\n<td>多语言支持让工具更通用</td>\n<td>对低资源语言可能需要额外优化</td>\n</tr>\n</tbody>\n</table>\n<p>💡 启发点：</p>\n<ul>\n<li>分词工具的选择应根据具体任务需求和数据规模调整。</li>\n<li>编解码的可逆性是提升数据一致性的重要特性。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span><strong>行动清单</strong></span></a></h2>\n<ol>\n<li>✅ 学习并实现SentencePiece的基本用法，尝试不同分词算法（如BPE）。</li>\n<li>⚠️ 使用Tokenizers库对多语言文本进行标准化和预分词。</li>\n<li>❗️ 对比不同分词工具在特定任务上的性能，记录实验结果。</li>\n</ol>\n<hr>\n<h2 id=\"📈-趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-趋势预测\"><span>📈 <strong>趋势预测</strong></span></a></h2>\n<p>随着自然语言处理技术的普及，多语言和低资源语言的支持将成为分词工具的重要发展方向。未来可能会出现更多结合深度学习和传统方法的新型分词工具。</p>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>SentencePiece在处理混合语言（如中英混合）时表现如何？是否需要额外优化？</li>\n<li>Tokenizers库如何与主流深度学习框架（如TensorFlow或PyTorch）无缝集成？</li>\n<li>在数据量有限的情况下，是否有更适合的小型分词模型？</li>\n</ol>\n<hr>\n<blockquote>\n<p>引用来源：<a href=\"https://github.com/google/sentencepiece\" target=\"_blank\" rel=\"noopener noreferrer\">SentencePiece官方文档</a>、<a href=\"https://huggingface.co/docs/tokenizers/\" target=\"_blank\" rel=\"noopener noreferrer\">Tokenizers库文档</a></p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/分词/常用分词库.md","filePathRelative":"notes_bak/大语言模型学习/分词/常用分词库.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/分词/常用分词库","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":10,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/分词/常用分词库/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":10,"created":"2025-03-27T04:51:56.518Z","updated":"2025-04-12T04:53:56.473Z","title":"常用分词库","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：分词算法、SentencePiece、Tokenizers库、NLP工具、多语言支持</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span><strong>核心内容总结</strong></span></a></h2>\n<p>文本分词是自然语言处理（NLP）中的关键步骤，而<code v-pre>SentencePiece</code>和<code v-pre>Tokenizers库</code>提供了高效、灵活的解决方案。以下是两者的核心特点和功能总结：</p>\n<ul>\n<li><strong>SentencePiece</strong>：支持多种分词算法（如BPE、ULM），具备多语言支持、编解码可逆性，无需预分词，快速轻量。</li>\n<li><strong>Tokenizers库</strong>：提供完整的编码流程，包括数据标准化、预分词、分词模型和后处理，支持灵活定制。</li>\n</ul>\n<hr>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span><strong>重点内容</strong></span></a></h2>\n<h3 id=\"_1-sentencepiece-的核心特点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-sentencepiece-的核心特点\"><span><strong>1. SentencePiece 的核心特点</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>多分词粒度支持</strong>：<br>\n支持BPE（Byte Pair Encoding）、ULM子词算法，以及字符级（char）和单词级（word）分词。</p>\n</li>\n<li>\n<p><strong>多语言兼容性</strong>：<br>\n使用Unicode编码字符，统一处理不同语言的输入（如英文与中文），避免了多语言编码差异问题。</p>\n</li>\n<li>\n<p><strong>编解码的可逆性</strong>：<br>\n通过显式处理空格并用特殊符号“▁”（U+2581）转义，实现了简单且可逆的编解码流程。<br>\n示例公式：</p>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span>Decode(Encode(Normalized(text))) = Normalized(text)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div></li>\n<li>\n<p><strong>无需预分词（Pre-tokenization）</strong>：<br>\n不需要对原始文本进行预处理，直接从原始数据中进行训练。</p>\n</li>\n</ul>\n<hr>\n<h3 id=\"_2-tokenizers库的编码流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-tokenizers库的编码流程\"><span><strong>2. Tokenizers库的编码流程</strong></span></a></h3>\n<p>Tokenizers库在文本分词时，采用以下管道化流程：</p>\n<h4 id=\"✅-normalization-标准化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-normalization-标准化\"><span>✅ <strong>Normalization（标准化）</strong></span></a></h4>\n<p>对文本进行清理和格式化，包括：</p>\n<ul>\n<li>删除多余空格</li>\n<li>去除变音符号</li>\n<li>转换为小写</li>\n<li>Unicode正规化</li>\n</ul>\n<p>代码示例：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizers</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> NFD</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> StripAccents</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Lowercase</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 定义一个normalizer</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalizer </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">Sequence</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">([</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    NFD</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(),</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">           # Unicode正规化</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    StripAccents</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(),</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">  # 去除变音符号</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    Lowercase</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">      # 转小写</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 对字符串进行标准化</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizer</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalize_str</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">Héllò hôw are ü?</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">print</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">  # Output: 'hello how are u?'</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"⚠️-pre-tokenization-预分词\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-pre-tokenization-预分词\"><span>⚠️ <strong>Pre-tokenization（预分词）</strong></span></a></h4>\n<p>将文本拆分为更小的单位，例如单词或子词。<br>\n示例：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Whitespace</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenizer </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Whitespace</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pre_tokenizer</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenize_str</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">Hello! How are you? I'm fine, thank you.</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">print</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># Output: [('Hello', (0, 5)), ('!', (5, 6)), ('How', (7, 10)), ...]</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"❗️-模型分词与后处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#❗️-模型分词与后处理\"><span>❗️ <strong>模型分词与后处理</strong></span></a></h4>\n<ul>\n<li>使用具体分词算法（如BPE）。</li>\n<li>添加特殊标记（如[CLS]、[SEP]）以满足特定任务需求。</li>\n</ul>\n<hr>\n<h3 id=\"_3-常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-常见错误与注意事项\"><span><strong>3. 常见错误与注意事项</strong></span></a></h3>\n<blockquote>\n<p>⚠️ <strong>常见错误</strong>：</p>\n<ul>\n<li>忽略文本标准化步骤可能导致编码不一致。</li>\n<li>未正确设置预分词器或模型参数会影响最终分词效果。</li>\n<li>对多语言文本未使用Unicode处理，可能导致字符编码问题。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"作者观点-vs-个人观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#作者观点-vs-个人观点\"><span><strong>作者观点 vs 个人观点</strong></span></a></h2>\n<table>\n<thead>\n<tr>\n<th><strong>作者观点</strong></th>\n<th><strong>个人观点</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SentencePiece无需预分词，适合从零开始训练</td>\n<td>SentencePiece对小型数据集可能不够高效</td>\n</tr>\n<tr>\n<td>Tokenizers库灵活可扩展，适合多任务应用</td>\n<td>Tokenizers库的学习曲线稍陡，需要良好编程基础</td>\n</tr>\n<tr>\n<td>多语言支持让工具更通用</td>\n<td>对低资源语言可能需要额外优化</td>\n</tr>\n</tbody>\n</table>\n<p>💡 启发点：</p>\n<ul>\n<li>分词工具的选择应根据具体任务需求和数据规模调整。</li>\n<li>编解码的可逆性是提升数据一致性的重要特性。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span><strong>行动清单</strong></span></a></h2>\n<ol>\n<li>✅ 学习并实现SentencePiece的基本用法，尝试不同分词算法（如BPE）。</li>\n<li>⚠️ 使用Tokenizers库对多语言文本进行标准化和预分词。</li>\n<li>❗️ 对比不同分词工具在特定任务上的性能，记录实验结果。</li>\n</ol>\n<hr>\n<h2 id=\"📈-趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-趋势预测\"><span>📈 <strong>趋势预测</strong></span></a></h2>\n<p>随着自然语言处理技术的普及，多语言和低资源语言的支持将成为分词工具的重要发展方向。未来可能会出现更多结合深度学习和传统方法的新型分词工具。</p>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>SentencePiece在处理混合语言（如中英混合）时表现如何？是否需要额外优化？</li>\n<li>Tokenizers库如何与主流深度学习框架（如TensorFlow或PyTorch）无缝集成？</li>\n<li>在数据量有限的情况下，是否有更适合的小型分词模型？</li>\n</ol>\n<hr>\n<blockquote>\n<p>引用来源：<a href=\"https://github.com/google/sentencepiece\" target=\"_blank\" rel=\"noopener noreferrer\">SentencePiece官方文档</a>、<a href=\"https://huggingface.co/docs/tokenizers/\" target=\"_blank\" rel=\"noopener noreferrer\">Tokenizers库文档</a></p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li><strong>分类</strong>：自然语言处理（NLP）</li>\n<li><strong>标签</strong>：分词算法、SentencePiece、Tokenizers库、NLP工具、多语言支持</li>\n<li><strong>日期</strong>：2025年4月2日</li>\n</ul>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span><strong>核心内容总结</strong></span></a></h2>\n<p>文本分词是自然语言处理（NLP）中的关键步骤，而<code v-pre>SentencePiece</code>和<code v-pre>Tokenizers库</code>提供了高效、灵活的解决方案。以下是两者的核心特点和功能总结：</p>\n<ul>\n<li><strong>SentencePiece</strong>：支持多种分词算法（如BPE、ULM），具备多语言支持、编解码可逆性，无需预分词，快速轻量。</li>\n<li><strong>Tokenizers库</strong>：提供完整的编码流程，包括数据标准化、预分词、分词模型和后处理，支持灵活定制。</li>\n</ul>\n<hr>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span><strong>重点内容</strong></span></a></h2>\n<h3 id=\"_1-sentencepiece-的核心特点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-sentencepiece-的核心特点\"><span><strong>1. SentencePiece 的核心特点</strong></span></a></h3>\n<ul>\n<li>\n<p><strong>多分词粒度支持</strong>：<br>\n支持BPE（Byte Pair Encoding）、ULM子词算法，以及字符级（char）和单词级（word）分词。</p>\n</li>\n<li>\n<p><strong>多语言兼容性</strong>：<br>\n使用Unicode编码字符，统一处理不同语言的输入（如英文与中文），避免了多语言编码差异问题。</p>\n</li>\n<li>\n<p><strong>编解码的可逆性</strong>：<br>\n通过显式处理空格并用特殊符号“▁”（U+2581）转义，实现了简单且可逆的编解码流程。<br>\n示例公式：</p>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span>Decode(Encode(Normalized(text))) = Normalized(text)</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div></li>\n<li>\n<p><strong>无需预分词（Pre-tokenization）</strong>：<br>\n不需要对原始文本进行预处理，直接从原始数据中进行训练。</p>\n</li>\n</ul>\n<hr>\n<h3 id=\"_2-tokenizers库的编码流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-tokenizers库的编码流程\"><span><strong>2. Tokenizers库的编码流程</strong></span></a></h3>\n<p>Tokenizers库在文本分词时，采用以下管道化流程：</p>\n<h4 id=\"✅-normalization-标准化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#✅-normalization-标准化\"><span>✅ <strong>Normalization（标准化）</strong></span></a></h4>\n<p>对文本进行清理和格式化，包括：</p>\n<ul>\n<li>删除多余空格</li>\n<li>去除变音符号</li>\n<li>转换为小写</li>\n<li>Unicode正规化</li>\n</ul>\n<p>代码示例：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizers</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> NFD</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> StripAccents</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Lowercase</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 定义一个normalizer</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalizer </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">Sequence</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">([</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    NFD</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(),</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">           # Unicode正规化</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    StripAccents</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(),</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">  # 去除变音符号</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">    Lowercase</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">      # 转小写</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># 对字符串进行标准化</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> normalizer</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">normalize_str</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">Héllò hôw are ü?</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">print</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\">  # Output: 'hello how are u?'</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"⚠️-pre-tokenization-预分词\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#⚠️-pre-tokenization-预分词\"><span>⚠️ <strong>Pre-tokenization（预分词）</strong></span></a></h4>\n<p>将文本拆分为更小的单位，例如单词或子词。<br>\n示例：</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">from</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> tokenizers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenizers </span><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Whitespace</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenizer </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> Whitespace</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">()</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output </span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> pre_tokenizer</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">.</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">pre_tokenize_str</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">Hello! How are you? I'm fine, thank you.</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">print</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">output</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A0ADA0;--shiki-dark:#758575DD\"># Output: [('Hello', (0, 5)), ('!', (5, 6)), ('How', (7, 10)), ...]</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"❗️-模型分词与后处理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#❗️-模型分词与后处理\"><span>❗️ <strong>模型分词与后处理</strong></span></a></h4>\n<ul>\n<li>使用具体分词算法（如BPE）。</li>\n<li>添加特殊标记（如[CLS]、[SEP]）以满足特定任务需求。</li>\n</ul>\n<hr>\n<h3 id=\"_3-常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-常见错误与注意事项\"><span><strong>3. 常见错误与注意事项</strong></span></a></h3>\n<blockquote>\n<p>⚠️ <strong>常见错误</strong>：</p>\n<ul>\n<li>忽略文本标准化步骤可能导致编码不一致。</li>\n<li>未正确设置预分词器或模型参数会影响最终分词效果。</li>\n<li>对多语言文本未使用Unicode处理，可能导致字符编码问题。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"作者观点-vs-个人观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#作者观点-vs-个人观点\"><span><strong>作者观点 vs 个人观点</strong></span></a></h2>\n<table>\n<thead>\n<tr>\n<th><strong>作者观点</strong></th>\n<th><strong>个人观点</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SentencePiece无需预分词，适合从零开始训练</td>\n<td>SentencePiece对小型数据集可能不够高效</td>\n</tr>\n<tr>\n<td>Tokenizers库灵活可扩展，适合多任务应用</td>\n<td>Tokenizers库的学习曲线稍陡，需要良好编程基础</td>\n</tr>\n<tr>\n<td>多语言支持让工具更通用</td>\n<td>对低资源语言可能需要额外优化</td>\n</tr>\n</tbody>\n</table>\n<p>💡 启发点：</p>\n<ul>\n<li>分词工具的选择应根据具体任务需求和数据规模调整。</li>\n<li>编解码的可逆性是提升数据一致性的重要特性。</li>\n</ul>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span><strong>行动清单</strong></span></a></h2>\n<ol>\n<li>✅ 学习并实现SentencePiece的基本用法，尝试不同分词算法（如BPE）。</li>\n<li>⚠️ 使用Tokenizers库对多语言文本进行标准化和预分词。</li>\n<li>❗️ 对比不同分词工具在特定任务上的性能，记录实验结果。</li>\n</ol>\n<hr>\n<h2 id=\"📈-趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈-趋势预测\"><span>📈 <strong>趋势预测</strong></span></a></h2>\n<p>随着自然语言处理技术的普及，多语言和低资源语言的支持将成为分词工具的重要发展方向。未来可能会出现更多结合深度学习和传统方法的新型分词工具。</p>\n<hr>\n<h2 id=\"思考-延伸问题\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-延伸问题\"><span>[思考] 延伸问题</span></a></h2>\n<ol>\n<li>SentencePiece在处理混合语言（如中英混合）时表现如何？是否需要额外优化？</li>\n<li>Tokenizers库如何与主流深度学习框架（如TensorFlow或PyTorch）无缝集成？</li>\n<li>在数据量有限的情况下，是否有更适合的小型分词模型？</li>\n</ol>\n<hr>\n<blockquote>\n<p>引用来源：<a href=\"https://github.com/google/sentencepiece\" target=\"_blank\" rel=\"noopener noreferrer\">SentencePiece官方文档</a>、<a href=\"https://huggingface.co/docs/tokenizers/\" target=\"_blank\" rel=\"noopener noreferrer\">Tokenizers库文档</a></p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- **分类**：自然语言处理（NLP）\n- **标签**：分词算法、SentencePiece、Tokenizers库、NLP工具、多语言支持\n- **日期**：2025年4月2日  \n\n---\n\n\n## **核心内容总结**\n文本分词是自然语言处理（NLP）中的关键步骤，而`SentencePiece`和`Tokenizers库`提供了高效、灵活的解决方案。以下是两者的核心特点和功能总结：\n\n- **SentencePiece**：支持多种分词算法（如BPE、ULM），具备多语言支持、编解码可逆性，无需预分词，快速轻量。\n- **Tokenizers库**：提供完整的编码流程，包括数据标准化、预分词、分词模型和后处理，支持灵活定制。\n\n---\n\n\n## **重点内容**\n\n### **1. SentencePiece 的核心特点**\n- **多分词粒度支持**：  \n  支持BPE（Byte Pair Encoding）、ULM子词算法，以及字符级（char）和单词级（word）分词。\n  \n- **多语言兼容性**：  \n  使用Unicode编码字符，统一处理不同语言的输入（如英文与中文），避免了多语言编码差异问题。\n\n- **编解码的可逆性**：  \n  通过显式处理空格并用特殊符号“▁”（U+2581）转义，实现了简单且可逆的编解码流程。  \n  示例公式：  \n  ```\n  Decode(Encode(Normalized(text))) = Normalized(text)\n  ```\n\n- **无需预分词（Pre-tokenization）**：  \n  不需要对原始文本进行预处理，直接从原始数据中进行训练。\n\n---\n\n\n### **2. Tokenizers库的编码流程**\nTokenizers库在文本分词时，采用以下管道化流程：\n\n#### ✅ **Normalization（标准化）**\n对文本进行清理和格式化，包括：\n- 删除多余空格\n- 去除变音符号\n- 转换为小写\n- Unicode正规化\n\n代码示例：\n\n```python\nfrom tokenizers import normalizers\nfrom tokenizers.normalizers import NFD, StripAccents, Lowercase\n\n# 定义一个normalizer\nnormalizer = normalizers.Sequence([\n    NFD(),           # Unicode正规化\n    StripAccents(),  # 去除变音符号\n    Lowercase()      # 转小写\n])\n\n# 对字符串进行标准化\noutput = normalizer.normalize_str(\"Héllò hôw are ü?\")\nprint(output)  # Output: 'hello how are u?'\n```\n\n\n#### ⚠️ **Pre-tokenization（预分词）**\n将文本拆分为更小的单位，例如单词或子词。  \n示例：\n\n```python\nfrom tokenizers.pre_tokenizers import Whitespace\n\npre_tokenizer = Whitespace()\noutput = pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")\nprint(output)\n# Output: [('Hello', (0, 5)), ('!', (5, 6)), ('How', (7, 10)), ...]\n```\n\n\n#### ❗️ **模型分词与后处理**\n- 使用具体分词算法（如BPE）。\n- 添加特殊标记（如[CLS]、[SEP]）以满足特定任务需求。\n\n---\n\n\n### **3. 常见错误与注意事项**\n> ⚠️ **常见错误**：\n> - 忽略文本标准化步骤可能导致编码不一致。\n> - 未正确设置预分词器或模型参数会影响最终分词效果。\n> - 对多语言文本未使用Unicode处理，可能导致字符编码问题。\n\n---\n\n\n## **作者观点 vs 个人观点**\n| **作者观点** | **个人观点** |\n|--------------|--------------|\n| SentencePiece无需预分词，适合从零开始训练 | SentencePiece对小型数据集可能不够高效 |\n| Tokenizers库灵活可扩展，适合多任务应用 | Tokenizers库的学习曲线稍陡，需要良好编程基础 |\n| 多语言支持让工具更通用 | 对低资源语言可能需要额外优化 |\n\n💡 启发点：  \n- 分词工具的选择应根据具体任务需求和数据规模调整。  \n- 编解码的可逆性是提升数据一致性的重要特性。\n\n---\n\n\n## **行动清单**\n1. ✅ 学习并实现SentencePiece的基本用法，尝试不同分词算法（如BPE）。\n2. ⚠️ 使用Tokenizers库对多语言文本进行标准化和预分词。\n3. ❗️ 对比不同分词工具在特定任务上的性能，记录实验结果。\n\n---\n\n\n## 📈 **趋势预测**\n随着自然语言处理技术的普及，多语言和低资源语言的支持将成为分词工具的重要发展方向。未来可能会出现更多结合深度学习和传统方法的新型分词工具。\n\n---\n\n\n## [思考] 延伸问题\n1. SentencePiece在处理混合语言（如中英混合）时表现如何？是否需要额外优化？\n2. Tokenizers库如何与主流深度学习框架（如TensorFlow或PyTorch）无缝集成？\n3. 在数据量有限的情况下，是否有更适合的小型分词模型？\n\n---\n\n> 引用来源：[SentencePiece官方文档](https://github.com/google/sentencepiece)、[Tokenizers库文档](https://huggingface.co/docs/tokenizers/)","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心内容总结","slug":"核心内容总结","link":"#核心内容总结","children":[]},{"level":2,"title":"重点内容","slug":"重点内容","link":"#重点内容","children":[{"level":3,"title":"1. SentencePiece 的核心特点","slug":"_1-sentencepiece-的核心特点","link":"#_1-sentencepiece-的核心特点","children":[]},{"level":3,"title":"2. Tokenizers库的编码流程","slug":"_2-tokenizers库的编码流程","link":"#_2-tokenizers库的编码流程","children":[]},{"level":3,"title":"3. 常见错误与注意事项","slug":"_3-常见错误与注意事项","link":"#_3-常见错误与注意事项","children":[]}]},{"level":2,"title":"作者观点 vs 个人观点","slug":"作者观点-vs-个人观点","link":"#作者观点-vs-个人观点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈 趋势预测","slug":"📈-趋势预测","link":"#📈-趋势预测","children":[]},{"level":2,"title":"[思考] 延伸问题","slug":"思考-延伸问题","link":"#思考-延伸问题","children":[]}]}}
