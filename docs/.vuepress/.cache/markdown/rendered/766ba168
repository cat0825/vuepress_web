{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：GLM1, 自回归填空, Transformer, 预训练</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>GLM1是一种基于Transformer的语言模型，通过自回归填空任务实现高效的语言模型预训练。它采用prefix-decoder结构，并使用二维位置编码技术来增强模型的性能。\n<img src=\"/img/user/附件/Pasted image 20250425104630.png\" alt=\"Pasted image 20250425104630.png\"></p>\n<h3 id=\"模型结构与创新点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构与创新点\"><span>模型结构与创新点</span></a></h3>\n<p>GLM1使用了prefix-decoder结构，这实际上是Transformer的decoder部分，通过特殊的mask实现了文本的双向和单向attention。以下是GLM1模型的一些关键改动：</p>\n<ul>\n<li><strong>Pre Deep Norm</strong>：在模型中加入了深度归一化层。</li>\n<li><strong>线性层输出</strong>：使用单层线性层进行token预测。</li>\n<li><strong>激活函数</strong>：从ReLU切换到GeLU。</li>\n</ul>\n<p>💡启发点：GLM1通过自回归填空任务预训练语言模型，为条件生成和无条件生成任务提供了新的可能性。\n<img src=\"/img/user/附件/Pasted image 20250425104726.png\" alt=\"Pasted image 20250425104726.png\">做分类任务示例</p>\n<h3 id=\"自回归填空任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#自回归填空任务\"><span>自回归填空任务</span></a></h3>\n<p>自回归填空任务结合了自编码和自回归思想：</p>\n<ul>\n<li><strong>自编码思想</strong>：在输入文本中随机删除连续的tokens。</li>\n<li><strong>自回归思想</strong>：顺序重建连续tokens，模型可以访问损坏文本和之前预测的spans。\n<img src=\"/img/user/附件/Pasted image 20250425104657.png\" alt=\"Pasted image 20250425104657.png\">\n通过改变缺失spans的数量和长度，自回归填空目标可以为条件生成和无条件生成任务预训练语言模型。</li>\n</ul>\n<h3 id=\"二维位置编码技术\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#二维位置编码技术\"><span>二维位置编码技术</span></a></h3>\n<p>GLM1采用二维位置编码技术：</p>\n<ul>\n<li><strong>位置标记</strong>：第一个位置id标记Part A中的位置，第二个位置id表示span内部的相对位置。</li>\n<li><strong>嵌入投影</strong>：位置id通过embedding表投影为两个向量，加入到输入token的embedding表达中。</li>\n</ul>\n<h2 id=\"多任务预训练策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多任务预训练策略\"><span>多任务预训练策略</span></a></h2>\n<p>GLM1采用多任务预训练策略，以优化生成更长文本与空白填充目标：</p>\n<ul>\n<li><strong>Doc-level文档级</strong>：从原始长度的50% - 100%范围内均匀采样一个span，旨在生成长文本。</li>\n<li><strong>Sentence-level句子级</strong>：限制被mask的span必须是完整句子，采样多个span以覆盖原始token的15%。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 使用prefix-decoder结构进行文本处理。</li>\n<li>⚠ 确保二维位置编码技术正确应用。</li>\n<li>❗ 在多任务预训练中平衡文档级和句子级目标。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在应用自回归填空任务时，注意不要过度依赖损坏文本，否则可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究GLM1的应用场景，如条件生成任务。</li>\n<li>探索二维位置编码技术在其他模型中的应用。</li>\n<li>测试多任务预训练策略对不同数据集的影响。</li>\n</ul>\n<blockquote>\n<p>原始出处：[论文：GLM：General Language Model Pretraining with Autoregressive Blank Infilling]</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/GLM系列/GLM1.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/GLM系列/GLM1.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/GLM系列/GLM1","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/GLM系列/GLM1/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-25T02:45:38.000Z","updated":"2025-04-25T02:47:46.000Z","title":"GLM1","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：GLM1, 自回归填空, Transformer, 预训练</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>GLM1是一种基于Transformer的语言模型，通过自回归填空任务实现高效的语言模型预训练。它采用prefix-decoder结构，并使用二维位置编码技术来增强模型的性能。\n<img src=\"/img/user/附件/Pasted image 20250425104630.png\" alt=\"Pasted image 20250425104630.png\"></p>\n<h3 id=\"模型结构与创新点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构与创新点\"><span>模型结构与创新点</span></a></h3>\n<p>GLM1使用了prefix-decoder结构，这实际上是Transformer的decoder部分，通过特殊的mask实现了文本的双向和单向attention。以下是GLM1模型的一些关键改动：</p>\n<ul>\n<li><strong>Pre Deep Norm</strong>：在模型中加入了深度归一化层。</li>\n<li><strong>线性层输出</strong>：使用单层线性层进行token预测。</li>\n<li><strong>激活函数</strong>：从ReLU切换到GeLU。</li>\n</ul>\n<p>💡启发点：GLM1通过自回归填空任务预训练语言模型，为条件生成和无条件生成任务提供了新的可能性。\n<img src=\"/img/user/附件/Pasted image 20250425104726.png\" alt=\"Pasted image 20250425104726.png\">做分类任务示例</p>\n<h3 id=\"自回归填空任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#自回归填空任务\"><span>自回归填空任务</span></a></h3>\n<p>自回归填空任务结合了自编码和自回归思想：</p>\n<ul>\n<li><strong>自编码思想</strong>：在输入文本中随机删除连续的tokens。</li>\n<li><strong>自回归思想</strong>：顺序重建连续tokens，模型可以访问损坏文本和之前预测的spans。\n<img src=\"/img/user/附件/Pasted image 20250425104657.png\" alt=\"Pasted image 20250425104657.png\">\n通过改变缺失spans的数量和长度，自回归填空目标可以为条件生成和无条件生成任务预训练语言模型。</li>\n</ul>\n<h3 id=\"二维位置编码技术\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#二维位置编码技术\"><span>二维位置编码技术</span></a></h3>\n<p>GLM1采用二维位置编码技术：</p>\n<ul>\n<li><strong>位置标记</strong>：第一个位置id标记Part A中的位置，第二个位置id表示span内部的相对位置。</li>\n<li><strong>嵌入投影</strong>：位置id通过embedding表投影为两个向量，加入到输入token的embedding表达中。</li>\n</ul>\n<h2 id=\"多任务预训练策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多任务预训练策略\"><span>多任务预训练策略</span></a></h2>\n<p>GLM1采用多任务预训练策略，以优化生成更长文本与空白填充目标：</p>\n<ul>\n<li><strong>Doc-level文档级</strong>：从原始长度的50% - 100%范围内均匀采样一个span，旨在生成长文本。</li>\n<li><strong>Sentence-level句子级</strong>：限制被mask的span必须是完整句子，采样多个span以覆盖原始token的15%。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 使用prefix-decoder结构进行文本处理。</li>\n<li>⚠ 确保二维位置编码技术正确应用。</li>\n<li>❗ 在多任务预训练中平衡文档级和句子级目标。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在应用自回归填空任务时，注意不要过度依赖损坏文本，否则可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究GLM1的应用场景，如条件生成任务。</li>\n<li>探索二维位置编码技术在其他模型中的应用。</li>\n<li>测试多任务预训练策略对不同数据集的影响。</li>\n</ul>\n<blockquote>\n<p>原始出处：[论文：GLM：General Language Model Pretraining with Autoregressive Blank Infilling]</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：自然语言处理</li>\n<li>标签：GLM1, 自回归填空, Transformer, 预训练</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>GLM1是一种基于Transformer的语言模型，通过自回归填空任务实现高效的语言模型预训练。它采用prefix-decoder结构，并使用二维位置编码技术来增强模型的性能。\n<img src=\"/img/user/附件/Pasted image 20250425104630.png\" alt=\"Pasted image 20250425104630.png\"></p>\n<h3 id=\"模型结构与创新点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构与创新点\"><span>模型结构与创新点</span></a></h3>\n<p>GLM1使用了prefix-decoder结构，这实际上是Transformer的decoder部分，通过特殊的mask实现了文本的双向和单向attention。以下是GLM1模型的一些关键改动：</p>\n<ul>\n<li><strong>Pre Deep Norm</strong>：在模型中加入了深度归一化层。</li>\n<li><strong>线性层输出</strong>：使用单层线性层进行token预测。</li>\n<li><strong>激活函数</strong>：从ReLU切换到GeLU。</li>\n</ul>\n<p>💡启发点：GLM1通过自回归填空任务预训练语言模型，为条件生成和无条件生成任务提供了新的可能性。\n<img src=\"/img/user/附件/Pasted image 20250425104726.png\" alt=\"Pasted image 20250425104726.png\">做分类任务示例</p>\n<h3 id=\"自回归填空任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#自回归填空任务\"><span>自回归填空任务</span></a></h3>\n<p>自回归填空任务结合了自编码和自回归思想：</p>\n<ul>\n<li><strong>自编码思想</strong>：在输入文本中随机删除连续的tokens。</li>\n<li><strong>自回归思想</strong>：顺序重建连续tokens，模型可以访问损坏文本和之前预测的spans。\n<img src=\"/img/user/附件/Pasted image 20250425104657.png\" alt=\"Pasted image 20250425104657.png\">\n通过改变缺失spans的数量和长度，自回归填空目标可以为条件生成和无条件生成任务预训练语言模型。</li>\n</ul>\n<h3 id=\"二维位置编码技术\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#二维位置编码技术\"><span>二维位置编码技术</span></a></h3>\n<p>GLM1采用二维位置编码技术：</p>\n<ul>\n<li><strong>位置标记</strong>：第一个位置id标记Part A中的位置，第二个位置id表示span内部的相对位置。</li>\n<li><strong>嵌入投影</strong>：位置id通过embedding表投影为两个向量，加入到输入token的embedding表达中。</li>\n</ul>\n<h2 id=\"多任务预训练策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多任务预训练策略\"><span>多任务预训练策略</span></a></h2>\n<p>GLM1采用多任务预训练策略，以优化生成更长文本与空白填充目标：</p>\n<ul>\n<li><strong>Doc-level文档级</strong>：从原始长度的50% - 100%范围内均匀采样一个span，旨在生成长文本。</li>\n<li><strong>Sentence-level句子级</strong>：限制被mask的span必须是完整句子，采样多个span以覆盖原始token的15%。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 使用prefix-decoder结构进行文本处理。</li>\n<li>⚠ 确保二维位置编码技术正确应用。</li>\n<li>❗ 在多任务预训练中平衡文档级和句子级目标。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：在应用自回归填空任务时，注意不要过度依赖损坏文本，否则可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究GLM1的应用场景，如条件生成任务。</li>\n<li>探索二维位置编码技术在其他模型中的应用。</li>\n<li>测试多任务预训练策略对不同数据集的影响。</li>\n</ul>\n<blockquote>\n<p>原始出处：[论文：GLM：General Language Model Pretraining with Autoregressive Blank Infilling]</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：自然语言处理\n- 标签：GLM1, 自回归填空, Transformer, 预训练\n- 日期：2025年4月12日\n\n\n## 内容概述\nGLM1是一种基于Transformer的语言模型，通过自回归填空任务实现高效的语言模型预训练。它采用prefix-decoder结构，并使用二维位置编码技术来增强模型的性能。\n\t![Pasted image 20250425104630.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250425104630.png)\n\n### 模型结构与创新点\nGLM1使用了prefix-decoder结构，这实际上是Transformer的decoder部分，通过特殊的mask实现了文本的双向和单向attention。以下是GLM1模型的一些关键改动：\n\n- **Pre Deep Norm**：在模型中加入了深度归一化层。\n- **线性层输出**：使用单层线性层进行token预测。\n- **激活函数**：从ReLU切换到GeLU。\n\n💡启发点：GLM1通过自回归填空任务预训练语言模型，为条件生成和无条件生成任务提供了新的可能性。\n![Pasted image 20250425104726.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250425104726.png)做分类任务示例\n\n\n### 自回归填空任务\n自回归填空任务结合了自编码和自回归思想：\n\n- **自编码思想**：在输入文本中随机删除连续的tokens。\n- **自回归思想**：顺序重建连续tokens，模型可以访问损坏文本和之前预测的spans。\n![Pasted image 20250425104657.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250425104657.png)\n通过改变缺失spans的数量和长度，自回归填空目标可以为条件生成和无条件生成任务预训练语言模型。\n\n\n### 二维位置编码技术\nGLM1采用二维位置编码技术：\n\n- **位置标记**：第一个位置id标记Part A中的位置，第二个位置id表示span内部的相对位置。\n- **嵌入投影**：位置id通过embedding表投影为两个向量，加入到输入token的embedding表达中。\n\n\n## 多任务预训练策略\nGLM1采用多任务预训练策略，以优化生成更长文本与空白填充目标：\n\n- **Doc-level文档级**：从原始长度的50% - 100%范围内均匀采样一个span，旨在生成长文本。\n- **Sentence-level句子级**：限制被mask的span必须是完整句子，采样多个span以覆盖原始token的15%。\n\n\n## 操作步骤\n1. ✅ 使用prefix-decoder结构进行文本处理。\n2. ⚠ 确保二维位置编码技术正确应用。\n3. ❗ 在多任务预训练中平衡文档级和句子级目标。\n\n\n## 常见错误\n> 警告：在应用自回归填空任务时，注意不要过度依赖损坏文本，否则可能导致模型性能下降。\n\n\n## 行动清单\n- 研究GLM1的应用场景，如条件生成任务。\n- 探索二维位置编码技术在其他模型中的应用。\n- 测试多任务预训练策略对不同数据集的影响。\n\n> 原始出处：[论文：GLM：General Language Model Pretraining with Autoregressive Blank Infilling]","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容概述","slug":"内容概述","link":"#内容概述","children":[{"level":3,"title":"模型结构与创新点","slug":"模型结构与创新点","link":"#模型结构与创新点","children":[]},{"level":3,"title":"自回归填空任务","slug":"自回归填空任务","link":"#自回归填空任务","children":[]},{"level":3,"title":"二维位置编码技术","slug":"二维位置编码技术","link":"#二维位置编码技术","children":[]}]},{"level":2,"title":"多任务预训练策略","slug":"多任务预训练策略","link":"#多任务预训练策略","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
