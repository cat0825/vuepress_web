---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/分词/wordpiece","tags":["NLP"],"permalink":"/大语言模型学习/分词/wordpiece/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-03-27T10:20:58.168+08:00","updated":"2025-04-12T12:53:49.745+08:00"}
---



## 元数据
- **分类**：自然语言处理 (NLP)
- **标签**：WordPiece, 分词算法, 自然语言处理, NLP模型, Tokenization
- **日期**：2025年4月2日  

---



## WordPiece分词算法简介
WordPiece是一种常见的分词算法，广泛应用于自然语言处理任务中（如BERT模型）。其核心思想与BPE（Byte Pair Encoding）类似，但在合并子词时采用了基于互信息（Mutual Information）的策略，能更好地平衡词表大小和OOV（Out-Of-Vocabulary，未登录词）问题。

💡 **启发点**：通过互信息优化子词合并，提升了语言模型的表现力。

---



## 核心观点与实现步骤

### WordPiece的核心思想
- 与BPE类似，WordPiece从一个基础词表出发，通过不断合并子词生成最终的词表。
- 不同于BPE按频率选择合并对，WordPiece通过计算子词间的互信息来决定合并顺序。
- **互信息的公式**：  
  假设合并子词 `x` 和 `y` 后生成新子词 `z`，互信息得分计算如下：
  ```math
  score = P(x) * P(y) / P(z)
  ```
  其中，`P(x)` 表示子词 `x` 在语料中的出现频率。

---


### WordPiece的实现步骤
以下是WordPiece分词的主要步骤，用简单的符号和标记描述：

✅ **步骤1**：准备基础词表  
   - 包含26个英文字母及常见符号，如`a, b, c, @, #`等。

✅ **步骤2**：将语料拆分为最小单元  
   - 每个单词被拆分为基础字母或符号，例如`hello`被拆分为`h, e, l, l, o`。

✅ **步骤3**：训练语言模型  
   - 基于拆分后的数据，使用Unigram语言模型训练子词概率。

✅ **步骤4**：选择互信息最大的子词对合并  
   - 从所有可能的子词对中选择，使得合并后能最大程度提高语料的概率。

✅ **步骤5**：重复合并，直到满足条件  
   - 条件可以是达到预设的词表大小或概率增量低于某一阈值。

---


### 实现代码片段
以下是WordPiece中计算互信息得分的核心代码：

```python
# 计算子词对的互信息得分
scores = {
    pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
    for pair, freq in pair_freqs.items()
}
return scores
```

---



## 优缺点分析
| **优点**                             | **缺点**                                   |
|--------------------------------------|--------------------------------------------|
| 较好地平衡了词表大小和未登录词问题   | 可能产生不合理的子词切分                  |
| 子词间关联性强，提高语言模型表现      | 对拼写错误敏感                             |
| 支持高效的语言模型训练                | 对前缀处理效果不佳                        |

💡 **启发点**：可以通过改进前缀和复合词处理，进一步优化算法效果。

---



## 常见错误与注意事项
⚠️ **常见错误1**：忽略基础词表的重要性  
- 基础词表过小会导致过多无意义的子词生成。

⚠️ **常见错误2**：不合理设置阈值  
- 如果合并阈值过低，可能导致训练时间过长或出现低质量子词。

⚠️ **常见错误3**：未考虑语料质量  
- 拼写错误、噪声数据会显著影响分词效果。

---



## 思考与延伸问题
1. 如何在WordPiece中更好地处理拼写错误或前缀问题？
2. 是否可以结合BPE和WordPiece的优点创建新的分词算法？
3. 在多语言环境下，WordPiece是否需要特殊优化？

---

> **来源**：[论文《Fast WordPiece Tokenization》](https://arxiv.org/pdf/2012.15524)

---



## 行动清单
- [ ] 实现WordPiece分词算法，并测试不同语料下的效果。
- [ ] 比较BPE与WordPiece在OOV处理上的性能差异。
- [ ] 探索基于互信息优化的新型分词方法。

---



## 后续追踪计划
📈 **趋势预测**：随着NLP模型对多语言支持需求增加，更高效、更通用的分词算法将成为研究热点。  
📋 **研究计划**：
1. 开展对比实验，评估不同分词算法在实际任务中的表现。
2. 探索如何结合深度学习优化分词过程，例如用Transformer预测子词合并。
