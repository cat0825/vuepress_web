{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>：深度学习优化<br>\n<strong>标签</strong>：显存优化，梯度处理，Loss Scale，FP16<br>\n<strong>日期</strong>：2023-10-30</p>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>本文探讨了深度学习模型训练过程中显存占用的优化策略，特别是围绕 <strong>FP16</strong> 和 <strong>FP32</strong> 的显存使用，以及梯度处理中的 <strong>Loss Scale</strong> 和 <strong>梯度裁剪</strong> 方法。通过这些技术，可以在保证模型训练精度的同时，减少显存占用并提升计算效率。</p>\n<hr>\n<h2 id=\"主要内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#主要内容\"><span>主要内容</span></a></h2>\n<h3 id=\"显存占用与数据类型的影响\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存占用与数据类型的影响\"><span>显存占用与数据类型的影响</span></a></h3>\n<ul>\n<li>数据类型对模型参数大小的影响：\n<ul>\n<li><strong>FP16</strong>（16位浮点数）：占用 $$2\\Phi$$ 显存。</li>\n<li><strong>FP32</strong>（32位浮点数）：占用 $$4\\Phi$$ 显存。</li>\n<li>当梯度从 FP16 转为 FP32 时，如果不删除 FP16 梯度，会导致显存占用增加至 $$20\\Phi$$；否则为 $$18\\Phi$$。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：合理管理梯度存储可以有效控制显存开销。</p>\n<hr>\n<h3 id=\"loss-scale-的两种策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#loss-scale-的两种策略\"><span>Loss Scale 的两种策略</span></a></h3>\n<h4 id=\"_1-常量损失放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-常量损失放大\"><span>1. <strong>常量损失放大</strong></span></a></h4>\n<ul>\n<li><strong>流程</strong>：\n✅ 在反向传播时，将 Loss 值放大 $$2^{\\text{loss_scale}}$$ 倍，以 FP16 存储梯度。<br>\n✅ 在更新权重前，检查梯度是否溢出（Inf/Nan）。<br>\n⚠ 如果溢出，跳过当前权重更新；否则将梯度缩小 $$2^{\\text{loss_scale}}$$ 倍，并转为 FP32 更新权重。</li>\n</ul>\n<h4 id=\"_2-动量损失放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-动量损失放大\"><span>2. <strong>动量损失放大</strong></span></a></h4>\n<ul>\n<li><strong>流程</strong>：\n✅ 初期设置较大的 $$\\text{loss_scale}$$（如 $$2^{24}$$），并计算梯度。<br>\n✅ 检查溢出情况：若无溢出，将梯度恢复为 FP32 并更新权重；若溢出，缩小 $$\\text{loss_scale}$$（如减半）并跳过更新。<br>\n✅ 在训练后期，可以尝试逐步增大 $$\\text{loss_scale}$$，以适应稳定的梯度波动。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：动态调整 Loss Scale 是一种平衡数值稳定性和精度的有效方法。</p>\n<hr>\n<h3 id=\"梯度裁剪-clip-gradients\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#梯度裁剪-clip-gradients\"><span>梯度裁剪（Clip Gradients）</span></a></h3>\n<ul>\n<li><strong>方法</strong>：根据全量梯度向量的 L2 范数进行裁剪，以限制梯度范围，避免过大的更新。</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>J</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac><mo separator=\"true\">,</mo><mspace width=\"1em\"/><mtext>若 </mtext><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>g</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mo>&gt;</mo><mi>c</mi><mo separator=\"true\">,</mo><mtext> 则 </mtext><mi>g</mi><mo>=</mo><mi>c</mi><mo>⋅</mo><mfrac><mi>g</mi><mrow><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>g</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">g_i = \\frac{\\partial J(w)}{\\partial w_i}, \\quad \\text{若 } ||g|| &gt; c, \\text{ 则 } g = c \\cdot \\frac{g}{||g||}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.263em;vertical-align:-0.836em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.836em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:1em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">若</span><span class=\"mord\"> </span></span><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\">∣∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\"> </span><span class=\"mord cjk_fallback\">则</span><span class=\"mord\"> </span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4445em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.0436em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1076em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\">∣∣</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n其中，$$c$$ 是设定的裁剪阈值。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：梯度裁剪可以有效防止梯度爆炸问题。</p>\n<hr>\n<h3 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h3>\n<blockquote>\n<p>⚠ <strong>常见错误</strong>：</p>\n<ul>\n<li>未删除 FP16 梯度，导致显存占用过高。</li>\n<li>动态 Loss Scale 中未正确处理溢出情况，可能导致训练中断。</li>\n<li>梯度裁剪阈值设置不当，可能影响模型收敛速度。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考] 板块</span></a></h2>\n<ol>\n<li>动态 Loss Scale 策略如何在不同模型和任务中自动调节？</li>\n<li>梯度裁剪是否会对稀疏参数优化产生负面影响？</li>\n<li>是否可以结合混合精度训练和其他显存优化技术进一步提升效率？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原文出处：<a href=\"#\">深度学习显存优化与梯度处理</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实现 FP16 和 FP32 混合精度训练，观察显存变化。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 尝试动量式 Loss Scale 策略，并记录其对训练稳定性的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同梯度裁剪阈值对模型性能的作用。</label></li>\n</ul>\n<hr>\n<p>📈 <strong>趋势预测</strong>\n随着硬件性能的提升和更大规模模型的出现，混合精度训练和动态 Loss Scale 技术将成为主流。同时，自动化显存管理和优化工具可能进一步简化开发者的工作流程。</p>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索更智能的显存管理工具，例如 NVIDIA Apex 和 DeepSpeed。</li>\n<li>研究动态 Loss Scale 在大语言模型（LLM）中的应用效果。</li>\n</ul>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Pre-training 预训练/深度学习中的显存优化与梯度处理方法.md","filePathRelative":"notes_bak/大语言模型学习/Pre-training 预训练/深度学习中的显存优化与梯度处理方法.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/深度学习中的显存优化与梯度处理方法","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/深度学习中的显存优化与梯度处理方法/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-10T09:50:30.000Z","updated":"2025-04-13T05:06:02.000Z","title":"深度学习中的显存优化与梯度处理方法","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>：深度学习优化<br>\n<strong>标签</strong>：显存优化，梯度处理，Loss Scale，FP16<br>\n<strong>日期</strong>：2023-10-30</p>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>本文探讨了深度学习模型训练过程中显存占用的优化策略，特别是围绕 <strong>FP16</strong> 和 <strong>FP32</strong> 的显存使用，以及梯度处理中的 <strong>Loss Scale</strong> 和 <strong>梯度裁剪</strong> 方法。通过这些技术，可以在保证模型训练精度的同时，减少显存占用并提升计算效率。</p>\n<hr>\n<h2 id=\"主要内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#主要内容\"><span>主要内容</span></a></h2>\n<h3 id=\"显存占用与数据类型的影响\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存占用与数据类型的影响\"><span>显存占用与数据类型的影响</span></a></h3>\n<ul>\n<li>数据类型对模型参数大小的影响：\n<ul>\n<li><strong>FP16</strong>（16位浮点数）：占用 $$2\\Phi$$ 显存。</li>\n<li><strong>FP32</strong>（32位浮点数）：占用 $$4\\Phi$$ 显存。</li>\n<li>当梯度从 FP16 转为 FP32 时，如果不删除 FP16 梯度，会导致显存占用增加至 $$20\\Phi$$；否则为 $$18\\Phi$$。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：合理管理梯度存储可以有效控制显存开销。</p>\n<hr>\n<h3 id=\"loss-scale-的两种策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#loss-scale-的两种策略\"><span>Loss Scale 的两种策略</span></a></h3>\n<h4 id=\"_1-常量损失放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-常量损失放大\"><span>1. <strong>常量损失放大</strong></span></a></h4>\n<ul>\n<li><strong>流程</strong>：\n✅ 在反向传播时，将 Loss 值放大 $$2^{\\text{loss_scale}}$$ 倍，以 FP16 存储梯度。<br>\n✅ 在更新权重前，检查梯度是否溢出（Inf/Nan）。<br>\n⚠ 如果溢出，跳过当前权重更新；否则将梯度缩小 $$2^{\\text{loss_scale}}$$ 倍，并转为 FP32 更新权重。</li>\n</ul>\n<h4 id=\"_2-动量损失放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-动量损失放大\"><span>2. <strong>动量损失放大</strong></span></a></h4>\n<ul>\n<li><strong>流程</strong>：\n✅ 初期设置较大的 $$\\text{loss_scale}$$（如 $$2^{24}$$），并计算梯度。<br>\n✅ 检查溢出情况：若无溢出，将梯度恢复为 FP32 并更新权重；若溢出，缩小 $$\\text{loss_scale}$$（如减半）并跳过更新。<br>\n✅ 在训练后期，可以尝试逐步增大 $$\\text{loss_scale}$$，以适应稳定的梯度波动。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：动态调整 Loss Scale 是一种平衡数值稳定性和精度的有效方法。</p>\n<hr>\n<h3 id=\"梯度裁剪-clip-gradients\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#梯度裁剪-clip-gradients\"><span>梯度裁剪（Clip Gradients）</span></a></h3>\n<ul>\n<li><strong>方法</strong>：根据全量梯度向量的 L2 范数进行裁剪，以限制梯度范围，避免过大的更新。</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>J</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac><mo separator=\"true\">,</mo><mspace width=\"1em\"/><mtext>若 </mtext><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>g</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mo>&gt;</mo><mi>c</mi><mo separator=\"true\">,</mo><mtext> 则 </mtext><mi>g</mi><mo>=</mo><mi>c</mi><mo>⋅</mo><mfrac><mi>g</mi><mrow><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>g</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">g_i = \\frac{\\partial J(w)}{\\partial w_i}, \\quad \\text{若 } ||g|| &gt; c, \\text{ 则 } g = c \\cdot \\frac{g}{||g||}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.263em;vertical-align:-0.836em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.836em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:1em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">若</span><span class=\"mord\"> </span></span><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\">∣∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\"> </span><span class=\"mord cjk_fallback\">则</span><span class=\"mord\"> </span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4445em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.0436em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1076em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\">∣∣</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n其中，$$c$$ 是设定的裁剪阈值。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：梯度裁剪可以有效防止梯度爆炸问题。</p>\n<hr>\n<h3 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h3>\n<blockquote>\n<p>⚠ <strong>常见错误</strong>：</p>\n<ul>\n<li>未删除 FP16 梯度，导致显存占用过高。</li>\n<li>动态 Loss Scale 中未正确处理溢出情况，可能导致训练中断。</li>\n<li>梯度裁剪阈值设置不当，可能影响模型收敛速度。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考] 板块</span></a></h2>\n<ol>\n<li>动态 Loss Scale 策略如何在不同模型和任务中自动调节？</li>\n<li>梯度裁剪是否会对稀疏参数优化产生负面影响？</li>\n<li>是否可以结合混合精度训练和其他显存优化技术进一步提升效率？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原文出处：<a href=\"#\">深度学习显存优化与梯度处理</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实现 FP16 和 FP32 混合精度训练，观察显存变化。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 尝试动量式 Loss Scale 策略，并记录其对训练稳定性的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同梯度裁剪阈值对模型性能的作用。</label></li>\n</ul>\n<hr>\n<p>📈 <strong>趋势预测</strong>\n随着硬件性能的提升和更大规模模型的出现，混合精度训练和动态 Loss Scale 技术将成为主流。同时，自动化显存管理和优化工具可能进一步简化开发者的工作流程。</p>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索更智能的显存管理工具，例如 NVIDIA Apex 和 DeepSpeed。</li>\n<li>研究动态 Loss Scale 在大语言模型（LLM）中的应用效果。</li>\n</ul>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<p><strong>分类</strong>：深度学习优化<br>\n<strong>标签</strong>：显存优化，梯度处理，Loss Scale，FP16<br>\n<strong>日期</strong>：2023-10-30</p>\n<hr>\n<h2 id=\"核心内容总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心内容总结\"><span>核心内容总结</span></a></h2>\n<p>本文探讨了深度学习模型训练过程中显存占用的优化策略，特别是围绕 <strong>FP16</strong> 和 <strong>FP32</strong> 的显存使用，以及梯度处理中的 <strong>Loss Scale</strong> 和 <strong>梯度裁剪</strong> 方法。通过这些技术，可以在保证模型训练精度的同时，减少显存占用并提升计算效率。</p>\n<hr>\n<h2 id=\"主要内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#主要内容\"><span>主要内容</span></a></h2>\n<h3 id=\"显存占用与数据类型的影响\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#显存占用与数据类型的影响\"><span>显存占用与数据类型的影响</span></a></h3>\n<ul>\n<li>数据类型对模型参数大小的影响：\n<ul>\n<li><strong>FP16</strong>（16位浮点数）：占用 $$2\\Phi$$ 显存。</li>\n<li><strong>FP32</strong>（32位浮点数）：占用 $$4\\Phi$$ 显存。</li>\n<li>当梯度从 FP16 转为 FP32 时，如果不删除 FP16 梯度，会导致显存占用增加至 $$20\\Phi$$；否则为 $$18\\Phi$$。</li>\n</ul>\n</li>\n</ul>\n<p>💡 <strong>启发点</strong>：合理管理梯度存储可以有效控制显存开销。</p>\n<hr>\n<h3 id=\"loss-scale-的两种策略\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#loss-scale-的两种策略\"><span>Loss Scale 的两种策略</span></a></h3>\n<h4 id=\"_1-常量损失放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-常量损失放大\"><span>1. <strong>常量损失放大</strong></span></a></h4>\n<ul>\n<li><strong>流程</strong>：\n✅ 在反向传播时，将 Loss 值放大 $$2^{\\text{loss_scale}}$$ 倍，以 FP16 存储梯度。<br>\n✅ 在更新权重前，检查梯度是否溢出（Inf/Nan）。<br>\n⚠ 如果溢出，跳过当前权重更新；否则将梯度缩小 $$2^{\\text{loss_scale}}$$ 倍，并转为 FP32 更新权重。</li>\n</ul>\n<h4 id=\"_2-动量损失放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-动量损失放大\"><span>2. <strong>动量损失放大</strong></span></a></h4>\n<ul>\n<li><strong>流程</strong>：\n✅ 初期设置较大的 $$\\text{loss_scale}$$（如 $$2^{24}$$），并计算梯度。<br>\n✅ 检查溢出情况：若无溢出，将梯度恢复为 FP32 并更新权重；若溢出，缩小 $$\\text{loss_scale}$$（如减半）并跳过更新。<br>\n✅ 在训练后期，可以尝试逐步增大 $$\\text{loss_scale}$$，以适应稳定的梯度波动。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：动态调整 Loss Scale 是一种平衡数值稳定性和精度的有效方法。</p>\n<hr>\n<h3 id=\"梯度裁剪-clip-gradients\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#梯度裁剪-clip-gradients\"><span>梯度裁剪（Clip Gradients）</span></a></h3>\n<ul>\n<li><strong>方法</strong>：根据全量梯度向量的 L2 范数进行裁剪，以限制梯度范围，避免过大的更新。</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>J</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac><mo separator=\"true\">,</mo><mspace width=\"1em\"/><mtext>若 </mtext><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>g</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mo>&gt;</mo><mi>c</mi><mo separator=\"true\">,</mo><mtext> 则 </mtext><mi>g</mi><mo>=</mo><mi>c</mi><mo>⋅</mo><mfrac><mi>g</mi><mrow><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>g</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">g_i = \\frac{\\partial J(w)}{\\partial w_i}, \\quad \\text{若 } ||g|| &gt; c, \\text{ 则 } g = c \\cdot \\frac{g}{||g||}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.263em;vertical-align:-0.836em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.836em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:1em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord cjk_fallback\">若</span><span class=\"mord\"> </span></span><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\">∣∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\"> </span><span class=\"mord cjk_fallback\">则</span><span class=\"mord\"> </span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4445em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.0436em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1076em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\">∣∣</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n其中，$$c$$ 是设定的裁剪阈值。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：梯度裁剪可以有效防止梯度爆炸问题。</p>\n<hr>\n<h3 id=\"常见错误与注意事项\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误与注意事项\"><span>常见错误与注意事项</span></a></h3>\n<blockquote>\n<p>⚠ <strong>常见错误</strong>：</p>\n<ul>\n<li>未删除 FP16 梯度，导致显存占用过高。</li>\n<li>动态 Loss Scale 中未正确处理溢出情况，可能导致训练中断。</li>\n<li>梯度裁剪阈值设置不当，可能影响模型收敛速度。</li>\n</ul>\n</blockquote>\n<hr>\n<h2 id=\"思考-板块\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考-板块\"><span>[思考] 板块</span></a></h2>\n<ol>\n<li>动态 Loss Scale 策略如何在不同模型和任务中自动调节？</li>\n<li>梯度裁剪是否会对稀疏参数优化产生负面影响？</li>\n<li>是否可以结合混合精度训练和其他显存优化技术进一步提升效率？</li>\n</ol>\n<hr>\n<blockquote>\n<p>原文出处：<a href=\"#\">深度学习显存优化与梯度处理</a></p>\n</blockquote>\n<hr>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 实现 FP16 和 FP32 混合精度训练，观察显存变化。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 尝试动量式 Loss Scale 策略，并记录其对训练稳定性的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同梯度裁剪阈值对模型性能的作用。</label></li>\n</ul>\n<hr>\n<p>📈 <strong>趋势预测</strong>\n随着硬件性能的提升和更大规模模型的出现，混合精度训练和动态 Loss Scale 技术将成为主流。同时，自动化显存管理和优化工具可能进一步简化开发者的工作流程。</p>\n<hr>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索更智能的显存管理工具，例如 NVIDIA Apex 和 DeepSpeed。</li>\n<li>研究动态 Loss Scale 在大语言模型（LLM）中的应用效果。</li>\n</ul>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 元数据\n**分类**：深度学习优化  \n**标签**：显存优化，梯度处理，Loss Scale，FP16  \n**日期**：2023-10-30  \n\n---\n\n\n\n## 核心内容总结\n本文探讨了深度学习模型训练过程中显存占用的优化策略，特别是围绕 **FP16** 和 **FP32** 的显存使用，以及梯度处理中的 **Loss Scale** 和 **梯度裁剪** 方法。通过这些技术，可以在保证模型训练精度的同时，减少显存占用并提升计算效率。\n\n---\n\n\n\n## 主要内容\n\n### 显存占用与数据类型的影响\n- 数据类型对模型参数大小的影响：\n  - **FP16**（16位浮点数）：占用 $$2\\Phi$$ 显存。\n  - **FP32**（32位浮点数）：占用 $$4\\Phi$$ 显存。\n  - 当梯度从 FP16 转为 FP32 时，如果不删除 FP16 梯度，会导致显存占用增加至 $$20\\Phi$$；否则为 $$18\\Phi$$。\n\n💡 **启发点**：合理管理梯度存储可以有效控制显存开销。\n\n---\n\n\n### Loss Scale 的两种策略\n\n#### 1. **常量损失放大**\n- **流程**：\n  ✅ 在反向传播时，将 Loss 值放大 $$2^{\\text{loss\\_scale}}$$ 倍，以 FP16 存储梯度。  \n  ✅ 在更新权重前，检查梯度是否溢出（Inf/Nan）。  \n  ⚠ 如果溢出，跳过当前权重更新；否则将梯度缩小 $$2^{\\text{loss\\_scale}}$$ 倍，并转为 FP32 更新权重。\n\n\n#### 2. **动量损失放大**\n- **流程**：\n  ✅ 初期设置较大的 $$\\text{loss\\_scale}$$（如 $$2^{24}$$），并计算梯度。  \n  ✅ 检查溢出情况：若无溢出，将梯度恢复为 FP32 并更新权重；若溢出，缩小 $$\\text{loss\\_scale}$$（如减半）并跳过更新。  \n  ✅ 在训练后期，可以尝试逐步增大 $$\\text{loss\\_scale}$$，以适应稳定的梯度波动。\n\n💡 **启发点**：动态调整 Loss Scale 是一种平衡数值稳定性和精度的有效方法。\n\n---\n\n\n### 梯度裁剪（Clip Gradients）\n- **方法**：根据全量梯度向量的 L2 范数进行裁剪，以限制梯度范围，避免过大的更新。  \n- **公式**：\n  $$\n  g_i = \\frac{\\partial J(w)}{\\partial w_i}, \\quad \\text{若 } ||g|| > c, \\text{ 则 } g = c \\cdot \\frac{g}{||g||}\n  $$\n  其中，$$c$$ 是设定的裁剪阈值。\n\n💡 **启发点**：梯度裁剪可以有效防止梯度爆炸问题。\n\n---\n\n\n### 常见错误与注意事项\n> ⚠ **常见错误**：\n> - 未删除 FP16 梯度，导致显存占用过高。\n> - 动态 Loss Scale 中未正确处理溢出情况，可能导致训练中断。\n> - 梯度裁剪阈值设置不当，可能影响模型收敛速度。\n\n---\n\n\n\n## [思考] 板块\n1. 动态 Loss Scale 策略如何在不同模型和任务中自动调节？\n2. 梯度裁剪是否会对稀疏参数优化产生负面影响？\n3. 是否可以结合混合精度训练和其他显存优化技术进一步提升效率？\n\n---\n\n> 原文出处：[深度学习显存优化与梯度处理](#)\n\n---\n\n\n\n## 行动清单\n- [ ] 实现 FP16 和 FP32 混合精度训练，观察显存变化。\n- [ ] 尝试动量式 Loss Scale 策略，并记录其对训练稳定性的影响。\n- [ ] 测试不同梯度裁剪阈值对模型性能的作用。\n\n---\n\n📈 **趋势预测**\n随着硬件性能的提升和更大规模模型的出现，混合精度训练和动态 Loss Scale 技术将成为主流。同时，自动化显存管理和优化工具可能进一步简化开发者的工作流程。\n\n---\n\n\n\n## 后续追踪\n- 探索更智能的显存管理工具，例如 NVIDIA Apex 和 DeepSpeed。\n- 研究动态 Loss Scale 在大语言模型（LLM）中的应用效果。","excerpt":"","includedFiles":[],"tasklistId":3,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"核心内容总结","slug":"核心内容总结","link":"#核心内容总结","children":[]},{"level":2,"title":"主要内容","slug":"主要内容","link":"#主要内容","children":[{"level":3,"title":"显存占用与数据类型的影响","slug":"显存占用与数据类型的影响","link":"#显存占用与数据类型的影响","children":[]},{"level":3,"title":"Loss Scale 的两种策略","slug":"loss-scale-的两种策略","link":"#loss-scale-的两种策略","children":[]},{"level":3,"title":"梯度裁剪（Clip Gradients）","slug":"梯度裁剪-clip-gradients","link":"#梯度裁剪-clip-gradients","children":[]},{"level":3,"title":"常见错误与注意事项","slug":"常见错误与注意事项","link":"#常见错误与注意事项","children":[]}]},{"level":2,"title":"[思考] 板块","slug":"思考-板块","link":"#思考-板块","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
