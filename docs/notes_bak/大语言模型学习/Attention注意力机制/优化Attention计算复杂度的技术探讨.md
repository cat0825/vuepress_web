---
dg-publish: true
dg-permalink: /å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ /Attentionæ³¨æ„åŠ›æœºåˆ¶/ä¼˜åŒ–Attentionè®¡ç®—å¤æ‚åº¦çš„æŠ€æœ¯æ¢è®¨
dg-home: false
dg-description: åœ¨æ­¤è¾“å…¥ç¬”è®°çš„æè¿°
dg-hide: false
dg-hide-title: false
dg-show-backlinks: true
dg-show-local-graph: true
dg-show-inline-title: true
dg-pinned: false
dg-passphrase: åœ¨æ­¤è¾“å…¥è®¿é—®å¯†ç 
dg-enable-mathjax: false
dg-enable-mermaid: false
dg-enable-uml: false
dg-note-icon: 0
dg-enable-dataview: false
tags:
  - NLP
permalink: /å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ /Attentionæ³¨æ„åŠ›æœºåˆ¶/ä¼˜åŒ–Attentionè®¡ç®—å¤æ‚åº¦çš„æŠ€æœ¯æ¢è®¨/
dgShowBacklinks: true
dgShowLocalGraph: true
dgShowInlineTitle: true
dgPassFrontmatter: true
noteIcon: 0
created: 2025-04-03T22:46:39.000+08:00
updated: 2025-04-13T13:06:02.000+08:00
title: ä¼˜åŒ–Attentionè®¡ç®—å¤æ‚åº¦çš„æŠ€æœ¯æ¢è®¨
createTime: 2025/05/13 17:33:53
---



## å…ƒæ•°æ®
- **åˆ†ç±»**ï¼šæ·±åº¦å­¦ä¹ /Transformerä¼˜åŒ–
- **æ ‡ç­¾**ï¼šAttentionæœºåˆ¶ã€è®¡ç®—å¤æ‚åº¦ã€Sparse Attentionã€Linear Attention
- **æ—¥æœŸ**ï¼š2024å¹´10æœˆ2æ—¥  

---



## å†…å®¹æ¦‚è¿°
æœ¬æ–‡æ¢è®¨äº†ä¼˜åŒ–Attentionè®¡ç®—å¤æ‚åº¦çš„å‡ ç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬Sparse Attentionå’ŒLinear Attentionã€‚æ ¸å¿ƒç›®æ ‡æ˜¯é™ä½ä¼ ç»ŸSelf Attentionçš„è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™å…¶åœ¨åºåˆ—æ•°æ®å¤„ç†ä¸­çš„å¼ºå¤§åŠŸèƒ½ã€‚

---



## æ ¸å¿ƒå†…å®¹

### Self Attentionçš„è®¡ç®—å¤æ‚åº¦é—®é¢˜
ä¼ ç»ŸSelf Attentionçš„è®¡ç®—å¤æ‚åº¦ä¸º \(O(N^2)\)ï¼Œéœ€è¦å¯¹åºåˆ—ä¸­çš„ä»»æ„ä¸¤ä¸ªå‘é‡è®¡ç®—ç›¸å…³æ€§ï¼Œç”Ÿæˆä¸€ä¸ª \(N \times N\) çš„ç›¸å…³åº¦çŸ©é˜µã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†é•¿åºåˆ—æ—¶ä¼šå¯¼è‡´è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚


### Sparse Attentionï¼šå±€éƒ¨ä¸è¿œç¨‹ç¨€ç–ç›¸å…³
Sparse Attentioné€šè¿‡é™åˆ¶æ³¨æ„åŠ›çŸ©é˜µä¸­éƒ¨åˆ†åŒºåŸŸçš„è®¡ç®—æ¥é™ä½å¤æ‚åº¦ï¼š
- **ä¸»è¦åŸç†**ï¼šç»“åˆç©ºæ´Attentionå’Œå±€éƒ¨Attentionï¼Œè®¾ç½®ç›¸å¯¹è·ç¦»è¶…è¿‡kæˆ–ä¸ºkçš„å€æ•°çš„æ³¨æ„åŠ›ä¸º0ã€‚
- **ä¼˜ç‚¹**ï¼šæå‡æ•ˆç‡ï¼Œé€‚åˆå¤§å¤šæ•°åªéœ€å±€éƒ¨ç´§å¯†ç›¸å…³æ€§çš„ä»»åŠ¡ã€‚
- **ä¸è¶³**ï¼š
  1. ä¿ç•™åŒºåŸŸéœ€äººå·¥é€‰æ‹©ï¼Œç¼ºä¹çµæ´»æ€§ã€‚
  2. å®ç°éœ€è¦ç‰¹å®šä¼˜åŒ–è®¾è®¡ï¼Œä¸æ˜“æ¨å¹¿ã€‚

ğŸ“ˆ**è¶‹åŠ¿é¢„æµ‹**ï¼šSparse Attentionå¯èƒ½æˆä¸ºç‰¹å®šä»»åŠ¡çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†éœ€è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•åŠ¨æ€é€‰æ‹©æ³¨æ„åŠ›åŒºåŸŸã€‚


### Linear Attentionï¼šä»å¹³æ–¹å¤æ‚åº¦åˆ°çº¿æ€§å¤æ‚åº¦
Linear Attentioné€šè¿‡ç§»é™¤Softmaxæ“ä½œï¼Œå°†è®¡ç®—å¤æ‚åº¦ä» \(O(N^2d)\) é™è‡³ \(O(Nd^2)\)ï¼š
- **æ ¸å¿ƒæ€æƒ³**ï¼šå…ˆè®¡ç®— \(K^T \cdot V\)ï¼Œå†ç»“åˆæ ¸å‡½æ•°å½¢å¼å¤„ç† \(Q \cdot K^T\)ï¼Œä»¥éè´Ÿæ¿€æ´»å‡½æ•°æ›¿ä»£Softmaxã€‚
- **å®ç°æ–¹å¼**ï¼š
  ```python
  def linear_attn(q, k, v, kv_mask=None):
      dim = q.shape[-1]
      if exists(kv_mask):
          mask_value = max_neg_value(q)
          mask = kv_mask[:, None, :, None]
          k = k.masked_fill(mask == 0, mask_value)
      kv = torch.einsum('bnd,bne->bde', k, v)
      q = torch.softmax(q, dim=-1)
      return torch.einsum('bnd,bde->bne', q, kv)
  ```

ğŸ’¡**å¯å‘ç‚¹**ï¼šæ ¸å‡½æ•°å½¢å¼çš„Attentionæœºåˆ¶åœ¨CVé¢†åŸŸå·²æœ‰åº”ç”¨ï¼Œæœªæ¥å¯ä»¥æ¢ç´¢æ›´å¤šåœºæ™¯é€‚é…ã€‚

---



## å¸¸è§é”™è¯¯
âš ï¸ **è¯¯åŒºè­¦å‘Š**ï¼š
1. **å¿½è§†ä»»åŠ¡éœ€æ±‚**ï¼šå¹¶éæ‰€æœ‰ä»»åŠ¡éƒ½é€‚åˆç¨€ç–æˆ–çº¿æ€§Attentionï¼Œéœ€ç»“åˆå…·ä½“åœºæ™¯é€‰æ‹©ã€‚
2. **å®ç°ä»£ç æ•ˆç‡ä½ä¸‹**ï¼šæœªä¼˜åŒ–çŸ©é˜µè¿ç®—å¯èƒ½å¯¼è‡´æ€§èƒ½åè€Œä¸‹é™ã€‚

---



## æ€è€ƒä¸å»¶ä¼¸é—®é¢˜
[æ€è€ƒ]  
1. å¦‚ä½•è®¾è®¡åŠ¨æ€é€‰æ‹©æ³¨æ„åŠ›åŒºåŸŸçš„æœºåˆ¶ï¼Œä½¿Sparse Attentionæ›´æ™ºèƒ½åŒ–ï¼Ÿ
2. Linear Attentionæ˜¯å¦é€‚åˆæ‰€æœ‰é•¿åºåˆ—ä»»åŠ¡ï¼Œæ˜¯å¦å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Ÿ
3. æ ¸å‡½æ•°å½¢å¼çš„Attentionèƒ½å¦åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸè¿›ä¸€æ­¥æ¨å¹¿ï¼Ÿ

---



## ä½œè€…è§‚ç‚¹ vs ä¸ªäººè§‚ç‚¹
| **ä½œè€…è§‚ç‚¹**                          | **ä¸ªäººè§‚ç‚¹**                           |
|---------------------------------------|----------------------------------------|
| Sparse Attentioné€‚åˆå±€éƒ¨ç›¸å…³æ€§ä»»åŠ¡    | åŠ¨æ€é€‰æ‹©æ³¨æ„åŠ›åŒºåŸŸæ˜¯æœªæ¥ç ”ç©¶æ–¹å‘       |
| Linear Attentionæ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦    | æ ¸å‡½æ•°å½¢å¼å€¼å¾—æ¢ç´¢æ›´å¹¿æ³›åº”ç”¨åœºæ™¯       |
| å®ç°éœ€ä¼˜åŒ–çŸ©é˜µè¿ç®—ä»¥æå‡æ•ˆç‡          | é«˜æ•ˆå®ç°æ˜¯æŠ€æœ¯æ¨å¹¿çš„å…³é”®               |

---



## è¡ŒåŠ¨æ¸…å•
1. âœ… æ·±å…¥ç ”ç©¶Sparse Attentionçš„åŠ¨æ€ä¼˜åŒ–æ–¹æ³•ã€‚
2. âš ï¸ æµ‹è¯•Linear Attentionåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚
3. â—ï¸ æ¢ç´¢æ ¸å‡½æ•°å½¢å¼åœ¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚è¯­éŸ³å¤„ç†ï¼‰çš„åº”ç”¨ã€‚

---



## æ•°æ®è¡¨æ ¼
| æŠ€æœ¯åç§°         | å¤æ‚åº¦ä¼˜åŒ–       | ä¼˜ç¼ºç‚¹                          |
|------------------|------------------|---------------------------------|
| Self Attention   | \(O(N^2)\)       | é«˜æˆæœ¬ï¼Œä½†æ•ˆæœå¼º                |
| Sparse Attention | é™ä½éƒ¨åˆ†åŒºåŸŸè®¡ç®— | é«˜æ•ˆä½†éœ€äººå·¥é€‰æ‹©ä¿ç•™åŒºåŸŸ         |
| Linear Attention | \(O(Nd^2)\)      | æ¥è¿‘çº¿æ€§ï¼Œé€‚åˆé•¿åºåˆ—ä»»åŠ¡         |

---



## åç»­è¿½è¸ªç ”ç©¶è®¡åˆ’
1. **Sparse Attentionæ™ºèƒ½åŒ–**ï¼šå¼€å‘åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›åŒºåŸŸçš„ç®—æ³•ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚
2. **Linear Attentionæ‰©å±•**ï¼šæµ‹è¯•æ ¸å‡½æ•°å½¢å¼åœ¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚å›¾åƒåˆ†å‰²ã€è¯­éŸ³è¯†åˆ«ï¼‰çš„è¡¨ç°ã€‚
3. **æ··åˆæ¨¡å‹æ¢ç´¢**ï¼šç»“åˆSparseå’ŒLinear Attentionï¼Œè®¾è®¡æ›´é«˜æ•ˆçš„æ··åˆæ¨¡å‹ã€‚

---

> æ¥æºï¼šæœ¬æ–‡å†…å®¹æ”¹ç¼–è‡ªåŸå§‹æŠ€æœ¯æ¢è®¨ï¼Œå®Œæ•´ä»£ç ä¸å®ç°å¯å‚è€ƒ [GitHub](https://github.com/lucidrains/linear-attention-transformer)ã€‚
