<template><div><h2 id="首token时延" tabindex="-1"><a class="header-anchor" href="#首token时延"><span>首Token时延</span></a></h2>
<p>在LLM（大型语言模型）推理过程中，生成首token是一个计算密集型任务。生成首token阶段也被称为预填充阶段（prefill phase）或上下文阶段（context phase）。生成首token的时间与处理输入给大模型的Prompt的计算量有关，与Prompt长度直接相关。例如，在Prompt长度相对较长的情况下，再考虑到技术优化，使用FlashAttention2生成首token的时间与输入Prompt的长度近似成线性关系。</p>
<h3 id="首个token的推理延迟" tabindex="-1"><a class="header-anchor" href="#首个token的推理延迟"><span>首个token的推理延迟</span></a></h3>
<p>首个token的推理延迟可以表示为：</p>
<p v-pre class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>首个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU半精度浮点算力</mtext><mtext>模型浮点计算量 (FLOPs)</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{首个token的推理延迟} \geq \frac{\text{GPU半精度浮点算力}}{\text{模型浮点计算量 (FLOPs)}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8304em;vertical-align:-0.136em;"></span><span class="mord text"><span class="mord cjk_fallback">首个</span><span class="mord">token</span><span class="mord cjk_fallback">的推理延迟</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2963em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">模型浮点计算量</span><span class="mord"> (FLOPs)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">GPU</span><span class="mord cjk_fallback">半精度浮点算力</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<h3 id="后续每个token的推理延迟" tabindex="-1"><a class="header-anchor" href="#后续每个token的推理延迟"><span>后续每个token的推理延迟</span></a></h3>
<p>对于后续每个token的推理延迟，可以表示为：</p>
<p v-pre class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>后续每个token的推理延迟</mtext><mo>≥</mo><mfrac><mtext>GPU HBM带宽</mtext><mtext>模型参数量 (字节数)</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{后续每个token的推理延迟} \geq \frac{\text{GPU HBM带宽}}{\text{模型参数量 (字节数)}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8304em;vertical-align:-0.136em;"></span><span class="mord text"><span class="mord cjk_fallback">后续每个</span><span class="mord">token</span><span class="mord cjk_fallback">的推理延迟</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2963em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">模型参数量</span><span class="mord"> (</span><span class="mord cjk_fallback">字节数</span><span class="mord">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">GPU HBM</span><span class="mord cjk_fallback">带宽</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<h2 id="优化system-prompt" tabindex="-1"><a class="header-anchor" href="#优化system-prompt"><span>优化System Prompt</span></a></h2>
<p>System Prompt Caching的基本思想是对System Prompt部分进行一次计算，并缓存其对应的Key和Value值（例如，存放在GPU显存中）。当LLM推理再次遇到相同的（甚至部分相同的）System Prompt时，可以直接利用已经缓存的System Prompt对应的Key和Value值，这样就避免了对于System Prompt的重复计算。</p>
<h3 id="第一种形式-prefix-sharing" tabindex="-1"><a class="header-anchor" href="#第一种形式-prefix-sharing"><span>第一种形式：Prefix Sharing</span></a></h3>
<p>Prefix Sharing适用于“Prompt = System Prompt + User Prompt”这样的场景，其中System Prompt就是前缀（Prefix）。
<img src="/img/user/附件/Pasted image 20250430224328.png" alt="Pasted image 20250430224328.png"></p>
<h3 id="第二种形式-prompt-cache" tabindex="-1"><a class="header-anchor" href="#第二种形式-prompt-cache"><span>第二种形式：Prompt Cache</span></a></h3>
<p><img src="/img/user/附件/Pasted image 20250430224338.png" alt="Pasted image 20250430224338.png">
Prompt Cache属于相对高级的用法，是对整个输入Prompt对应的Key和Value值进行Caching操作，不局限于共享前缀。</p>
<p>特别地，对于多轮对话场景，以及基于LLM的AI Agent应用场景，上述第二种方式，即Prompt Cache，可以支持Session Prompt Cache。在一个多轮对话session里，输入到LLM的Prompt会携带多轮对话历史，涉及到很多重复计算。通过Session Prompt Cache可以显著减少不必要的重复计算，节省GPU资源，提高对话响应速度和用户体验。</p>
<p>通过对首Token时延和System Prompt Caching的优化，可以有效提高LLM推理的效率，降低计算资源消耗，从而提升用户体验。这些技术在实际应用中具有重要价值，特别是在涉及大量文本处理和生成任务的场景中。</p>
</div></template>


