---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/训练推理优化/FlashAttention/介绍","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/训练推理优化/FlashAttention/介绍/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-28T22:19:52.000+08:00","updated":"2025-04-29T11:00:58.000+08:00"}
---



# FlashAttention：加速与节省显存的精确注意力机制
元数据：

分类：人工智能技术

标签：FlashAttention, Transformer, 计算优化, 显存节省, IO感知

日期：2025年4月12日

## 核心观点总结
FlashAttention提出了一种创新的注意力机制，通过加速计算和节省显存来优化Transformer模型。它的设计旨在解决随着序列长度 $N$ 的二次增长所带来的资源和效率问题。与传统注意力机制不同，FlashAttention在保持结果精确的同时减少了显存复杂度和计算时间。


## 重点段落与数据

### 加速计算（Fast）
- FlashAttention通过IO感知减少HBM访问次数来加快计算速度，而不是减少计算量（FLOPs）。
- 使用了分块技术（tiling）和算子融合来实现这一目标。


### 显存节省（Memory-efficient）
- 通过引入统计量，改变注意力机制的计算顺序，避免实例化注意力矩阵。
- 显存复杂度从 $O(N^2)$ 降低到了 $O(N)$。


### 精确注意力（Exact Attention）
- FlashAttention与原生注意力的结果完全等价，不同于稀疏注意力，它只是分块计算，而不是近似计算。


## 技术术语通俗解释
- **IO感知**：指的是在计算过程中优化输入输出操作以减少延迟。
- **HBM访问**：指的是高带宽内存的访问次数，减少这些访问可以提高计算效率。
- **分块技术（Tiling）**：将大任务分成小块来处理，以提高效率。
- **算子融合**：将多个计算步骤合并为一个，以减少中间步骤和内存使用。


## 操作步骤
1. ✅ 使用分块技术进行计算任务分割。
2. ⚠ 避免实例化完整的注意力矩阵。
3. ❗ 引入统计量以优化计算顺序。


## 常见错误
> 注意避免在实现过程中忽略了IO感知的重要性，这可能导致计算效率未能达到预期。


## 💡 启发点
FlashAttention的创新在于它通过改变计算顺序和数据处理方式来优化资源使用，而不是简单地减少计算量。这种方法为其他领域的优化提供了新的思路。


## 行动清单
- 研究FlashAttention在不同模型中的应用效果。
- 探索分块技术如何在其他计算任务中应用。
- 分析显存节省对大型模型训练的影响。

> 来源：论文《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》及相关知乎文章。(https://zhuanlan.zhihu.com/p/669926191 ， https://zhuanlan.zhihu.com/p/676655352 ， https://zhuanlan.zhihu.com/p/663932651)
