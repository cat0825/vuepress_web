{"content":"<p><strong>分类</strong>：人工智能</p>\n<p><strong>标签</strong>：预训练评估、困惑度、Benchmark</p>\n<p><strong>日期</strong>：2023年10月20日</p>\n<h2 id=\"预训练评估的核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练评估的核心观点\"><span>预训练评估的核心观点</span></a></h2>\n<p>预训练评估是大语言模型（LLM）全链路评估中较为简单的环节，主要关注模型的知识掌握程度，而非指令跟随能力或安全性等。</p>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"困惑度-ppl-测量\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#困惑度-ppl-测量\"><span>困惑度（PPL）测量</span></a></h3>\n<ul>\n<li><strong>数据准备</strong>：使用百科、逻辑、代码等数据集。</li>\n<li><strong>观察趋势</strong>：每日观察模型在这些集合上的损失（loss）表现，正常情况下损失会逐渐下降并趋于稳定。</li>\n<li><strong>模型对比</strong>：困惑度只能在同一模型的不同版本之间进行比较，因为不同的tokenizer压缩率会影响loss的可比性。</li>\n</ul>\n<h3 id=\"benchmark评估\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#benchmark评估\"><span>Benchmark评估</span></a></h3>\n<p>推荐使用开源平台OpenCompass进行Benchmark评估。以下是一些常用的Benchmark：</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>用途</th>\n<th>数据地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MMLU</td>\n<td>评估广泛主题领域的理解和推理能力</td>\n<td><a href=\"https://github.com/hendrycks/test\" target=\"_blank\" rel=\"noopener noreferrer\">MMLU数据集</a></td>\n</tr>\n<tr>\n<td>GLUE</td>\n<td>全面评估语言理解能力</td>\n<td><a href=\"https://huggingface.co/datasets/nyu-mll/glue\" target=\"_blank\" rel=\"noopener noreferrer\">GLUE数据集</a></td>\n</tr>\n<tr>\n<td>MultiNLI</td>\n<td>评估根据陈述推理正确类别的能力</td>\n<td><a href=\"https://huggingface.co/datasets/multi_nli\" target=\"_blank\" rel=\"noopener noreferrer\">MultiNLI数据集</a></td>\n</tr>\n<tr>\n<td>SuperGLUE</td>\n<td>评估语言理解和推理的更深层次</td>\n<td><a href=\"https://huggingface.co/datasets/super_glue\" target=\"_blank\" rel=\"noopener noreferrer\">SuperGLUE数据集</a></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"技术术语解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语解释\"><span>技术术语解释</span></a></h3>\n<ul>\n<li><strong>困惑度（PPL）</strong>：衡量模型预测下一个词的难易程度，数值越低表示模型预测越准确。</li>\n<li><strong>Benchmark</strong>：用于评估模型性能的标准化测试集。</li>\n</ul>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>如何在不同的领域中优化LLM的知识掌握？</li>\n<li>在困惑度下降趋于稳定后，还有哪些优化空间？</li>\n<li>Benchmark测试结果如何反映在实际应用中？</li>\n</ul>\n<blockquote>\n<p>来源：预训练评估文档</p>\n</blockquote>\n<hr>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 准备数据集，包括百科、逻辑和代码。</li>\n<li>⚠ 每日观察测试集合上的loss表现。</li>\n<li>❗ 使用OpenCompass进行Benchmark评估。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：不同模型之间直接比较困惑度可能导致误解，因为tokenizer压缩率不同。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用多种Benchmark可以全面评估模型的各方面能力。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>继续优化模型以降低困惑度。</li>\n<li>扩展Benchmark测试以涵盖更多领域。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，随着更复杂的数据集和更高效的算法，模型的知识掌握能力将进一步提升。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索更多领域特定的Benchmark。</li>\n<li>研究不同tokenizer对困惑度的影响。\n[[大语言模型学习/Pre-training 预训练/预训练评估2|预训练评估2]]</li>\n</ul>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Pre-training 预训练/预训练评估.md","filePathRelative":"notes_bak/大语言模型学习/Pre-training 预训练/预训练评估.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/预训练评估","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/预训练评估/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-10T13:49:30.000Z","updated":"2025-04-13T05:06:02.000Z","title":"预训练评估","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><p><strong>分类</strong>：人工智能</p>\n<p><strong>标签</strong>：预训练评估、困惑度、Benchmark</p>\n<p><strong>日期</strong>：2023年10月20日</p>\n<h2 id=\"预训练评估的核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练评估的核心观点\"><span>预训练评估的核心观点</span></a></h2>\n<p>预训练评估是大语言模型（LLM）全链路评估中较为简单的环节，主要关注模型的知识掌握程度，而非指令跟随能力或安全性等。</p>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"困惑度-ppl-测量\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#困惑度-ppl-测量\"><span>困惑度（PPL）测量</span></a></h3>\n<ul>\n<li><strong>数据准备</strong>：使用百科、逻辑、代码等数据集。</li>\n<li><strong>观察趋势</strong>：每日观察模型在这些集合上的损失（loss）表现，正常情况下损失会逐渐下降并趋于稳定。</li>\n<li><strong>模型对比</strong>：困惑度只能在同一模型的不同版本之间进行比较，因为不同的tokenizer压缩率会影响loss的可比性。</li>\n</ul>\n<h3 id=\"benchmark评估\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#benchmark评估\"><span>Benchmark评估</span></a></h3>\n<p>推荐使用开源平台OpenCompass进行Benchmark评估。以下是一些常用的Benchmark：</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>用途</th>\n<th>数据地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MMLU</td>\n<td>评估广泛主题领域的理解和推理能力</td>\n<td><a href=\"https://github.com/hendrycks/test\" target=\"_blank\" rel=\"noopener noreferrer\">MMLU数据集</a></td>\n</tr>\n<tr>\n<td>GLUE</td>\n<td>全面评估语言理解能力</td>\n<td><a href=\"https://huggingface.co/datasets/nyu-mll/glue\" target=\"_blank\" rel=\"noopener noreferrer\">GLUE数据集</a></td>\n</tr>\n<tr>\n<td>MultiNLI</td>\n<td>评估根据陈述推理正确类别的能力</td>\n<td><a href=\"https://huggingface.co/datasets/multi_nli\" target=\"_blank\" rel=\"noopener noreferrer\">MultiNLI数据集</a></td>\n</tr>\n<tr>\n<td>SuperGLUE</td>\n<td>评估语言理解和推理的更深层次</td>\n<td><a href=\"https://huggingface.co/datasets/super_glue\" target=\"_blank\" rel=\"noopener noreferrer\">SuperGLUE数据集</a></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"技术术语解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语解释\"><span>技术术语解释</span></a></h3>\n<ul>\n<li><strong>困惑度（PPL）</strong>：衡量模型预测下一个词的难易程度，数值越低表示模型预测越准确。</li>\n<li><strong>Benchmark</strong>：用于评估模型性能的标准化测试集。</li>\n</ul>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>如何在不同的领域中优化LLM的知识掌握？</li>\n<li>在困惑度下降趋于稳定后，还有哪些优化空间？</li>\n<li>Benchmark测试结果如何反映在实际应用中？</li>\n</ul>\n<blockquote>\n<p>来源：预训练评估文档</p>\n</blockquote>\n<hr>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 准备数据集，包括百科、逻辑和代码。</li>\n<li>⚠ 每日观察测试集合上的loss表现。</li>\n<li>❗ 使用OpenCompass进行Benchmark评估。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：不同模型之间直接比较困惑度可能导致误解，因为tokenizer压缩率不同。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用多种Benchmark可以全面评估模型的各方面能力。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>继续优化模型以降低困惑度。</li>\n<li>扩展Benchmark测试以涵盖更多领域。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，随着更复杂的数据集和更高效的算法，模型的知识掌握能力将进一步提升。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索更多领域特定的Benchmark。</li>\n<li>研究不同tokenizer对困惑度的影响。\n[[大语言模型学习/Pre-training 预训练/预训练评估2|预训练评估2]]</li>\n</ul>\n</template>","contentStripped":"<p><strong>分类</strong>：人工智能</p>\n<p><strong>标签</strong>：预训练评估、困惑度、Benchmark</p>\n<p><strong>日期</strong>：2023年10月20日</p>\n<h2 id=\"预训练评估的核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练评估的核心观点\"><span>预训练评估的核心观点</span></a></h2>\n<p>预训练评估是大语言模型（LLM）全链路评估中较为简单的环节，主要关注模型的知识掌握程度，而非指令跟随能力或安全性等。</p>\n<h2 id=\"重点内容\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点内容\"><span>重点内容</span></a></h2>\n<h3 id=\"困惑度-ppl-测量\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#困惑度-ppl-测量\"><span>困惑度（PPL）测量</span></a></h3>\n<ul>\n<li><strong>数据准备</strong>：使用百科、逻辑、代码等数据集。</li>\n<li><strong>观察趋势</strong>：每日观察模型在这些集合上的损失（loss）表现，正常情况下损失会逐渐下降并趋于稳定。</li>\n<li><strong>模型对比</strong>：困惑度只能在同一模型的不同版本之间进行比较，因为不同的tokenizer压缩率会影响loss的可比性。</li>\n</ul>\n<h3 id=\"benchmark评估\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#benchmark评估\"><span>Benchmark评估</span></a></h3>\n<p>推荐使用开源平台OpenCompass进行Benchmark评估。以下是一些常用的Benchmark：</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>用途</th>\n<th>数据地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MMLU</td>\n<td>评估广泛主题领域的理解和推理能力</td>\n<td><a href=\"https://github.com/hendrycks/test\" target=\"_blank\" rel=\"noopener noreferrer\">MMLU数据集</a></td>\n</tr>\n<tr>\n<td>GLUE</td>\n<td>全面评估语言理解能力</td>\n<td><a href=\"https://huggingface.co/datasets/nyu-mll/glue\" target=\"_blank\" rel=\"noopener noreferrer\">GLUE数据集</a></td>\n</tr>\n<tr>\n<td>MultiNLI</td>\n<td>评估根据陈述推理正确类别的能力</td>\n<td><a href=\"https://huggingface.co/datasets/multi_nli\" target=\"_blank\" rel=\"noopener noreferrer\">MultiNLI数据集</a></td>\n</tr>\n<tr>\n<td>SuperGLUE</td>\n<td>评估语言理解和推理的更深层次</td>\n<td><a href=\"https://huggingface.co/datasets/super_glue\" target=\"_blank\" rel=\"noopener noreferrer\">SuperGLUE数据集</a></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"技术术语解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语解释\"><span>技术术语解释</span></a></h3>\n<ul>\n<li><strong>困惑度（PPL）</strong>：衡量模型预测下一个词的难易程度，数值越低表示模型预测越准确。</li>\n<li><strong>Benchmark</strong>：用于评估模型性能的标准化测试集。</li>\n</ul>\n<h2 id=\"思考\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#思考\"><span>思考</span></a></h2>\n<ul>\n<li>如何在不同的领域中优化LLM的知识掌握？</li>\n<li>在困惑度下降趋于稳定后，还有哪些优化空间？</li>\n<li>Benchmark测试结果如何反映在实际应用中？</li>\n</ul>\n<blockquote>\n<p>来源：预训练评估文档</p>\n</blockquote>\n<hr>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 准备数据集，包括百科、逻辑和代码。</li>\n<li>⚠ 每日观察测试集合上的loss表现。</li>\n<li>❗ 使用OpenCompass进行Benchmark评估。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>警告：不同模型之间直接比较困惑度可能导致误解，因为tokenizer压缩率不同。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用多种Benchmark可以全面评估模型的各方面能力。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>继续优化模型以降低困惑度。</li>\n<li>扩展Benchmark测试以涵盖更多领域。</li>\n</ul>\n<h2 id=\"📈趋势预测\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📈趋势预测\"><span>📈趋势预测</span></a></h2>\n<p>未来，随着更复杂的数据集和更高效的算法，模型的知识掌握能力将进一步提升。</p>\n<h2 id=\"后续追踪\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#后续追踪\"><span>后续追踪</span></a></h2>\n<ul>\n<li>探索更多领域特定的Benchmark。</li>\n<li>研究不同tokenizer对困惑度的影响。\n[[大语言模型学习/Pre-training 预训练/预训练评估2|预训练评估2]]</li>\n</ul>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"**分类**：人工智能\n\n**标签**：预训练评估、困惑度、Benchmark\n\n**日期**：2023年10月20日\n\n## 预训练评估的核心观点\n预训练评估是大语言模型（LLM）全链路评估中较为简单的环节，主要关注模型的知识掌握程度，而非指令跟随能力或安全性等。\n\n\n## 重点内容\n\n### 困惑度（PPL）测量\n- **数据准备**：使用百科、逻辑、代码等数据集。\n- **观察趋势**：每日观察模型在这些集合上的损失（loss）表现，正常情况下损失会逐渐下降并趋于稳定。\n- **模型对比**：困惑度只能在同一模型的不同版本之间进行比较，因为不同的tokenizer压缩率会影响loss的可比性。\n\n\n### Benchmark评估\n推荐使用开源平台OpenCompass进行Benchmark评估。以下是一些常用的Benchmark：\n\n| 名称     | 用途                            | 数据地址 |\n|----------|---------------------------------|----------|\n| MMLU     | 评估广泛主题领域的理解和推理能力 | [MMLU数据集](https://github.com/hendrycks/test) |\n| GLUE     | 全面评估语言理解能力             | [GLUE数据集](https://huggingface.co/datasets/nyu-mll/glue) |\n| MultiNLI | 评估根据陈述推理正确类别的能力   | [MultiNLI数据集](https://huggingface.co/datasets/multi_nli) |\n| SuperGLUE| 评估语言理解和推理的更深层次     | [SuperGLUE数据集](https://huggingface.co/datasets/super_glue) |\n\n\n### 技术术语解释\n- **困惑度（PPL）**：衡量模型预测下一个词的难易程度，数值越低表示模型预测越准确。\n- **Benchmark**：用于评估模型性能的标准化测试集。\n\n\n## 思考\n- 如何在不同的领域中优化LLM的知识掌握？\n- 在困惑度下降趋于稳定后，还有哪些优化空间？\n- Benchmark测试结果如何反映在实际应用中？\n\n> 来源：预训练评估文档\n\n---\n\n\n## 操作步骤\n1. ✅ 准备数据集，包括百科、逻辑和代码。\n2. ⚠ 每日观察测试集合上的loss表现。\n3. ❗ 使用OpenCompass进行Benchmark评估。\n\n\n## 常见错误\n> 警告：不同模型之间直接比较困惑度可能导致误解，因为tokenizer压缩率不同。\n\n\n## 💡启发点\n- 使用多种Benchmark可以全面评估模型的各方面能力。\n\n\n## 行动清单\n- 继续优化模型以降低困惑度。\n- 扩展Benchmark测试以涵盖更多领域。\n\n\n## 📈趋势预测\n未来，随着更复杂的数据集和更高效的算法，模型的知识掌握能力将进一步提升。\n\n\n## 后续追踪\n- 探索更多领域特定的Benchmark。\n- 研究不同tokenizer对困惑度的影响。\n[[大语言模型学习/Pre-training 预训练/预训练评估2\\|预训练评估2]]","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"预训练评估的核心观点","slug":"预训练评估的核心观点","link":"#预训练评估的核心观点","children":[]},{"level":2,"title":"重点内容","slug":"重点内容","link":"#重点内容","children":[{"level":3,"title":"困惑度（PPL）测量","slug":"困惑度-ppl-测量","link":"#困惑度-ppl-测量","children":[]},{"level":3,"title":"Benchmark评估","slug":"benchmark评估","link":"#benchmark评估","children":[]},{"level":3,"title":"技术术语解释","slug":"技术术语解释","link":"#技术术语解释","children":[]}]},{"level":2,"title":"思考","slug":"思考","link":"#思考","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"📈趋势预测","slug":"📈趋势预测","link":"#📈趋势预测","children":[]},{"level":2,"title":"后续追踪","slug":"后续追踪","link":"#后续追踪","children":[]}]}}
