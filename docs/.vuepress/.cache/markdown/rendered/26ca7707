{"content":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习技术</li>\n<li>标签：QLoRA, LoRA, 微调, 内存优化, 人工智能</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容摘要\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容摘要\"><span>内容摘要</span></a></h2>\n<p>QLoRA是一种基于LoRA的微调方法，通过引入4-bit NormalFloat、双重量化和Paged Optimizers等技术，显著降低了模型的内存使用，同时保持了高性能。这种创新方法使得在单GPU上微调最大的公开可用模型成为可能。</p>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>QLoRA通过在每个网络层添加适配器，避免了以前微调方法中几乎所有的准确性折衷。这种方法将拥有65B参数的模型内存需求从&gt;780GB降低到&lt;48GB。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424111525.png\" alt=\"Pasted image 20250424111525.png\"></p>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>LoRA</strong>：一种用于减少模型参数的技术，通过在深度学习模型中添加适配器来优化内存使用。</li>\n<li><strong>4-bit NormalFloat</strong>：一种数据表示方式，使用较少的比特来存储浮点数，从而节省内存。</li>\n<li><strong>双重量化</strong>：一种技术，通过对数据进行两次量化来进一步减少存储需求。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 在每个网络层添加适配器以优化内存使用。</li>\n<li>⚠ 引入4-bit NormalFloat以减少浮点数存储空间。</li>\n<li>❗ 使用双重量化技术以进一步降低内存需求。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在实施QLoRA时，确保适配器正确集成到每个网络层，否则可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>QLoRA的创新在于其能够在不牺牲性能的情况下显著降低内存需求，使得大型模型的微调在单GPU上成为可能。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究QLoRA在其他类型模型中的应用潜力。</li>\n<li>探索更多关于4-bit NormalFloat和双重量化的技术细节。</li>\n<li>实验QLoRA与其他微调方法的性能比较。</li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数模型</th>\n<th>原始内存需求</th>\n<th>优化后内存需求</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>65B</td>\n<td>&gt;780GB</td>\n<td>&lt;48GB</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>来源：原始内容来自用户提供的文本。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/QLoRA.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/QLoRA.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/QLoRA","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/QLoRA/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T03:14:40.000Z","updated":"2025-04-24T03:17:34.000Z","title":"QLoRA","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习技术</li>\n<li>标签：QLoRA, LoRA, 微调, 内存优化, 人工智能</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容摘要\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容摘要\"><span>内容摘要</span></a></h2>\n<p>QLoRA是一种基于LoRA的微调方法，通过引入4-bit NormalFloat、双重量化和Paged Optimizers等技术，显著降低了模型的内存使用，同时保持了高性能。这种创新方法使得在单GPU上微调最大的公开可用模型成为可能。</p>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>QLoRA通过在每个网络层添加适配器，避免了以前微调方法中几乎所有的准确性折衷。这种方法将拥有65B参数的模型内存需求从&gt;780GB降低到&lt;48GB。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424111525.png\" alt=\"Pasted image 20250424111525.png\"></p>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>LoRA</strong>：一种用于减少模型参数的技术，通过在深度学习模型中添加适配器来优化内存使用。</li>\n<li><strong>4-bit NormalFloat</strong>：一种数据表示方式，使用较少的比特来存储浮点数，从而节省内存。</li>\n<li><strong>双重量化</strong>：一种技术，通过对数据进行两次量化来进一步减少存储需求。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 在每个网络层添加适配器以优化内存使用。</li>\n<li>⚠ 引入4-bit NormalFloat以减少浮点数存储空间。</li>\n<li>❗ 使用双重量化技术以进一步降低内存需求。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在实施QLoRA时，确保适配器正确集成到每个网络层，否则可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>QLoRA的创新在于其能够在不牺牲性能的情况下显著降低内存需求，使得大型模型的微调在单GPU上成为可能。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究QLoRA在其他类型模型中的应用潜力。</li>\n<li>探索更多关于4-bit NormalFloat和双重量化的技术细节。</li>\n<li>实验QLoRA与其他微调方法的性能比较。</li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数模型</th>\n<th>原始内存需求</th>\n<th>优化后内存需求</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>65B</td>\n<td>&gt;780GB</td>\n<td>&lt;48GB</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>来源：原始内容来自用户提供的文本。</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"元数据\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#元数据\"><span>元数据</span></a></h2>\n<ul>\n<li>分类：机器学习技术</li>\n<li>标签：QLoRA, LoRA, 微调, 内存优化, 人工智能</li>\n<li>日期：2025年4月12日</li>\n</ul>\n<h2 id=\"内容摘要\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容摘要\"><span>内容摘要</span></a></h2>\n<p>QLoRA是一种基于LoRA的微调方法，通过引入4-bit NormalFloat、双重量化和Paged Optimizers等技术，显著降低了模型的内存使用，同时保持了高性能。这种创新方法使得在单GPU上微调最大的公开可用模型成为可能。</p>\n<h2 id=\"核心观点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点\"><span>核心观点</span></a></h2>\n<p>QLoRA通过在每个网络层添加适配器，避免了以前微调方法中几乎所有的准确性折衷。这种方法将拥有65B参数的模型内存需求从&gt;780GB降低到&lt;48GB。</p>\n<p><img src=\"/img/user/附件/Pasted image 20250424111525.png\" alt=\"Pasted image 20250424111525.png\"></p>\n<h2 id=\"技术术语通俗解释\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#技术术语通俗解释\"><span>技术术语通俗解释</span></a></h2>\n<ul>\n<li><strong>LoRA</strong>：一种用于减少模型参数的技术，通过在深度学习模型中添加适配器来优化内存使用。</li>\n<li><strong>4-bit NormalFloat</strong>：一种数据表示方式，使用较少的比特来存储浮点数，从而节省内存。</li>\n<li><strong>双重量化</strong>：一种技术，通过对数据进行两次量化来进一步减少存储需求。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 在每个网络层添加适配器以优化内存使用。</li>\n<li>⚠ 引入4-bit NormalFloat以减少浮点数存储空间。</li>\n<li>❗ 使用双重量化技术以进一步降低内存需求。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>⚠ 在实施QLoRA时，确保适配器正确集成到每个网络层，否则可能导致模型性能下降。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<p>QLoRA的创新在于其能够在不牺牲性能的情况下显著降低内存需求，使得大型模型的微调在单GPU上成为可能。</p>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>研究QLoRA在其他类型模型中的应用潜力。</li>\n<li>探索更多关于4-bit NormalFloat和双重量化的技术细节。</li>\n<li>实验QLoRA与其他微调方法的性能比较。</li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>参数模型</th>\n<th>原始内存需求</th>\n<th>优化后内存需求</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>65B</td>\n<td>&gt;780GB</td>\n<td>&lt;48GB</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>来源：原始内容来自用户提供的文本。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 元数据\n- 分类：机器学习技术\n- 标签：QLoRA, LoRA, 微调, 内存优化, 人工智能\n- 日期：2025年4月12日\n\n\n## 内容摘要\nQLoRA是一种基于LoRA的微调方法，通过引入4-bit NormalFloat、双重量化和Paged Optimizers等技术，显著降低了模型的内存使用，同时保持了高性能。这种创新方法使得在单GPU上微调最大的公开可用模型成为可能。\n\n\n## 核心观点\nQLoRA通过在每个网络层添加适配器，避免了以前微调方法中几乎所有的准确性折衷。这种方法将拥有65B参数的模型内存需求从>780GB降低到<48GB。\n\n![Pasted image 20250424111525.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424111525.png)\n\n\n## 技术术语通俗解释\n- **LoRA**：一种用于减少模型参数的技术，通过在深度学习模型中添加适配器来优化内存使用。\n- **4-bit NormalFloat**：一种数据表示方式，使用较少的比特来存储浮点数，从而节省内存。\n- **双重量化**：一种技术，通过对数据进行两次量化来进一步减少存储需求。\n\n\n## 操作步骤\n1. ✅ 在每个网络层添加适配器以优化内存使用。\n2. ⚠ 引入4-bit NormalFloat以减少浮点数存储空间。\n3. ❗ 使用双重量化技术以进一步降低内存需求。\n\n\n## 常见错误\n> ⚠ 在实施QLoRA时，确保适配器正确集成到每个网络层，否则可能导致模型性能下降。\n\n\n## 💡启发点\nQLoRA的创新在于其能够在不牺牲性能的情况下显著降低内存需求，使得大型模型的微调在单GPU上成为可能。\n\n\n## 行动清单\n- 研究QLoRA在其他类型模型中的应用潜力。\n- 探索更多关于4-bit NormalFloat和双重量化的技术细节。\n- 实验QLoRA与其他微调方法的性能比较。\n\n\n## 数据转换\n| 参数模型 | 原始内存需求 | 优化后内存需求 |\n|----------|--------------|----------------|\n| 65B      | >780GB       | <48GB          |\n\n> 来源：原始内容来自用户提供的文本。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"元数据","slug":"元数据","link":"#元数据","children":[]},{"level":2,"title":"内容摘要","slug":"内容摘要","link":"#内容摘要","children":[]},{"level":2,"title":"核心观点","slug":"核心观点","link":"#核心观点","children":[]},{"level":2,"title":"技术术语通俗解释","slug":"技术术语通俗解释","link":"#技术术语通俗解释","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"数据转换","slug":"数据转换","link":"#数据转换","children":[]}]}}
