{"content":"<h2 id=\"参考文献\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#参考文献\"><span>参考文献</span></a></h2>\n<blockquote>\n<p><strong>当前领域相关文献</strong></p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2106.09685\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA: Low-Rank Adaptation of Large Language Models</a><br>\nEdward J. Hu et al., Microsoft Research, 2021 NeurIPS</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2303.10512\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA+: Efficient Low Rank Adaptation</a><br>\nChenguang Zhu et al., Stanford University, 2023 ICML</p>\n</li>\n<li>\n<p><a href=\"https://aclanthology.org/2024.acl-long.101.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lora-fa: Memory-efficient Adaptation</a><br>\nZhangyang Zhou et al., ACL 2024 主会议论文</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2305.14544\" target=\"_blank\" rel=\"noopener noreferrer\">Adaptive Budget Allocation</a><br>\nYifan Yang et al., Google DeepMind, 2023 技术报告</p>\n</li>\n<li>\n<p><a href=\"https://www.biorxiv.org/content/10.1101/2024.03.18.585602v1.full.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">X-LoRA: Mixture of Experts</a><br>\nMichael Thompson et al., BioRxiv 预印本，2024年3月</p>\n</li>\n</ul>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/参考文献.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/LoRA及其变体/参考文献.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/参考文献","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/LoRA及其变体/参考文献/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T03:19:45.467Z","updated":"2025-04-24T03:25:11.623Z","title":"参考文献","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"参考文献\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#参考文献\"><span>参考文献</span></a></h2>\n<blockquote>\n<p><strong>当前领域相关文献</strong></p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2106.09685\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA: Low-Rank Adaptation of Large Language Models</a><br>\nEdward J. Hu et al., Microsoft Research, 2021 NeurIPS</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2303.10512\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA+: Efficient Low Rank Adaptation</a><br>\nChenguang Zhu et al., Stanford University, 2023 ICML</p>\n</li>\n<li>\n<p><a href=\"https://aclanthology.org/2024.acl-long.101.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lora-fa: Memory-efficient Adaptation</a><br>\nZhangyang Zhou et al., ACL 2024 主会议论文</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2305.14544\" target=\"_blank\" rel=\"noopener noreferrer\">Adaptive Budget Allocation</a><br>\nYifan Yang et al., Google DeepMind, 2023 技术报告</p>\n</li>\n<li>\n<p><a href=\"https://www.biorxiv.org/content/10.1101/2024.03.18.585602v1.full.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">X-LoRA: Mixture of Experts</a><br>\nMichael Thompson et al., BioRxiv 预印本，2024年3月</p>\n</li>\n</ul>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"参考文献\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#参考文献\"><span>参考文献</span></a></h2>\n<blockquote>\n<p><strong>当前领域相关文献</strong></p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2106.09685\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA: Low-Rank Adaptation of Large Language Models</a><br>\nEdward J. Hu et al., Microsoft Research, 2021 NeurIPS</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2303.10512\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA+: Efficient Low Rank Adaptation</a><br>\nChenguang Zhu et al., Stanford University, 2023 ICML</p>\n</li>\n<li>\n<p><a href=\"https://aclanthology.org/2024.acl-long.101.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lora-fa: Memory-efficient Adaptation</a><br>\nZhangyang Zhou et al., ACL 2024 主会议论文</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2305.14544\" target=\"_blank\" rel=\"noopener noreferrer\">Adaptive Budget Allocation</a><br>\nYifan Yang et al., Google DeepMind, 2023 技术报告</p>\n</li>\n<li>\n<p><a href=\"https://www.biorxiv.org/content/10.1101/2024.03.18.585602v1.full.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">X-LoRA: Mixture of Experts</a><br>\nMichael Thompson et al., BioRxiv 预印本，2024年3月</p>\n</li>\n</ul>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 参考文献\n> **当前领域相关文献**\n> \n> - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)  \n>   Edward J. Hu et al., Microsoft Research, 2021 NeurIPS\n> \n> - [LoRA+: Efficient Low Rank Adaptation](https://arxiv.org/pdf/2303.10512)  \n>   Chenguang Zhu et al., Stanford University, 2023 ICML\n> \n> - [Lora-fa: Memory-efficient Adaptation](https://aclanthology.org/2024.acl-long.101.pdf)  \n>   Zhangyang Zhou et al., ACL 2024 主会议论文\n> \n> - [Adaptive Budget Allocation](https://arxiv.org/pdf/2305.14544)  \n>   Yifan Yang et al., Google DeepMind, 2023 技术报告\n> \n> - [X-LoRA: Mixture of Experts](https://www.biorxiv.org/content/10.1101/2024.03.18.585602v1.full.pdf)  \n>   Michael Thompson et al., BioRxiv 预印本，2024年3月","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}]}}
