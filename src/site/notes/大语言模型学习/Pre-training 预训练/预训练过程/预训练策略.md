---
{"dg-publish":true,"dg-permalink":"/大语言模型学习/Pre-training-预训练/预训练过程/预训练策略","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Pre-training-预训练/预训练过程/预训练策略/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-09T22:02:28.000+08:00","updated":"2025-04-13T13:06:02.000+08:00"}
---



## 元数据
- 分类：深度学习
- 标签：预训练策略，学习率调度，模型优化
- 日期：2023年10月25日

---



## 核心观点总结
在深度学习模型的预训练过程中，优化策略至关重要。本文探讨了如何通过调整 **batch_size**、采用 **WSD调度器** 和 **预训练Trick** 来提升模型训练效率，同时总结了四阶段预训练设置的具体流程。

---



## 重点内容

### 最优 Batch Size 的选择
- **关键点**：Batch size 影响模型收敛速度与资源消耗的平衡。
- 数据实验表明：
  - Batch size 过大，数据和计算量增加。
  - Batch size 过小，训练步数增多且损失函数下降受限。
- C4 Loss 的规律公式：
  $$
  BS = L \cdot 6.2393 / (1.2110 \times 10^9)
  $$
- ![Pasted image 20250409220613.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250409220613.png)


### WSD 调度器的三阶段学习率策略
- **分阶段特点**：
  1. **快速收敛阶段**（Warmup）：学习率线性上升。
  2. **稳定阶段**（Stable）：保持最大学习率。
  3. **退火阶段**（Decay）：采用余弦退火算法逐步降低学习率。
- 学习率公式：
  $$
  lr(s) =
  \begin{cases} 
    \frac{s}{W} \cdot \eta, & s < W \\ 
    \eta, & W \leq s < S \\ 
    f(s - S) \cdot \eta, & S \leq s < S + D
  \end{cases}
  $$
  - $$\eta$$：最大学习率  
  - $$f(s - S)$$：关于 $$s$$ 的减函数，范围 $$0 < f(s - S) \leq 1$$


### 提高效率的预训练技巧
- **显存优化**：
  - 若显存充足，尽量避免引入复杂并行技术（如 tensor_parallel）。
  - 不开启 **offload** 和 **重算** 可节省时间。
- **指令数据**：
  - 加入更多指令数据有助于提升模型性能。

---



## ✅ 四阶段预训练设置流程
1. **Warmup 阶段**：
   - 学习率缓慢上升到最大值。
2. **中期稳定阶段**：
   - 使用较大的学习率，是否引入衰减需视实验而定。
3. **后期适应阶段**：
   - 改变 RoPE 的 base 频率，增加文本长度，让模型适应长文本任务。
4. **收尾退火阶段**：
   - 使用高质量数据（如 IFT 数据）强化模型能力，为 benchmark 测试做准备。

---



## ⚠ 常见错误与注意事项
> **警告区块**  
- 忽视 batch size 对训练效率的影响，可能导致资源浪费或性能不足。  
- 在显存不足时强行引入复杂并行技术，可能引发调试困难和性能下降。

---



## 📈 趋势预测
未来预训练策略可能会更加注重以下方向：
1. 自动化调参工具的普及，减少人工调整成本。
2. 更智能的数据采样方法，提升高质量数据使用比例。
3. 多模型协同训练策略（如多任务联合训练）的进一步发展。

---



## [思考] 延伸问题
1. 如何在不同硬件条件下灵活调整 batch size 和学习率？
2. 是否存在更高效的调度器替代 WSD 调度器？
3. 长文本适应性优化是否能迁移至多模态任务中？

---

> 原文出处：深度学习预训练策略文档

---



## 行动清单
- [ ] 实验不同 batch size 对小模型的影响。
- [ ] 测试 WSD 调度器与余弦退火算法在大规模任务中的性能差异。
- [ ] 探索高效的显存管理技术以支持更大规模的预训练。
