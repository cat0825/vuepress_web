{"content":"<h2 id=\"分类-自动推断\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-自动推断\"><span>分类：自动推断</span></a></h2>\n<h2 id=\"标签-强化学习、vapo算法、推理任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-强化学习、vapo算法、推理任务\"><span>标签：强化学习、VAPO算法、推理任务</span></a></h2>\n<h2 id=\"日期-2025年4月12日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2025年4月12日\"><span>日期：2025年4月12日</span></a></h2>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>VAPO（Value-model-based Augmented PPO）是一种新提出的算法，旨在提升复杂推理任务中的强化学习效率和稳定性。该算法在训练中使用了价值模型（Value Model），以更精确地估计每个动作对未来收益的影响，从而优化策略。VAPO在AIME 2024数据集上表现出色，超越了之前的算法，如Deepseek-R1-Zero-Qwen-32B和DAPO。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"value-model-based-v-s-value-model-free\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#value-model-based-v-s-value-model-free\"><span>Value-model-based V.S. Value-model-free</span></a></h3>\n<ul>\n<li>传统的无价值模型方法（如GRPO、DAPO）在大型模型的强化学习中有效，但在复杂任务中表现不稳定。</li>\n<li>VAPO通过准确训练的价值模型可以实现更高的性能上限，因为它能够提供细粒度的奖励，优化策略。</li>\n</ul>\n<h3 id=\"value-model-的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#value-model-的挑战\"><span>Value Model 的挑战</span></a></h3>\n<ul>\n<li>在长序列任务中训练一个完美的价值模型非常困难。</li>\n<li>价值模型往往会在长轨迹序列中产生偏差，尤其是在使用自举方法时。</li>\n</ul>\n<h3 id=\"蒙特卡洛估计与价值模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#蒙特卡洛估计与价值模型\"><span>蒙特卡洛估计与价值模型</span></a></h3>\n<ul>\n<li>蒙特卡洛估计通常伴随着高方差，而价值模型可以生成低方差的估计，增强训练稳定性。</li>\n<li>准确的价值模型有助于利用探索过程中产生的样本，提高强化学习的上限。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定任务目标，并选择合适的数据集。</li>\n<li>⚠ 使用价值模型初始化，注意避免偏差。</li>\n<li>❗ 在训练过程中监控方差和偏差，调整策略。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在初始化价值模型时，避免使用不完整上下文，这可能导致偏差。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用价值模型进行细粒度策略优化是提升复杂任务表现的关键。</li>\n<li>低方差估计有助于增强训练稳定性，这对于长序列任务尤为重要。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>进一步研究价值模型在其他数据集上的表现。</li>\n<li>探索不同初始化策略对VAPO性能的影响。</li>\n<li>评估VAPO在实际应用中的可行性和效率。</li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>算法</th>\n<th>数据集</th>\n<th>得分</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>VAPO</td>\n<td>AIME 2024</td>\n<td>60.4</td>\n</tr>\n<tr>\n<td>Deepseek-R1-Zero-Qwen-32B</td>\n<td>AIME 2024</td>\n<td>未知</td>\n</tr>\n<tr>\n<td>DAPO</td>\n<td>AIME 2024</td>\n<td>未知</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>本文内容基于论文《VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks》，链接: <a href=\"https://arxiv.org/pdf/2504.05118\" target=\"_blank\" rel=\"noopener noreferrer\">arxiv.org/pdf/2504.05118</a></p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/RL强化学习基础/优化PPO方向的算法/VAPO.md","filePathRelative":"notes_bak/大语言模型学习/RL强化学习基础/优化PPO方向的算法/VAPO.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/RL强化学习基础/优化PPO方向的算法/VAPO","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/RL强化学习基础/优化PPO方向的算法/VAPO/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-22T14:29:41.000Z","updated":"2025-04-22T14:46:13.000Z","title":"VAPO","createTime":"2025/05/13 17:33:52"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"分类-自动推断\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-自动推断\"><span>分类：自动推断</span></a></h2>\n<h2 id=\"标签-强化学习、vapo算法、推理任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-强化学习、vapo算法、推理任务\"><span>标签：强化学习、VAPO算法、推理任务</span></a></h2>\n<h2 id=\"日期-2025年4月12日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2025年4月12日\"><span>日期：2025年4月12日</span></a></h2>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>VAPO（Value-model-based Augmented PPO）是一种新提出的算法，旨在提升复杂推理任务中的强化学习效率和稳定性。该算法在训练中使用了价值模型（Value Model），以更精确地估计每个动作对未来收益的影响，从而优化策略。VAPO在AIME 2024数据集上表现出色，超越了之前的算法，如Deepseek-R1-Zero-Qwen-32B和DAPO。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"value-model-based-v-s-value-model-free\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#value-model-based-v-s-value-model-free\"><span>Value-model-based V.S. Value-model-free</span></a></h3>\n<ul>\n<li>传统的无价值模型方法（如GRPO、DAPO）在大型模型的强化学习中有效，但在复杂任务中表现不稳定。</li>\n<li>VAPO通过准确训练的价值模型可以实现更高的性能上限，因为它能够提供细粒度的奖励，优化策略。</li>\n</ul>\n<h3 id=\"value-model-的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#value-model-的挑战\"><span>Value Model 的挑战</span></a></h3>\n<ul>\n<li>在长序列任务中训练一个完美的价值模型非常困难。</li>\n<li>价值模型往往会在长轨迹序列中产生偏差，尤其是在使用自举方法时。</li>\n</ul>\n<h3 id=\"蒙特卡洛估计与价值模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#蒙特卡洛估计与价值模型\"><span>蒙特卡洛估计与价值模型</span></a></h3>\n<ul>\n<li>蒙特卡洛估计通常伴随着高方差，而价值模型可以生成低方差的估计，增强训练稳定性。</li>\n<li>准确的价值模型有助于利用探索过程中产生的样本，提高强化学习的上限。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定任务目标，并选择合适的数据集。</li>\n<li>⚠ 使用价值模型初始化，注意避免偏差。</li>\n<li>❗ 在训练过程中监控方差和偏差，调整策略。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在初始化价值模型时，避免使用不完整上下文，这可能导致偏差。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用价值模型进行细粒度策略优化是提升复杂任务表现的关键。</li>\n<li>低方差估计有助于增强训练稳定性，这对于长序列任务尤为重要。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>进一步研究价值模型在其他数据集上的表现。</li>\n<li>探索不同初始化策略对VAPO性能的影响。</li>\n<li>评估VAPO在实际应用中的可行性和效率。</li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>算法</th>\n<th>数据集</th>\n<th>得分</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>VAPO</td>\n<td>AIME 2024</td>\n<td>60.4</td>\n</tr>\n<tr>\n<td>Deepseek-R1-Zero-Qwen-32B</td>\n<td>AIME 2024</td>\n<td>未知</td>\n</tr>\n<tr>\n<td>DAPO</td>\n<td>AIME 2024</td>\n<td>未知</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>本文内容基于论文《VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks》，链接: <a href=\"https://arxiv.org/pdf/2504.05118\" target=\"_blank\" rel=\"noopener noreferrer\">arxiv.org/pdf/2504.05118</a></p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"分类-自动推断\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类-自动推断\"><span>分类：自动推断</span></a></h2>\n<h2 id=\"标签-强化学习、vapo算法、推理任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签-强化学习、vapo算法、推理任务\"><span>标签：强化学习、VAPO算法、推理任务</span></a></h2>\n<h2 id=\"日期-2025年4月12日\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期-2025年4月12日\"><span>日期：2025年4月12日</span></a></h2>\n<h2 id=\"核心观点总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#核心观点总结\"><span>核心观点总结</span></a></h2>\n<p>VAPO（Value-model-based Augmented PPO）是一种新提出的算法，旨在提升复杂推理任务中的强化学习效率和稳定性。该算法在训练中使用了价值模型（Value Model），以更精确地估计每个动作对未来收益的影响，从而优化策略。VAPO在AIME 2024数据集上表现出色，超越了之前的算法，如Deepseek-R1-Zero-Qwen-32B和DAPO。</p>\n<h2 id=\"重点段落\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#重点段落\"><span>重点段落</span></a></h2>\n<h3 id=\"value-model-based-v-s-value-model-free\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#value-model-based-v-s-value-model-free\"><span>Value-model-based V.S. Value-model-free</span></a></h3>\n<ul>\n<li>传统的无价值模型方法（如GRPO、DAPO）在大型模型的强化学习中有效，但在复杂任务中表现不稳定。</li>\n<li>VAPO通过准确训练的价值模型可以实现更高的性能上限，因为它能够提供细粒度的奖励，优化策略。</li>\n</ul>\n<h3 id=\"value-model-的挑战\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#value-model-的挑战\"><span>Value Model 的挑战</span></a></h3>\n<ul>\n<li>在长序列任务中训练一个完美的价值模型非常困难。</li>\n<li>价值模型往往会在长轨迹序列中产生偏差，尤其是在使用自举方法时。</li>\n</ul>\n<h3 id=\"蒙特卡洛估计与价值模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#蒙特卡洛估计与价值模型\"><span>蒙特卡洛估计与价值模型</span></a></h3>\n<ul>\n<li>蒙特卡洛估计通常伴随着高方差，而价值模型可以生成低方差的估计，增强训练稳定性。</li>\n<li>准确的价值模型有助于利用探索过程中产生的样本，提高强化学习的上限。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 确定任务目标，并选择合适的数据集。</li>\n<li>⚠ 使用价值模型初始化，注意避免偏差。</li>\n<li>❗ 在训练过程中监控方差和偏差，调整策略。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>在初始化价值模型时，避免使用不完整上下文，这可能导致偏差。</p>\n</blockquote>\n<h2 id=\"💡启发点\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#💡启发点\"><span>💡启发点</span></a></h2>\n<ul>\n<li>使用价值模型进行细粒度策略优化是提升复杂任务表现的关键。</li>\n<li>低方差估计有助于增强训练稳定性，这对于长序列任务尤为重要。</li>\n</ul>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul>\n<li>进一步研究价值模型在其他数据集上的表现。</li>\n<li>探索不同初始化策略对VAPO性能的影响。</li>\n<li>评估VAPO在实际应用中的可行性和效率。</li>\n</ul>\n<h2 id=\"数据转换\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据转换\"><span>数据转换</span></a></h2>\n<table>\n<thead>\n<tr>\n<th>算法</th>\n<th>数据集</th>\n<th>得分</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>VAPO</td>\n<td>AIME 2024</td>\n<td>60.4</td>\n</tr>\n<tr>\n<td>Deepseek-R1-Zero-Qwen-32B</td>\n<td>AIME 2024</td>\n<td>未知</td>\n</tr>\n<tr>\n<td>DAPO</td>\n<td>AIME 2024</td>\n<td>未知</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"来源标注\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#来源标注\"><span>来源标注</span></a></h2>\n<blockquote>\n<p>本文内容基于论文《VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks》，链接: <a href=\"https://arxiv.org/pdf/2504.05118\" target=\"_blank\" rel=\"noopener noreferrer\">arxiv.org/pdf/2504.05118</a></p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 分类：自动推断\n\n\n\n## 标签：强化学习、VAPO算法、推理任务\n\n\n\n## 日期：2025年4月12日\n\n\n\n## 核心观点总结\nVAPO（Value-model-based Augmented PPO）是一种新提出的算法，旨在提升复杂推理任务中的强化学习效率和稳定性。该算法在训练中使用了价值模型（Value Model），以更精确地估计每个动作对未来收益的影响，从而优化策略。VAPO在AIME 2024数据集上表现出色，超越了之前的算法，如Deepseek-R1-Zero-Qwen-32B和DAPO。\n\n\n\n## 重点段落\n\n### Value-model-based V.S. Value-model-free\n- 传统的无价值模型方法（如GRPO、DAPO）在大型模型的强化学习中有效，但在复杂任务中表现不稳定。\n- VAPO通过准确训练的价值模型可以实现更高的性能上限，因为它能够提供细粒度的奖励，优化策略。\n\n\n### Value Model 的挑战\n- 在长序列任务中训练一个完美的价值模型非常困难。\n- 价值模型往往会在长轨迹序列中产生偏差，尤其是在使用自举方法时。\n\n\n### 蒙特卡洛估计与价值模型\n- 蒙特卡洛估计通常伴随着高方差，而价值模型可以生成低方差的估计，增强训练稳定性。\n- 准确的价值模型有助于利用探索过程中产生的样本，提高强化学习的上限。\n\n\n\n## 操作步骤\n1. ✅ 确定任务目标，并选择合适的数据集。\n2. ⚠ 使用价值模型初始化，注意避免偏差。\n3. ❗ 在训练过程中监控方差和偏差，调整策略。\n\n\n\n## 常见错误\n> 在初始化价值模型时，避免使用不完整上下文，这可能导致偏差。\n\n\n\n## 💡启发点\n- 使用价值模型进行细粒度策略优化是提升复杂任务表现的关键。\n- 低方差估计有助于增强训练稳定性，这对于长序列任务尤为重要。\n\n\n\n## 行动清单\n- 进一步研究价值模型在其他数据集上的表现。\n- 探索不同初始化策略对VAPO性能的影响。\n- 评估VAPO在实际应用中的可行性和效率。\n\n\n\n## 数据转换\n| 算法                 | 数据集         | 得分   |\n|----------------------|---------------|--------|\n| VAPO                 | AIME 2024     | 60.4   |\n| Deepseek-R1-Zero-Qwen-32B | AIME 2024     | 未知   |\n| DAPO                 | AIME 2024     | 未知   |\n\n\n\n## 来源标注\n> 本文内容基于论文《VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks》，链接: [arxiv.org/pdf/2504.05118](https://arxiv.org/pdf/2504.05118)","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"分类：自动推断","slug":"分类-自动推断","link":"#分类-自动推断","children":[]},{"level":2,"title":"标签：强化学习、VAPO算法、推理任务","slug":"标签-强化学习、vapo算法、推理任务","link":"#标签-强化学习、vapo算法、推理任务","children":[]},{"level":2,"title":"日期：2025年4月12日","slug":"日期-2025年4月12日","link":"#日期-2025年4月12日","children":[]},{"level":2,"title":"核心观点总结","slug":"核心观点总结","link":"#核心观点总结","children":[]},{"level":2,"title":"重点段落","slug":"重点段落","link":"#重点段落","children":[{"level":3,"title":"Value-model-based V.S. Value-model-free","slug":"value-model-based-v-s-value-model-free","link":"#value-model-based-v-s-value-model-free","children":[]},{"level":3,"title":"Value Model 的挑战","slug":"value-model-的挑战","link":"#value-model-的挑战","children":[]},{"level":3,"title":"蒙特卡洛估计与价值模型","slug":"蒙特卡洛估计与价值模型","link":"#蒙特卡洛估计与价值模型","children":[]}]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"💡启发点","slug":"💡启发点","link":"#💡启发点","children":[]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]},{"level":2,"title":"数据转换","slug":"数据转换","link":"#数据转换","children":[]},{"level":2,"title":"来源标注","slug":"来源标注","link":"#来源标注","children":[]}]}}
