{"content":"<h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自然语言处理, 机器学习</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<p>PaLM, Transformer, 自然语言处理, 机器学习, 模型训练</p>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>本文介绍了PaLM（Pathways Language Model）的结构、训练设置和优化策略。PaLM采用了标准的Transformer架构，并进行了多项改进以提升模型性能和训练稳定性。\n<img src=\"/img/user/附件/Pasted image 20250424124216.png\" alt=\"Pasted image 20250424124216.png\"><img src=\"/img/user/附件/Pasted image 20250424124234.png\" alt=\"Pasted image 20250424124234.png\"></p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>PaLM使用标准的Transformer架构，采用了仅包含解码器的结构，并进行了以下改动：</p>\n<ul>\n<li><strong>SwiGLU激活函数</strong>：替换了传统的FFN部分。</li>\n<li><strong>并行层设计</strong>：改变了序列化表述，使用并行化结构以提升计算效率。</li>\n<li><strong>其他优化</strong>：引入MQA、RoPE、共享输入输出embedding，并去掉了dense层和Layer Norm中的偏差项。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：使用SwiGLU激活函数和并行层设计显著提高了模型的训练效率和稳定性。</p>\n<h2 id=\"训练设置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练设置\"><span>训练设置</span></a></h2>\n<ul>\n<li><strong>权重初始化</strong>：核心权重采用“fan-in variance scaling”初始化，而输入embedding则使用标准正态分布初始化。</li>\n<li><strong>优化器</strong>：使用Adafactor优化器进行训练，相较于传统Adam优化器，其学习率根据参数矩阵的均方根进行缩放。</li>\n<li><strong>损失函数</strong>：采用标准语言模型损失函数，并额外添加辅助损失以提高softmax标准化的稳定性。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始化权重时，核心权重使用“fan-in variance scaling”。</li>\n<li>⚠ 使用Adafactor优化器，注意学习率衰减策略。</li>\n<li>❗ 设置损失函数时，添加辅助损失来稳定softmax标准化。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>使用不当的权重初始化方法可能导致模型收敛缓慢或不稳定。确保在不同层中使用适合的初始化策略。</p>\n</blockquote>\n<h2 id=\"数据与公式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据与公式\"><span>数据与公式</span></a></h2>\n<h3 id=\"公式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#公式\"><span>公式</span></a></h3>\n<ul>\n<li>权重初始化：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>W</mi><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mfrac><mn>1</mn><msub><mi>n</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">W \\sim N(0, \\frac{1}{n_{in}})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1574em;vertical-align:-0.836em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3214em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">in</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.836em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n<li>输入embedding初始化：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>E</mi><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">E \\sim N(0, 1)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n</ul>\n<h3 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>序列长度</td>\n<td>2048</td>\n</tr>\n<tr>\n<td>词表大小</td>\n<td>256K</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 研究并实现SwiGLU激活函数的具体应用。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探讨并行层设计对模型效率的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同优化器对大规模模型训练的效果。</label></li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于PaLM: Scaling Language Modeling with Pathways论文分析与总结。</p>\n</blockquote>\n","env":{"base":"/","filePath":"/Users/qianyuhe/Desktop/my-project/docs/notes_bak/大语言模型学习/Common Models常见模型/PLaM系列/PLaM.md","filePathRelative":"notes_bak/大语言模型学习/Common Models常见模型/PLaM系列/PLaM.md","frontmatter":{"dg-publish":true,"dg-permalink":"/大语言模型学习/Common-Models常见模型/PLaM系列/PaLM","dg-home":false,"dg-description":"在此输入笔记的描述","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"在此输入访问密码","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/大语言模型学习/Common-Models常见模型/PLaM系列/PaLM/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-24T04:41:12.000Z","updated":"2025-04-24T04:47:57.000Z","title":"PLaM","createTime":"2025/05/13 17:33:53"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自然语言处理, 机器学习</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<p>PaLM, Transformer, 自然语言处理, 机器学习, 模型训练</p>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>本文介绍了PaLM（Pathways Language Model）的结构、训练设置和优化策略。PaLM采用了标准的Transformer架构，并进行了多项改进以提升模型性能和训练稳定性。\n<img src=\"/img/user/附件/Pasted image 20250424124216.png\" alt=\"Pasted image 20250424124216.png\"><img src=\"/img/user/附件/Pasted image 20250424124234.png\" alt=\"Pasted image 20250424124234.png\"></p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>PaLM使用标准的Transformer架构，采用了仅包含解码器的结构，并进行了以下改动：</p>\n<ul>\n<li><strong>SwiGLU激活函数</strong>：替换了传统的FFN部分。</li>\n<li><strong>并行层设计</strong>：改变了序列化表述，使用并行化结构以提升计算效率。</li>\n<li><strong>其他优化</strong>：引入MQA、RoPE、共享输入输出embedding，并去掉了dense层和Layer Norm中的偏差项。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：使用SwiGLU激活函数和并行层设计显著提高了模型的训练效率和稳定性。</p>\n<h2 id=\"训练设置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练设置\"><span>训练设置</span></a></h2>\n<ul>\n<li><strong>权重初始化</strong>：核心权重采用“fan-in variance scaling”初始化，而输入embedding则使用标准正态分布初始化。</li>\n<li><strong>优化器</strong>：使用Adafactor优化器进行训练，相较于传统Adam优化器，其学习率根据参数矩阵的均方根进行缩放。</li>\n<li><strong>损失函数</strong>：采用标准语言模型损失函数，并额外添加辅助损失以提高softmax标准化的稳定性。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始化权重时，核心权重使用“fan-in variance scaling”。</li>\n<li>⚠ 使用Adafactor优化器，注意学习率衰减策略。</li>\n<li>❗ 设置损失函数时，添加辅助损失来稳定softmax标准化。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>使用不当的权重初始化方法可能导致模型收敛缓慢或不稳定。确保在不同层中使用适合的初始化策略。</p>\n</blockquote>\n<h2 id=\"数据与公式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据与公式\"><span>数据与公式</span></a></h2>\n<h3 id=\"公式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#公式\"><span>公式</span></a></h3>\n<ul>\n<li>权重初始化：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>W</mi><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mfrac><mn>1</mn><msub><mi>n</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">W \\sim N(0, \\frac{1}{n_{in}})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1574em;vertical-align:-0.836em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3214em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">in</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.836em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n<li>输入embedding初始化：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>E</mi><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">E \\sim N(0, 1)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n</ul>\n<h3 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>序列长度</td>\n<td>2048</td>\n</tr>\n<tr>\n<td>词表大小</td>\n<td>256K</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 研究并实现SwiGLU激活函数的具体应用。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探讨并行层设计对模型效率的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同优化器对大规模模型训练的效果。</label></li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于PaLM: Scaling Language Modeling with Pathways论文分析与总结。</p>\n</blockquote>\n</template>","contentStripped":"<h2 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h2>\n<p>自然语言处理, 机器学习</p>\n<h2 id=\"标签\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#标签\"><span>标签</span></a></h2>\n<p>PaLM, Transformer, 自然语言处理, 机器学习, 模型训练</p>\n<h2 id=\"日期\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#日期\"><span>日期</span></a></h2>\n<p>2025年4月12日</p>\n<h2 id=\"内容概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#内容概述\"><span>内容概述</span></a></h2>\n<p>本文介绍了PaLM（Pathways Language Model）的结构、训练设置和优化策略。PaLM采用了标准的Transformer架构，并进行了多项改进以提升模型性能和训练稳定性。\n<img src=\"/img/user/附件/Pasted image 20250424124216.png\" alt=\"Pasted image 20250424124216.png\"><img src=\"/img/user/附件/Pasted image 20250424124234.png\" alt=\"Pasted image 20250424124234.png\"></p>\n<h2 id=\"模型结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型结构\"><span>模型结构</span></a></h2>\n<p>PaLM使用标准的Transformer架构，采用了仅包含解码器的结构，并进行了以下改动：</p>\n<ul>\n<li><strong>SwiGLU激活函数</strong>：替换了传统的FFN部分。</li>\n<li><strong>并行层设计</strong>：改变了序列化表述，使用并行化结构以提升计算效率。</li>\n<li><strong>其他优化</strong>：引入MQA、RoPE、共享输入输出embedding，并去掉了dense层和Layer Norm中的偏差项。</li>\n</ul>\n<p>💡 <strong>启发点</strong>：使用SwiGLU激活函数和并行层设计显著提高了模型的训练效率和稳定性。</p>\n<h2 id=\"训练设置\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#训练设置\"><span>训练设置</span></a></h2>\n<ul>\n<li><strong>权重初始化</strong>：核心权重采用“fan-in variance scaling”初始化，而输入embedding则使用标准正态分布初始化。</li>\n<li><strong>优化器</strong>：使用Adafactor优化器进行训练，相较于传统Adam优化器，其学习率根据参数矩阵的均方根进行缩放。</li>\n<li><strong>损失函数</strong>：采用标准语言模型损失函数，并额外添加辅助损失以提高softmax标准化的稳定性。</li>\n</ul>\n<h2 id=\"操作步骤\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#操作步骤\"><span>操作步骤</span></a></h2>\n<ol>\n<li>✅ 初始化权重时，核心权重使用“fan-in variance scaling”。</li>\n<li>⚠ 使用Adafactor优化器，注意学习率衰减策略。</li>\n<li>❗ 设置损失函数时，添加辅助损失来稳定softmax标准化。</li>\n</ol>\n<h2 id=\"常见错误\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#常见错误\"><span>常见错误</span></a></h2>\n<blockquote>\n<p>使用不当的权重初始化方法可能导致模型收敛缓慢或不稳定。确保在不同层中使用适合的初始化策略。</p>\n</blockquote>\n<h2 id=\"数据与公式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据与公式\"><span>数据与公式</span></a></h2>\n<h3 id=\"公式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#公式\"><span>公式</span></a></h3>\n<ul>\n<li>权重初始化：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>W</mi><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mfrac><mn>1</mn><msub><mi>n</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">W \\sim N(0, \\frac{1}{n_{in}})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1574em;vertical-align:-0.836em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3214em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">in</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.836em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n<li>输入embedding初始化：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>E</mi><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">E \\sim N(0, 1)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n</ul>\n<h3 id=\"数据表格\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#数据表格\"><span>数据表格</span></a></h3>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>序列长度</td>\n<td>2048</td>\n</tr>\n<tr>\n<td>词表大小</td>\n<td>256K</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"行动清单\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#行动清单\"><span>行动清单</span></a></h2>\n<ul class=\"task-list-container\">\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-0\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-0\"> 研究并实现SwiGLU激活函数的具体应用。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-1\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-1\"> 探讨并行层设计对模型效率的影响。</label></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" id=\"task-item-2\" disabled=\"disabled\"><label class=\"task-list-item-label\" for=\"task-item-2\"> 测试不同优化器对大规模模型训练的效果。</label></li>\n</ul>\n<blockquote>\n<p>来源：本文内容基于PaLM: Scaling Language Modeling with Pathways论文分析与总结。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"\n## 分类\n自然语言处理, 机器学习\n\n\n## 标签\nPaLM, Transformer, 自然语言处理, 机器学习, 模型训练\n\n\n## 日期\n2025年4月12日\n\n\n## 内容概述\n本文介绍了PaLM（Pathways Language Model）的结构、训练设置和优化策略。PaLM采用了标准的Transformer架构，并进行了多项改进以提升模型性能和训练稳定性。\n![Pasted image 20250424124216.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424124216.png)![Pasted image 20250424124234.png](/img/user/%E9%99%84%E4%BB%B6/Pasted%20image%2020250424124234.png)\n\n\n## 模型结构\nPaLM使用标准的Transformer架构，采用了仅包含解码器的结构，并进行了以下改动：\n- **SwiGLU激活函数**：替换了传统的FFN部分。\n- **并行层设计**：改变了序列化表述，使用并行化结构以提升计算效率。\n- **其他优化**：引入MQA、RoPE、共享输入输出embedding，并去掉了dense层和Layer Norm中的偏差项。\n\n💡 **启发点**：使用SwiGLU激活函数和并行层设计显著提高了模型的训练效率和稳定性。\n\n\n## 训练设置\n- **权重初始化**：核心权重采用“fan-in variance scaling”初始化，而输入embedding则使用标准正态分布初始化。\n- **优化器**：使用Adafactor优化器进行训练，相较于传统Adam优化器，其学习率根据参数矩阵的均方根进行缩放。\n- **损失函数**：采用标准语言模型损失函数，并额外添加辅助损失以提高softmax标准化的稳定性。\n\n\n## 操作步骤\n1. ✅ 初始化权重时，核心权重使用“fan-in variance scaling”。\n2. ⚠ 使用Adafactor优化器，注意学习率衰减策略。\n3. ❗ 设置损失函数时，添加辅助损失来稳定softmax标准化。\n\n\n## 常见错误\n> 使用不当的权重初始化方法可能导致模型收敛缓慢或不稳定。确保在不同层中使用适合的初始化策略。\n\n\n## 数据与公式\n\n### 公式\n- 权重初始化：\n  $$\n  W \\sim N(0, \\frac{1}{n_{in}})\n  $$\n- 输入embedding初始化：\n  $$\n  E \\sim N(0, 1)\n  $$\n\n\n### 数据表格\n| 参数 | 值 |\n|------|----|\n| 序列长度 | 2048 |\n| 词表大小 | 256K |\n\n\n## 行动清单\n- [ ] 研究并实现SwiGLU激活函数的具体应用。\n- [ ] 探讨并行层设计对模型效率的影响。\n- [ ] 测试不同优化器对大规模模型训练的效果。\n\n> 来源：本文内容基于PaLM: Scaling Language Modeling with Pathways论文分析与总结。","excerpt":"","includedFiles":[],"tasklistId":3,"title":"","headers":[{"level":2,"title":"分类","slug":"分类","link":"#分类","children":[]},{"level":2,"title":"标签","slug":"标签","link":"#标签","children":[]},{"level":2,"title":"日期","slug":"日期","link":"#日期","children":[]},{"level":2,"title":"内容概述","slug":"内容概述","link":"#内容概述","children":[]},{"level":2,"title":"模型结构","slug":"模型结构","link":"#模型结构","children":[]},{"level":2,"title":"训练设置","slug":"训练设置","link":"#训练设置","children":[]},{"level":2,"title":"操作步骤","slug":"操作步骤","link":"#操作步骤","children":[]},{"level":2,"title":"常见错误","slug":"常见错误","link":"#常见错误","children":[]},{"level":2,"title":"数据与公式","slug":"数据与公式","link":"#数据与公式","children":[{"level":3,"title":"公式","slug":"公式","link":"#公式","children":[]},{"level":3,"title":"数据表格","slug":"数据表格","link":"#数据表格","children":[]}]},{"level":2,"title":"行动清单","slug":"行动清单","link":"#行动清单","children":[]}]}}
